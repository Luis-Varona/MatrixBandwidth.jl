var documenterSearchIndex = {"docs":
[{"location":"public_api/#MatrixBandwidth.jl-–-Public-API","page":"Public API","title":"MatrixBandwidth.jl – Public API","text":"","category":"section"},{"location":"public_api/","page":"Public API","title":"Public API","text":"Documentation for MatrixBandwidth's public API.","category":"page"},{"location":"public_api/","page":"Public API","title":"Public API","text":"note: Note\nThe following documentation covers only the public API of the package. For internal details, see the private API documentation.","category":"page"},{"location":"public_api/#MatrixBandwidth.MatrixBandwidth","page":"Public API","title":"MatrixBandwidth.MatrixBandwidth","text":"MatrixBandwidth\n\nFast algorithms for matrix bandwidth minimization and matrix bandwidth recognition in Julia.\n\nThe bandwidth of an nn matrix A is the minimum non-negative integer k  0 1  n - 1 such that Ai j = 0 whenever i - j  k. Equivalently, A has bandwidth at most k if all entries above the kᵗʰ superdiagonal and below the kᵗʰ subdiagonal are zero, and A has bandwidth at least k if there exists any nonzero entry in the kᵗʰ superdiagonal or subdiagonal.\n\nThe matrix bandwidth minimization problem involves finding a permutation matrix P such that the bandwidth of PAPᵀ is minimized; this is known to be NP-complete. Several heuristic algorithms (such as Gibbs–Poole–Stockmeyer) run in polynomial time while still producing near-optimal orderings in practice, but exact methods (like Caprara–Salazar-González) are at least exponential in time complexity and thus are only feasible for relatively small matrices.\n\nOn the other hand, the matrix bandwidth recognition problem entails determining whether there exists a permutation matrix P such that the bandwidth of PAPᵀ is at most some fixed non-negative integer k  ℕ—an optimal permutation that fully minimizes the bandwidth of A is not required. Unlike the NP-hard minimization problem, this is decidable in O(nᵏ) time.\n\nThe following algorithms are currently supported:\n\nMinimization\nExact\nCaprara–Salazar-González algorithm (Minimization.CapraraSalazarGonzalez)\nDel Corso–Manzini algorithm (Minimization.DelCorsoManzini)\nDel Corso–Manzini algorithm with perimeter search   (Minimization.DelCorsoManziniWithPS)\nSaxe–Gurari–Sudborough algorithm (Minimization.SaxeGurariSudborough)\nBrute-force search (Minimization.BruteForceSearch)\nHeuristic\nGibbs–Poole–Stockmeyer algorithm (Minimization.GibbsPooleStockmeyer)\nCuthill–McKee algorithm (Minimization.CuthillMcKee)\nReverse Cuthill–McKee algorithm (Minimization.ReverseCuthillMcKee)\nMetaheuristic\nGreedy randomized adaptive search procedure (GRASP) (Minimization.GRASP)\nSimulated annealing (Minimization.SimulatedAnnealing)\nGenetic algorithm (Minimization.GeneticAlgorithm)\nRecognition\nCaprara–Salazar-González algorithm (Recognition.CapraraSalazarGonzalez)\nDel Corso–Manzini algorithm (Recognition.DelCorsoManzini)\nDel Corso–Manzini algorithm with perimeter search   (Recognition.DelCorsoManziniWithPS)\nSaxe–Gurari–Sudborough algorithm (Recognition.SaxeGurariSudborough)\nBrute-force search (Recognition.BruteForceSearch)\n\nThis package also exports several additional core functions, including (but not limited to) bandwidth and profile to compute the original bandwidth and profile of a matrix prior to any reordering.\n\nThe full documentation is available at GitHub Pages.\n\n\n\n\n\n","category":"module"},{"location":"public_api/#MatrixBandwidth.bandwidth-Tuple{AbstractMatrix{<:Number}}","page":"Public API","title":"MatrixBandwidth.bandwidth","text":"bandwidth(A) -> Int\n\nCompute the bandwidth of A before any permutation of its rows and columns.\n\nThe bandwidth of an nn matrix A is the minimum non-negative integer k  0 1  n - 1 such that Ai j = 0 whenever i - j  k. Equivalently, A has bandwidth at most k if all entries above the kᵗʰ superdiagonal and below the kᵗʰ subdiagonal are zero, and A has bandwidth at least k if there exists any nonzero entry in the kᵗʰ superdiagonal or subdiagonal.\n\nIn contrast to minimize_bandwidth, this function does not attempt to minimize the bandwidth of A by permuting its rows and columns—it simply computes its bandwidth as is.\n\nArguments\n\nA::AbstractMatrix{<:Number}: the (square) matrix whose bandwidth is computed.\n\nReturns\n\n::Int: the bandwidth of A.\n\nPerformance\n\nGiven an nn input matrix A, this relatively simple algorithm runs in O(n²) time.\n\nExamples\n\nbandwidth correctly identifies the bandwidth of a pentadiagonal matrix as 2 and does not attempt to find a minimizing permutation upon shuffling of its rows and columns:\n\njulia> using Random\n\njulia> Random.seed!(242622);\n\njulia> (n, k) = (8, 2);\n\njulia> perm = randperm(n);\n\njulia> A = (!iszero).(random_banded_matrix(8, 2))\n8×8 BitMatrix:\n 1  0  0  0  0  0  0  0\n 0  1  0  1  0  0  0  0\n 0  0  0  1  1  0  0  0\n 0  1  1  1  0  1  0  0\n 0  0  1  0  0  0  0  0\n 0  0  0  1  0  0  0  0\n 0  0  0  0  0  0  0  0\n 0  0  0  0  0  0  0  0\n\njulia> bandwidth(A)\n2\n\njulia> A_shuffled = A[perm, perm]\n8×8 BitMatrix:\n 0  0  0  0  0  0  1  0\n 0  0  0  0  0  0  0  0\n 0  0  0  1  0  0  0  0\n 0  0  1  0  0  0  1  0\n 0  0  0  0  1  0  0  0\n 0  0  0  0  0  1  1  0\n 1  0  0  1  0  1  1  0\n 0  0  0  0  0  0  0  0\n\njulia> bandwidth(A_shuffled)\n6\n\nNotes\n\nSome texts define matrix bandwidth to be the minimum non-negative integer k such that Ai j = 0 whenever i - j  k instead, particularly in more mathematically-minded communities. Effectively, this definition treats diagonal matrices as bandwidth 1, tridiagonal matrices as bandwidth 2, and so on. Our definition, on the other hand, is more common in computer science contexts, treating diagonal matrices as bandwidth 0 and tridiagonal matrices as bandwidth 1. (Both definitions, however, agree that the bandwidth of an empty matrix is simply 0.)\n\n\n\n\n\n","category":"method"},{"location":"public_api/#MatrixBandwidth.bandwidth_lower_bound-Tuple{AbstractMatrix{<:Number}}","page":"Public API","title":"MatrixBandwidth.bandwidth_lower_bound","text":"bandwidth_lower_bound(A) -> Int\n\nCompute a lower bound on the bandwidth of A using [CS05, pp. 359–60]'s results.\n\nA is assumed to be structurally symmetric, since the bound from [CS05, pp.359–60] was discovered in the context of undirected graphs (whose adjacency matrices are symmetric).\n\nThe bandwidth of an nn matrix A is the minimum non-negative integer k  0 1  n - 1 such that Ai j = 0 whenever i - j  k. Equivalently, A has bandwidth at most k if all entries above the kᵗʰ superdiagonal and below the kᵗʰ subdiagonal are zero, and A has bandwidth at least k if there exists any nonzero entry in the kᵗʰ superdiagonal or subdiagonal.\n\nIn contrast to minimize_bandwidth, this function does not attempt to truly minimize the bandwidth of A—it simply returns a lower bound on its bandwidth up to symmetric permutation of its rows and columns. This bound is not generally tight, but it indeed matches the true minimum in many non-trivial cases and is easily computable in O(n³) time (dominated by the Floyd–Warshall algorithm call; the core logic itself runs in O(n²) time).\n\nArguments\n\nA::AbstractMatrix{<:Number}: the (square) matrix on whose bandwidth a lower bound is to   be computed. A must be structurally symmetric (i.e., A[i, j] must be nonzero if   and only if A[j, i] is nonzero for 1  i j  n).\n\nReturns\n\n::Int: a lower bound on the bandwidth of A. (This bound is tight in many non-trivial   cases but not universally so.)\n\nExamples\n\nThe function correctly computes a bound less than (or equal to) the true minimum bandwidth of a matrix up to symmetric permutation:\n\njulia> using Random, SparseArrays, Combinatorics\n\njulia> Random.seed!(21);\n\njulia> (n, p) = (9, 0.4);\n\njulia> A = sprand(n, n, p);\n\njulia> A = A + A' # Ensure structural symmetry;\n\njulia> minimize_bandwidth(A, Minimization.BruteForceSearch())\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Brute-force search\n * Approach: exact\n * Minimum Bandwidth: 5\n * Original Bandwidth: 8\n * Matrix Size: 9×9\n\njulia> bandwidth_lower_bound(A) # Always less than or equal to the true minimum bandwidth\n4\n\nNotes\n\nSome texts define matrix bandwidth to be the minimum non-negative integer k such that Ai j = 0 whenever i - j  k instead, particularly in more mathematically-minded communities. Effectively, this definition treats diagonal matrices as bandwidth 1, tridiagonal matrices as bandwidth 2, and so on. Our definition, on the other hand, is more common in computer science contexts, treating diagonal matrices as bandwidth 0 and tridiagonal matrices as bandwidth 1. (Both definitions, however, agree that the bandwidth of an empty matrix is simply 0.)\n\n\n\n\n\n","category":"method"},{"location":"public_api/#MatrixBandwidth.profile-Tuple{AbstractMatrix{<:Number}}","page":"Public API","title":"MatrixBandwidth.profile","text":"profile(A) -> Int\n\nCompute the profile of A before any permutation of its rows and columns.\n\nThe profile of a structurally symmetric nn matrix A is traditionally defined as the sum of the distances from each diagonal entry to the leftmost nonzero entry in that row—in other words, ᵢ₁ⁿ (i - fᵢ), where each fᵢ is the smallest index such that Ai fᵢ  0 [Maf14, pp. 187-88]. Generalizing this property to all square matrices, we define the column profile of a matrix to be the sum of the distances from each diagonal entry to the farthest (not necessarily topmost) nonzero entry in that column and the row profile to be the sum of the distances from each diagonal entry to the farthest (not necessarily leftmost) nonzero entry in that row. (Note that both of these properties are equal to traditional matrix profile for structurally symmetric matrices.)\n\nOne of the most common contexts in which matrix profile is relevant is sparse matrix storage, where lower-profile matrices occupy less space in memory [Maf14, p.188]. Since Julia's SparseArrays package defaults to compressed sparse column storage over compressed sparse row, we therefore compute column profile by default unless the dimension is otherwise specified.\n\nArguments\n\nA::AbstractMatrix{<:Number}: the (square) matrix whose profile is computed.\n\nKeyword Arguments\n\ndim::Symbol=:col: the dimension along which the profile is computed; must be either   :col (the default) or :row.\n\nReturns\n\n::Int: the profile of A along the specified dimension.\n\nPerformance\n\nGiven an nn input matrix A, this relatively simple algorithm runs in O(n²) time.\n\nExamples\n\nprofile computes the column profile of a matrix by default:\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(2287);\n\njulia> (n, p) = (25, 0.05);\n\njulia> A = sprand(n, n, p)\n25×25 SparseMatrixCSC{Float64, Int64} with 29 stored entries:\n⎡⠀⠀⠀⠀⠀⠀⠐⠀⠒⠀⡀⠀⠀⎤\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠂⠐⠂⎥\n⎢⠀⠀⠀⢀⠌⠀⠀⠀⢀⠈⠀⠀⠀⎥\n⎢⠠⢄⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠂⡀⠀⠀⠌⠀⠈⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠢⡀⢄⡈⠀⠀⠀⎥\n⎣⠀⠀⠀⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⎦\n\njulia> profile(A)\n211\n\nThe dimension (:row or :col) can also be explicitly specified:\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(3647);\n\njulia> (n, p) = (25, 0.05);\n\njulia> A = sprand(n, n, p)\n25×25 SparseMatrixCSC{Float64, Int64} with 31 stored entries:\n⎡⠄⠀⠀⠀⠀⠀⠀⠘⠀⠀⠀⠁⠀⎤\n⎢⠄⢀⠀⠀⠁⠀⠀⠀⠀⢀⠀⠀⠀⎥\n⎢⠀⢀⡂⠀⠀⠀⠀⠀⠀⠀⠀⡄⠀⎥\n⎢⠀⠀⠀⠀⠀⡀⠂⠀⠀⠀⠀⠀⠀⎥\n⎢⠁⠀⠁⠀⠁⠀⠀⠁⠄⢀⠈⠀⠀⎥\n⎢⠂⠐⠐⠐⠠⠀⠄⠀⠀⠀⠠⣀⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎦\n\njulia> profile(A, dim=:row)\n147\n\njulia> profile(A, dim=:col)\n175\n\n\n\n\n\n","category":"method"},{"location":"public_api/#MatrixBandwidth.random_banded_matrix-Tuple{Int64, Int64}","page":"Public API","title":"MatrixBandwidth.random_banded_matrix","text":"random_banded_matrix(n, k; p=0.5, rng=default_rng()) -> Matrix{Float64}\n\nGenerate a random n×n structurally symmetric k-banded matrix with band density ≈ p.\n\nBy definition of structural symmetry, the (i j)-th entry of the matrix is nonzero if and only if the (j i)-th entry is nonzero as well. All entries from this matrix are from the interval [0, 1]. Entries up to the kᵗʰ superdiagonal and down to the kᵗʰ subdiagonal are nonzero with probability p.\n\nIt is also guaranteed that each of these bands (besides the main diagonal) has at least one nonzero entry (even when p is very small), thus ensuring that the matrix has bandwidth precisely k before any reordering. (There may, however, still exist a symmetric permutation inducing a minimum bandwidth less than k, especially for small values of p.)\n\nArguments\n\nn::Int: the order of the matrix to generate. Must be positive.\nk::Int: the desired matrix bandwidth. Must satisfy 0 ≤ k < n.\n\nKeyword Arguments\n\np::Real=0.5: the band density. Must satisfy 0 < p ≤ 1. Defaults to 0.5.\nrng::AbstractRNG=Random.default_rng(): the random number generator to use. Defaults to   Random.default_rng().\n\nReturns\n\n::Matrix{Float64}: a random n×n matrix with bandwidth exactly k and sparse bands   with density p.\n\nExamples\n\nGenerate a 66 matrix with bandwidth 1 and the maximum number of nonzero entries:\n\njulia> using Random\n\njulia> A = random_banded_matrix(6, 1; p=1, rng=MersenneTwister(1228))\n6×6 Matrix{Float64}:\n 0.310239  0.346413  0.0       0.0        0.0       0.0\n 0.509981  0.917073  0.390771  0.0        0.0       0.0\n 0.0       0.760045  0.808396  0.0195686  0.0       0.0\n 0.0       0.0       0.222338  0.853164   0.806888  0.0\n 0.0       0.0       0.0       0.421603   0.132165  0.805813\n 0.0       0.0       0.0       0.0        0.305339  0.0799183\n\njulia> bandwidth(A)\n1\n\nGenerate a 77 matrix with bandwidth 3 and band density 03:\n\njulia> using Random\n\njulia> A = random_banded_matrix(7, 3; p=0.3, rng=MersenneTwister(0402))\n7×7 Matrix{Float64}:\n 0.0       0.132699  0.0       0.0       0.0  0.0       0.0\n 0.869352  0.0       0.324319  0.926496  0.0  0.0       0.0\n 0.0       0.891878  0.0       0.658102  0.0  0.0       0.0\n 0.0       0.88859   0.399559  0.0       0.0  0.284285  0.703377\n 0.0       0.0       0.0       0.0       0.0  0.0       0.0\n 0.0       0.0       0.0       0.489594  0.0  0.0       0.393573\n 0.0       0.0       0.0       0.412412  0.0  0.47063   0.0\n\njulia> bandwidth(A)\n3\n\nGenerate an 88 diagonal (bandwidth 0) matrix with default band density (05):\n\njulia> using Random\n\njulia> A = random_banded_matrix(8, 0; rng=MersenneTwister(0102))\n8×8 Matrix{Float64}:\n 0.0  0.0        0.0       0.0       0.0  0.0      0.0  0.0\n 0.0  0.0762399  0.0       0.0       0.0  0.0      0.0  0.0\n 0.0  0.0        0.373113  0.0       0.0  0.0      0.0  0.0\n 0.0  0.0        0.0       0.726309  0.0  0.0      0.0  0.0\n 0.0  0.0        0.0       0.0       0.0  0.0      0.0  0.0\n 0.0  0.0        0.0       0.0       0.0  0.41974  0.0  0.0\n 0.0  0.0        0.0       0.0       0.0  0.0      0.0  0.0\n 0.0  0.0        0.0       0.0       0.0  0.0      0.0  0.293132\n\njulia> bandwidth(A)\n0\n\nNotes\n\nUsers of the MatrixBandwidth package may find this function useful when generating random test data for whatever frameworks, algorithms, etc. they are implementing.\n\n\n\n\n\n","category":"method"},{"location":"public_api/#MatrixBandwidth.Minimization","page":"Public API","title":"MatrixBandwidth.Minimization","text":"MatrixBandwidth.Minimization\n\nExact, heuristic, and metaheuristic algorithms for matrix bandwidth minimization in Julia.\n\nThe bandwidth of an nn matrix A is the minimum non-negative integer k  0 1  n - 1 such that Ai j = 0 whenever i - j  k. Equivalently, A has bandwidth at most k if all entries above the kᵗʰ superdiagonal and below the kᵗʰ subdiagonal are zero, and A has bandwidth at least k if there exists any nonzero entry in the kᵗʰ superdiagonal or subdiagonal.\n\nThe matrix bandwidth minimization problem involves finding a permutation matrix P such that the bandwidth of PAPᵀ is minimized; this is known to be NP-complete. Several heuristic algorithms (such as Gibbs–Poole–Stockmeyer) run in polynomial time while still producing near-optimal orderings in practice, but exact methods (like Caprara–Salazar-González) are at least exponential in time complexity and thus are only feasible for relatively small matrices.\n\nThe following algorithms are currently supported:\n\nExact\nCaprara–Salazar-González algorithm (CapraraSalazarGonzalez)\nDel Corso–Manzini algorithm (DelCorsoManzini)\nDel Corso–Manzini algorithm with perimeter search (DelCorsoManziniWithPS)\nSaxe–Gurari–Sudborough algorithm (SaxeGurariSudborough)\nBrute-force search (BruteForceSearch)\nHeuristic\nGibbs–Poole–Stockmeyer algorithm (GibbsPooleStockmeyer)\nCuthill–McKee algorithm (CuthillMcKee)\nReverse Cuthill–McKee algorithm (ReverseCuthillMcKee)\nMetaheuristic\nGreedy randomized adaptive search procedure (GRASP) (GRASP)\nSimulated annealing (SimulatedAnnealing)\nGenetic algorithm (GeneticAlgorithm)\n\nThis submodule is part of the MatrixBandwidth.jl package.\n\n\n\n\n\n","category":"module"},{"location":"public_api/#MatrixBandwidth.Minimization.MinimizationResult","page":"Public API","title":"MatrixBandwidth.Minimization.MinimizationResult","text":"MinimizationResult{A,M,O} <: AbstractResult\n\nOutput struct for matrix bandwidth minimization results.\n\nFields\n\nalgorithm::A<:AbstractSolver: the solver used to minimize the bandwidth.\nmatrix::M<:AbstractMatrix{<:Number}: the original matrix whose bandwidth is minimized.\nordering::O<:Vector{Int}: the (near-)optimal ordering of the rows and columns.\nbandwidth::Int: the minimized bandwidth of the matrix.\napproach::Symbol: the approach used by the solver. (Should be one of :exact,   :heuristic, and :metaheuristic.)\n\nConstructors\n\nMinimizationResult(algorithm, matrix, ordering, bandwidth): constructs a new   MinimizationResult instance with the given fields. The approach field is   automatically determined based on the algorithm type.\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Minimization.minimize_bandwidth","page":"Public API","title":"MatrixBandwidth.Minimization.minimize_bandwidth","text":"minimize_bandwidth(A, solver=GibbsPooleStockmeyer()) -> MinimizationResult\n\nMinimize the bandwidth of A using the algorithm defined by solver.\n\nThe bandwidth of an nn matrix A is the minimum non-negative integer k  0 1  n - 1 such that Ai j = 0 whenever i - j  k. Equivalently, A has bandwidth at most k if all entries above the kᵗʰ superdiagonal and below the kᵗʰ subdiagonal are zero, and A has bandwidth at least k if there exists any nonzero entry in the kᵗʰ superdiagonal or subdiagonal.\n\nThis function computes a (near-)optimal ordering π of the rows and columns of A so that the bandwidth of PAPᵀ is minimized, where P is the permutation matrix corresponding to π. This is known to be an NP-complete problem; however, several heuristic algorithms such as Gibbs–Poole–Stockmeyer run in polynomial time while still still producing near-optimal orderings in practice. Exact methods like Caprara–Salazar-González are also available, but they are at least exponential in time complexity and thus only feasible for relatively small matrices.\n\nArguments\n\nA::AbstractMatrix{<:Number}: the (square) matrix whose bandwidth is minimized.\nsolver::AbstractSolver: the matrix bandwidth minimization algorithm to use; defaults to   GibbsPooleStockmeyer. (See the Minimization module documentation for   a full list of supported solvers.)\n\nReturns\n\n::MinimizationResult: a struct containing the algorithm used, the original matrix A,   the (near-)optimal ordering of the rows and columns, and the minimized bandwidth.\n\nExamples\n\n[TODO: Add here once more solvers are implemented. For now, refer to the Examples sections of the GibbsPooleStockmeyer, CuthillMcKee, and ReverseCuthillMcKee docstrings.]\n\nNotes\n\nSome texts define matrix bandwidth to be the minimum non-negative integer k such that Ai j = 0 whenever i - j  k instead, particularly in more mathematically-minded communities. Effectively, this definition treats diagonal matrices as bandwidth 1, tridiagonal matrices as bandwidth 2, and so on. Our definition, on the other hand, is more common in computer science contexts, treating diagonal matrices as bandwidth 0 and tridiagonal matrices as bandwidth 1. (Both definitions, however, agree that the bandwidth of an empty matrix is simply 0.)\n\n\n\n\n\n","category":"function"},{"location":"public_api/#MatrixBandwidth.Minimization.Exact","page":"Public API","title":"MatrixBandwidth.Minimization.Exact","text":"MatrixBandwidth.Minimization.Exact\n\nExact solvers for matrix bandwidth minimization.\n\nExact methods are those which guarantee an optimal ordering producing the true minimum bandwidth of a matrix. Since bandwidth minimization is an NP-complete problem, existing exact algorithms are, at best, exponential in time complexity—much worse than many polynomial-time heuristic approaches (e.g., Gibbs–Poole–Stockmeyer). Such methods, therefore, are not feasible for large matrices, but they remain useful when precise solutions are required for small-to-medium-sized inputs (say, up to 100100).\n\nThe following exact algorithms are currently supported:\n\nCaprara–Salazar-González algorithm (CapraraSalazarGonzalez)\nDel Corso–Manzini algorithm (DelCorsoManzini)\nDel Corso–Manzini algorithm with perimeter search (DelCorsoManziniWithPS)\nSaxe–Gurari–Sudborough algorithm (SaxeGurariSudborough)\nBrute-force search (BruteForceSearch)\n\nThis submodule is part of the MatrixBandwidth.Minimization submodule of the MatrixBandwidth.jl package.\n\n\n\n\n\n","category":"module"},{"location":"public_api/#MatrixBandwidth.Minimization.Exact.BruteForceSearch","page":"Public API","title":"MatrixBandwidth.Minimization.Exact.BruteForceSearch","text":"BruteForceSearch <: ExactSolver <: AbstractSolver <: AbstractAlgorithm\n\nThe simplest exact method for minimizing the bandwidth of a matrix is to iterate over all possible symmetric permutations and compare the bandwidths they induce.\n\nSince i₁ i₂  iₙ induces the same bandwidth as iₙ iₙ₁  i₁, we restrict our search to orderings such that i₁  iₙ (with equality checked just in case n = 1).\n\nPerformance\n\nGiven an nn input matrix A, this brute-force algorithm runs in O(n  n²) time:\n\nPrecisely n2 permutations are checked (except when n = 1, in which case   1 = 1 permutation is checked). This is, clearly, O(n).\nFor each permutation, the bandwidth function is called on view(A, perm, perm),   which takes O(n²) time.\nTherefore, the overall time complexity is O(n  n²).\n\nIndeed, due to the need to exhaustively check all permutations, this is close to a lower bound as well on the the algorithm's time complexity. (The only reason we cannot claim to have a precise value for the big-Θ complexity is that the bandwidth function is not exactly Θ(n²), although it is close.)\n\nExamples\n\nThe algorithm always iterates over all possible permutations, so it is infeasible to go above 99 or 1010 without incurring multiple-hour runtimes. Nevertheless, we see that it is quite effective for, say, 88:\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(628318);\n\njulia> (n, p) = (8, 0.2);\n\njulia> A = sprand(Bool, n, n, p);\n\njulia> A = A + A' # Ensure structural symmetry;\n\njulia> minimize_bandwidth(A, Minimization.BruteForceSearch())\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Brute-force search\n * Approach: exact\n * Minimum Bandwidth: 3\n * Original Bandwidth: 6\n * Matrix Size: 8×8\n\nNotes\n\nBrute force is by far the slowest approach to matrix bandwidth minimization and should only be used in very niche cases (like verifying the correctness of other algorithms in unit tests). For 1010 matrices, the algorithm already takes several minutes to run (between 2 to 5 on most commercial machines) and allocates over 4 gigabytes of memory. Given the O(n  n²) time complexity, minimizing the bandwidth of any 1111 matrix would take over an hour.\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Minimization.Exact.CapraraSalazarGonzalez","page":"Public API","title":"MatrixBandwidth.Minimization.Exact.CapraraSalazarGonzalez","text":"CapraraSalazarGonzalez <: ExactSolver <: AbstractSolver <: AbstractAlgorithm\n\nThe Caprara–Salazar-González minimization algorithm is an exact method for minimizing the bandwidth of a structurally symmetric matrix A. For a fixed k  ℕ, the algorithm performs a bidirectional depth-first search of all partial orderings of the rows and columns of A, adding indices one at a time to both the left and right ends. Partial orderings are pruned not only by ensuring that adjacent pairs of currently placed indices are within k of each other but also by employing a branch-and-bound framework with lower bounds on bandwidtth compatibility computed via integer linear programming relaxations. This search is repeated with incrementing values of k until a bandwidth-k ordering is found [CS05], with k initialized to some lower bound on the minimum bandwidth of A up to symmetric permutation.\n\nSpecifically, this implementation of the Caprara–Salazar-González algorithm uses the min(α(A) γ(A)) lower bound from the original paper [CS05, pp. 359–60] as the initial value of k. (Further implementation details can be found in the source code for bandwidth_lower_bound.)\n\nAs noted above, the Caprara–Salazar-González algorithm requires structurally symmetric input (that is, Ai j must be nonzero if and only if Aj i is nonzero for 1  i j  n).\n\nPerformance\n\n[TODO: Write here]\n\nExamples\n\n[TODO: Write here]\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Minimization.Exact.DelCorsoManzini","page":"Public API","title":"MatrixBandwidth.Minimization.Exact.DelCorsoManzini","text":"DelCorsoManzini <: ExactSolver <: AbstractSolver <: AbstractAlgorithm\n\nThe Del Corso–Manzini minimization algorithm is an exact method for minimizing the bandwidth of a structurally symmetric matrix A. For a fixed k  ℕ, the algorithm performs a depth-first search of all partial orderings of the rows and columns of A, adding indices one at a time. Partial orderings are pruned not only by ensuring that adjacent pairs of currently placed indices are within k of each other but also by tracking the latest positions at which the remaining indices can be placed. This search is repeated with incrementing values of k until a bandwidth-k ordering is found [DM99], with k initialized to some lower bound on the minimum bandwidth of A up to symmetric permutation.\n\nSpecifically, this implementation of the Del Corso–Manzini algorithm uses the min(α(A) γ(A)) lower bound from [CS05, pp. 359–60] as the initial value of k. (Further implementation details can be found in the source code for bandwidth_lower_bound.) This improves upon the original algorithm, which used the maximum number of nonzero off-diagonal entries in a single row as a lower bound on the minimum bandwidth of A up to symmetric permutation [DM99, p. 192–93].\n\nAs noted above, the Del Corso–Manzini algorithm requires structurally symmetric input (that is, Ai j must be nonzero if and only if Aj i is nonzero for 1  i j  n).\n\nPerformance\n\nGiven an nn input matrix A, the Del Corso–Manzini algorithm runs in O(n  n³) time:\n\nFor each underlying \"bandwidth ≤ k\" check, we perform a depth-first search of   O(n) partial orderings.\nChecking plausibility of each partial ordering takes O(nk) time, resulting in   O(n  nk) steps for each value of k.\nThe difference between the maximum possible bandwidth (n - 1) and our initial lower   bound grows linearly in n, so we run the underlying O(n  nk) recognition   algorithm O(n) times.\nFinally, ₖ₀ⁿ¹ nk = O(n³), so the overall time complexity is O(n  n³).\n\nOf course, this is but an upper bound on the time complexity of Del Corso–Manzini, achieved only in the most pathological of cases. In practice, efficient pruning techniques and compatibility checks—along with [CS05, pp. 359–60]'s relatively tight initial lower bound on the minimum bandwidth—result in approximately exponential growth in time complexity with respect to n.\n\nBased on experimental results, the algorithm is feasible for nn matrices up to n  100 or so.\n\nExamples\n\nWe verify the optimality of the ordering found by Del Corso–Manzini for a random 99 matrix via a brute-force search over all possible permutations up to reversal:\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(0117);\n\njulia> (n, p) = (9, 0.5);\n\njulia> A = sprand(n, n, p);\n\njulia> A = A + A' # Ensure structural symmetry;\n\njulia> res_bf = minimize_bandwidth(A, Minimization.BruteForceSearch())\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Brute-force search\n * Approach: exact\n * Minimum Bandwidth: 5\n * Original Bandwidth: 8\n * Matrix Size: 9×9\n\njulia> res_dcm = minimize_bandwidth(A, Minimization.DelCorsoManzini())\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Del Corso–Manzini\n * Approach: exact\n * Minimum Bandwidth: 5\n * Original Bandwidth: 8\n * Matrix Size: 9×9\n\nWe now generate (and shuffle) a random 4040 matrix with minimum bandwidth 10 using MatrixBandwidth.random_banded_matrix. Del Corso–Manzini then finds a bandwidth-10 ordering, which is (we claim) optimal up to symmetric permutation. (In some cases, random_banded_matrix(n, k) does generate matrices with minimum bandwidth < k. Nevertheless, this example demonstrates that Del Corso–Manzini at the very least finds a good ordering, even though exact optimality—which is guaranteed by the original paper [DM99]—is not explicitly verified.)\n\njulia> using Random\n\njulia> Random.seed!(0201);\n\njulia> (n, k) = (40, 10);\n\njulia> A = random_banded_matrix(n, k);\n\njulia> perm = randperm(n);\n\njulia> A_shuffled = A[perm, perm];\n\njulia> bandwidth(A)\n10\n\njulia> bandwidth(A_shuffled) # Much larger after shuffling\n36\n\njulia> minimize_bandwidth(A_shuffled, Minimization.DelCorsoManzini())\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Del Corso–Manzini\n * Approach: exact\n * Minimum Bandwidth: 10\n * Original Bandwidth: 36\n * Matrix Size: 40×40\n\nNotes\n\nFor readers of the original paper, what we call the Del Corso–Manzini minimization algorithm here is designated the \"MB-ID algorithm\" in [DM99, p. 191]. The so-called \"MB-PS algorithm,\" on the other hand, we implement in DelCorsoManziniWithPS.\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS","page":"Public API","title":"MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS","text":"DelCorsoManziniWithPS{D} <: ExactSolver <: AbstractSolver <: AbstractAlgorithm\n\nThe Del Corso–Manzini minimization algorithm with perimeter search is an exact method for minimizing the bandwidth of a structurally symmetric matrix A. The base Del Corso–Manzini algorithm performs a depth-first search of all partial orderings of the rows and columns of A for some fixed k  ℕ, adding indices one at a time. Partial orderings are pruned not only by ensuring that adjacent pairs of currently placed indices are within k of each other but also by tracking the latest positions at which the remaining indices can be placed. This search is repeated with incrementing values of k until a bandwidth-k ordering is found [DM99], with k initialized to some lower bound on the minimum bandwidth of A up to symmetric permutation.\n\nThe incorporation of perimeter search to this approach entails precomputing a \"perimeter\" of d-permutations of row indices of A, where d is a positive integer passed as a parameter to the solver. Each permutation represents a way to select the last d entries of the ordering, and as the construction of the partial ordering progresses, potential endings are pruned to exclude those incompatible with already placed indices. In addition to pruning a potential ending if it contains indices already placed, compatibility is also checked via precomputed time stamps indicating, for each potential ending, a loose lower bound on the earliest position at which any given index can be placed should said ending be selected.\n\nLike our implementation of the base Del Corso–Manzini algorithm (see DelCorsoManzini), this implementation uses the min(α(A) γ(A)) lower bound from [CS05, pp. 359–60] as the initial value of k. (Further implementation details can be found in the source code for bandwidth_lower_bound.) This improves upon the original algorithm, which used the maximum number of nonzero off-diagonal entries in a single row as a lower bound on the minimum bandwidth of A up to symmetric permutation [DM99, p. 194].\n\nAs noted above, the Del Corso–Manzini algorithm with perimeter search requires structurally symmetric input (that is, Ai j must be nonzero if and only if Aj i is nonzero for 1  i j  n).\n\nFields\n\ndepth::D<:Union{Nothing,Int}: the perimeter search depth. If this field is not set (and)   thus automatically initialized to nothing), a default depth is automatically computed   by Recognition.dcm_ps_optimal_depth as a function of the input matrix every   time the solver is passed to MatrixBandwidth.Minimization.minimize_bandwidth.   Otherwise, it must be manually set to a positive integer.\n\nConstructors\n\nDelCorsoManziniWithPS(): constructs a new DelCorsoManziniWithPS instance with the   default perimeter search depth initialized to nothing.\nDelCorsoManziniWithPS(depth::Int): constructs a new DelCorsoManziniWithPS instance   with the specified perimeter search depth. depth must be a positive integer.\n\nPerformance\n\nGiven an nn input matrix A and perimeter search depth d, the Del Corso–Manzini algorithm with perimeter search runs in O(n  nᴰ¹) time, where Dᴰ = max(d 2):\n\nFor each underlying \"bandwidth ≤ k\" check, we perform a depth-first search of   O(n) partial orderings.\nChecking plausibility of each partial ordering takes O(nk) time, and checking   compatibility with all size-d LPOs takes O(nᵈ) time. Thus, the overall time   complexity for each value of k is O(n  (nᵈ + nk)).\nThe difference between the maximum possible bandwidth (n - 1) and our initial lower   bound grows linearly in n, so we run the underlying O(n  (nᵈ + nk))   recognition algorithm O(n) times.\nFinally, ₖ₀ⁿ¹ (nᵈ + nk) = O(nᵈ¹ + n³), so the overall time complexity   is O(n  nᴰ¹), where D = max(d 2).\n\nOf course, this is but an upper bound on the time complexity of Del Corso–Manzini with perimeter search, achieved only in the most pathological of cases. In practice, efficient pruning techniques and compatibility checks—along with [CS05, pp. 359–60]'s relatively tight initial lower bound on the minimum bandwidth—result in approximately exponential growth in time complexity with respect to n.\n\nBased on experimental results, the algorithm is feasible for nn matrices up to n  100 or so.\n\nExamples\n\nWe verify the optimality of the ordering found by Del Corso–Manzini with perimeter search for a random 99 matrix via a brute-force search over all possible permutations up to reversal. The depth parameter is not explicitly set; instead, some near-optimal value is automatically computed upon the first MatrixBandwidth.Minimization.minimize_bandwidth function call.\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(548836);\n\njulia> (n, p) = (9, 0.2);\n\njulia> A = sprand(n, n, p);\n\njulia> A = A + A' # Ensure structural symmetry;\n\njulia> res_bf = minimize_bandwidth(A, Minimization.BruteForceSearch())\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Brute-force search\n * Approach: exact\n * Minimum Bandwidth: 3\n * Original Bandwidth: 8\n * Matrix Size: 9×9\n\njulia> res_dcm = minimize_bandwidth(A, Minimization.DelCorsoManziniWithPS())\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Del Corso–Manzini with perimeter search\n * Approach: exact\n * Minimum Bandwidth: 3\n * Original Bandwidth: 8\n * Matrix Size: 9×9\n\nWe now generate (and shuffle) a random 3030 matrix with minimum bandwidth 8 using MatrixBandwidth.random_banded_matrix. Del Corso–Manzini with perimeter search then finds a bandwidth-8 ordering, which is (we claim) optimal up to symmetric permutation. (In some cases, random_banded_matrix(n, k) does generate matrices with minimum bandwidth < k. Nevertheless, this example demonstrates that Del Corso–Manzini at the very least finds a good ordering, even though exact optimality—which is guaranteed by the original paper [DM99]—is not explicitly verified.) In this case, we set the depth parameter to 4 beforehand instead of relying on Recognition.dcm_ps_optimal_depth.\n\njulia> using Random\n\njulia> Random.seed!(78779);\n\njulia> (n, k, depth) = (30, 8, 4);\n\njulia> A = random_banded_matrix(n, k);\n\njulia> perm = randperm(n);\n\njulia> A_shuffled = A[perm, perm];\n\njulia> bandwidth(A)\n8\n\njulia> bandwidth(A_shuffled) # Much larger after shuffling\n25\n\njulia> minimize_bandwidth(A_shuffled, Minimization.DelCorsoManziniWithPS(depth))\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Del Corso–Manzini with perimeter search\n * Approach: exact\n * Minimum Bandwidth: 8\n * Original Bandwidth: 25\n * Matrix Size: 30×30\n\nNotes\n\nFor readers of the original paper, what we call the Del Corso–Manzini minimization algorithm with perimeter search here is designated the \"MB-PS algorithm\" in [DM99, p. 193]. The so-called \"MB-ID algorithm,\" on the other hand, we implement in DelCorsoManzini.\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Minimization.Exact.SaxeGurariSudborough","page":"Public API","title":"MatrixBandwidth.Minimization.Exact.SaxeGurariSudborough","text":"SaxeGurariSudborough <: ExactSolver <: AbstractSolver <: AbstractAlgorithm\n\nTODO: Write here\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Minimization.Heuristic","page":"Public API","title":"MatrixBandwidth.Minimization.Heuristic","text":"MatrixBandwidth.Minimization.Heuristic\n\nHeuristic solvers for matrix bandwidth minimization.\n\nHeuristic methods are those which aim to produce near-optimal solutions in a more performant manner than exact methods. While precise bandwidth minimization is NP-complete, many heuristic algorithms (such as Gibbs–Poole–Stockmeyer) run in polynomial time.\n\nHeuristic algorithms differ from metaheuristic ones in that they do not employ higher-level iterative search frameworks (e.g., stochastic techniques) to survey the global search space and escape local minima; instead, they rely on straightforward deterministic procedures to find good solutions in a single pass.\n\nThe following heuristic algorithms are currently supported:\n\nGibbs–Poole–Stockmeyer algorithm (GibbsPooleStockmeyer)\nCuthill–McKee algorithm (CuthillMcKee)\nReverse Cuthill–McKee algorithm (ReverseCuthillMcKee)\n\nThis submodule is part of the MatrixBandwidth.Minimization submodule of the MatrixBandwidth.jl package.\n\n\n\n\n\n","category":"module"},{"location":"public_api/#MatrixBandwidth.Minimization.Heuristic.CuthillMcKee","page":"Public API","title":"MatrixBandwidth.Minimization.Heuristic.CuthillMcKee","text":"CuthillMcKee <: HeuristicSolver <: AbstractSolver <: AbstractAlgorithm\n\nThe Cuthill–McKee algorithm is a heuristic method for minimizing the bandwidth of a structurally symmetric matrix A. It considers the graph G(A) whose adjacency matrix is A (ignoring weights and self-loops) and performs a breadth-first search of each connected component of G(A), starting from a low-degree node then visiting its neighbors in order of increasing degree. Particularly effective when A is sparse, this heuristic typically produces an ordering which induces a matrix bandwidth either equal to or very close to the true minimum [CM69, pp. 157–58].\n\nAs noted above, the Cuthill–McKee algorithm requires structurally symmetric input (that is, Ai j must be nonzero if and only if Aj i is nonzero for 1  i j  n).\n\nFields\n\nnode_selector::Function: a function that selects a node from some connected component of   the input matrix from which to start the breadth-first search. If no custom heuristic is   specified, this field defaults to pseudo_peripheral_node, which picks a node   \"farthest\" from the others in the component (not necessarily the lowest-degree node).\n\nPerformance\n\nGiven an nn input matrix A, the Cuthill–McKee algorithm runs in O(n²) time.\n\n[CG80] provide a linear-time implementation in the number of nonzero entries of A, which is still quadratic when A is dense but often much faster when dealing with sparse matrices. However, this would require that A be stored as a graph or a sparse matrix, which runs counter to our desire to provide a bandwidth minimization API for all AbstractMatrix{<:Number} types, including dense matrices. (In the future, however, we may indeed consider supporting this more performant implementation for sparse matrices.)\n\nIt was found in [Geo71, pp. 114–15] that reversing the ordering produced by Cuthill–McKee tends to induce a more optimal matrix profile (a measure of how far, on average, nonzero entries are from the diagonal; see also MatrixBandwidth.profile). This so-called reverse Cuthill–McKee variant is preferred in almost all cases—see ReverseCuthillMcKee and the associated method of _bool_minimal_band_ordering for     our implementation.\n\nExamples\n\nIn the following examples, MatrixBandwidth.random_banded_matrix is used to generate random matrices with minimum bandwidth close to k. In some cases, however, the true minimum bandwidth up to symmetric permutation may be even less than k, making it hard to verify whether Cuthill–McKee finds a truly optimal ordering or simply a near-optimal one. Nevertheless, the results are still very good in practice.\n\nCuthill–McKee finds a good ordering for a 3030 matrix:\n\njulia> using Random\n\njulia> Random.seed!(13);\n\njulia> (n, k) = (30, 5);\n\njulia> A = random_banded_matrix(n, k);\n\njulia> perm = randperm(n);\n\njulia> A_shuffled = A[perm, perm];\n\njulia> bandwidth(A)\n5\n\njulia> bandwidth(A_shuffled) # Much larger after shuffling\n25\n\njulia> minimize_bandwidth(A_shuffled, Minimization.CuthillMcKee())\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Cuthill–McKee\n * Approach: heuristic\n * Minimum Bandwidth: 5\n * Original Bandwidth: 25\n * Matrix Size: 30×30\n\nCuthill–McKee finds a good ordering for a structurally symmetric 183183 matrix with multiple (separate) connected components:\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(37452);\n\njulia> (max_cc_size, max_band, p, num_ccs) = (60, 9, 0.2, 7);\n\njulia> components = Vector{SparseMatrixCSC{Float64, Int64}}(undef, num_ccs);\n\njulia> for i in 1:num_ccs # Some components may themselves be disconnected\n           cc_size = rand(1:max_cc_size);\n           cc_band = rand(0:min(max_band, cc_size - 1));\n           components[i] = sparse(random_banded_matrix(cc_size, cc_band; p=p));\n       end\n\njulia> A = blockdiag(components...); # `A` has least 7 connected components\n\njulia> perm = randperm(sum(map(cc -> size(cc, 1), components)));\n\njulia> A_shuffled = A[perm, perm];\n\njulia> res = minimize_bandwidth(A_shuffled, Minimization.CuthillMcKee());\n\njulia> A # The original matrix\n276×276 SparseMatrixCSC{Float64, Int64} with 464 stored entries:\n⎡⢾⡷⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠘⢻⣲⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠘⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠈⠿⡧⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠉⢯⡷⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠚⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⢻⣶⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠯⡧⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣤⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠛⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠱⣢⡀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⡢⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢴⣷⡀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠿⣧⡀⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⡷⎦\n\njulia> A_shuffled # A far-from-optimal ordering of `A`\n276×276 SparseMatrixCSC{Float64, Int64} with 464 stored entries:\n⎡⠁⢄⠀⢀⠀⠀⠀⢀⠠⠀⠀⠐⠀⠀⠀⠐⢀⡐⠀⠀⠀⢀⠀⠀⠀⠀⠐⠀⢠⠀⠀⠀⡄⠀⠀⠐⠀⠀⠂⠄⎤\n⎢⠀⢀⠱⠂⠀⠀⠀⠈⠀⠀⠀⠀⠀⠀⢨⠀⠀⠀⠀⠀⡀⠁⠠⠀⠘⠀⠀⠡⢀⠈⠀⠀⠀⠀⠀⠀⠄⠀⠁⠁⎥\n⎢⠀⠀⠀⠀⠑⢀⠀⠂⠀⠀⠀⠀⢐⠀⠀⠠⠈⠠⠀⠀⠀⠐⠀⠐⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⢢⢀⢀⠀⎥\n⎢⠀⢀⡀⠀⠠⠀⠁⠄⠀⠠⠀⠄⠀⠀⠀⠄⠀⠀⠀⠀⢀⠀⠀⢀⠀⠑⠀⠀⠐⠠⠀⠀⠠⠨⠂⠀⠀⠀⠀⠀⎥\n⎢⠀⠂⠀⠀⠀⠀⠀⡀⠱⢆⡀⠂⠀⠀⠀⠀⠀⠀⢀⢊⠀⠐⠐⠈⠀⠈⠀⢀⠄⠀⡀⠀⢁⢀⠠⠀⠃⠀⠊⠀⎥\n⎢⢀⠀⠀⠀⠀⠀⠀⠄⠠⠈⠑⠀⢀⠐⠀⠌⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀⠀⢀⠉⢀⠀⠠⠈⠀⠀⣁⠁⎥\n⎢⠀⠀⠀⠀⠐⠐⠀⠀⠀⠀⢀⠐⠁⠄⠈⠀⢌⠀⠆⠠⢀⠀⠄⠐⠰⠀⠀⠀⠁⠰⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⢀⠀⠂⠒⠀⡀⠀⠄⠀⠀⡀⠄⠂⠀⠐⢄⠁⢀⠀⠀⠀⡀⠀⠀⠀⠀⡠⠀⠀⠀⠀⠀⠀⠀⠀⢈⠀⠀⠀⠁⎥\n⎢⢀⠰⠀⠀⠂⡀⠀⠀⠀⠀⠀⠈⠂⠑⠁⢀⠐⠄⠄⠂⠂⠜⠄⠀⠀⠀⡄⠀⠀⢀⠀⠠⠀⢀⠄⠀⢀⠀⠂⡂⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⡠⢐⠀⠀⠈⡁⠀⠀⠠⠁⠀⢀⠀⠀⠀⠀⡀⠀⠀⢀⠀⠈⠃⠀⠸⠈⠠⠀⠀⠀⢄⠂⎥\n⎢⠀⢀⠄⠈⢀⠀⠀⠐⢀⠀⠀⠀⠀⠐⠀⠠⣈⠄⠀⠀⠐⢀⠀⡀⠀⠀⠀⠀⠀⠀⠐⠀⠀⠊⠀⠠⠀⠐⠀⠀⎥\n⎢⠀⠀⠀⠂⢀⠀⠀⢀⡐⠀⠀⠀⢀⠁⠀⠀⠀⠁⠀⠀⠀⠠⠄⣥⠉⠈⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⡀⠀⠀⠀⎥\n⎢⠀⠀⠒⠀⠀⠀⢄⠀⡀⠀⠀⠀⠐⠂⠀⠀⠀⠀⠀⠈⠀⠀⡃⠀⠁⢀⠀⠀⢀⡀⢈⠈⠀⠀⠀⠂⠀⠠⠂⠂⎥\n⎢⠐⠀⠄⡀⠀⠀⠀⠀⠀⢀⠀⠈⠀⠀⠀⠊⠀⠉⠀⢀⠀⠀⠀⠀⠀⠀⠑⠀⠀⠀⠀⠀⢀⠀⠈⠀⠛⠃⢄⠀⎥\n⎢⠀⠒⡀⠐⠀⠀⠐⡀⠀⠁⠀⠀⢁⡀⠀⠀⠀⢀⡀⠀⠀⠀⠀⠀⠀⠰⠀⠀⠀⢄⠀⠰⠀⠠⠠⢀⠀⠀⢂⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⡄⠐⠂⠀⠀⠀⠀⡀⠉⠀⠐⠀⠀⠈⡂⠐⠀⠀⢀⡀⠀⣠⠀⠄⠠⠀⠀⡀⠀⠀⎥\n⎢⠀⠉⠀⠀⠀⠀⡀⡂⠁⢐⠀⠐⠀⠀⠀⠀⠀⢀⡒⠂⡠⠀⠀⠀⠀⠀⠀⠐⠀⡀⠀⠄⠑⠄⠀⠀⠀⠀⠀⠀⎥\n⎢⢀⠀⠀⠀⠀⡀⠈⠀⠀⠂⡀⠂⠀⠀⡀⢀⠀⠁⠀⠂⠀⡀⠀⠀⠠⠀⠂⠀⠀⢂⠀⠂⠀⠀⠁⢀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠁⠈⢒⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠐⠀⠀⢀⠀⠀⠈⠀⡀⠿⠀⠀⠀⠀⠠⠀⠀⠀⠀⠱⠆⠀⠀⎥\n⎣⠈⠄⠅⠀⠀⠐⠀⠀⠊⠀⠅⠘⠀⠀⠄⠀⠨⠠⠠⠑⠀⠀⠀⠀⠨⠀⠀⠑⠈⠐⠀⠀⠀⠀⠀⠀⠀⠀⠔⢅⎦\n\njulia> A_shuffled[res.ordering, res.ordering] # A near-optimal reordering of `A_shuffled`\n276×276 SparseMatrixCSC{Float64, Int64} with 464 stored entries:\n⎡⠱⣦⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠉⠻⣦⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠘⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⡦⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⡦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠺⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠚⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢄⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢄⠀⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⎦\n\njulia> bandwidth(A)\n7\n\njulia> bandwidth(A_shuffled) # Much larger after shuffling\n266\n\njulia> res # Even better than the original bandwidth (which was, clearly, not yet optimal)\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Cuthill–McKee\n * Approach: heuristic\n * Minimum Bandwidth: 5\n * Original Bandwidth: 266\n * Matrix Size: 276×276\n\nNotes\n\nNote that the node_selector field must be of the form (A::AbstractMatrix{Bool}) -> Integer (i.e., it must take in an boolean matrix and return an integer). If this is not the case, an ArgumentError is thrown upon construction.\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Minimization.Heuristic.GibbsPooleStockmeyer","page":"Public API","title":"MatrixBandwidth.Minimization.Heuristic.GibbsPooleStockmeyer","text":"GibbsPooleStockmeyer <: HeuristicSolver <: AbstractSolver <: AbstractAlgorithm\n\nThe Gibbs–Poole–Stockmeyer algorithm is a heuristic method for minimizing the bandwidth of a structurally symmetric matrix A. It considers the graph G(A) whose adjacency matrix is A (ignoring weights and self-loops) and builds an ordering by identifying a pair of \"endpoints\" in the graph far from each other, constructing sets of levels from these endpoints, and merging these level structures in such a way that minimizes the size of the largest level in the final combined structure. Based on the classical reverse Cuthill–McKee algorithm [Geo71], this heuristic typically produces an ordering which induces a matrix bandwidth either equal to or very close to the true minimum, with improvements in bandwidths over reverse Cuthill–McKee more noticeable once input size exceeds 400400 or so [GPS76, pp. 246–47].\n\nWhereas the original paper outlined a strategy for conditionally reversing the orderings of individual \"connected components\" [GPS76, p. 241] (treating the input matrix A as an undirected graph), this implementation instead reverses the entire final ordering in every case, similarly to ReverseCuthillMcKee. Conditional reversals are not only more complex to implement but also slightly more time-consuming, with the only benefit being a marginally smaller matrix profile (a measure of how far, on average, nonzero entries are from the diagonal; see also MatrixBandwidth.profile). Since such reversal strategies do not affect matrix bandwidth (the primary focus of this package), we opt instead for the simpler unconditional reversal.\n\nAs noted above, the Gibbs–Poole–Stockmeyer algorithm requires structurally symmetric input (that is, Ai j must be nonzero if and only if Aj i is nonzero for 1  i j  n).\n\nFields\n\nnode_selector::Function: a function that selects a node from some connected component of   the input matrix from which to start the breadth-first search. If no custom heuristic is   specified, this field defaults to pseudo_peripheral_node, which picks a node   \"farthest\" from the others in the component (not necessarily the lowest-degree node).\n\nPerformance\n\nGiven an nn input matrix A, the Gibbs–Poole–Stockmeyer algorithm runs in O(n²) time.\n\n[Lew82] provides a notably faster and more memory-efficient implementation, relying on sparse storage of the input matrix. However, this would run counter to our desire to provide a bandwidth minimization API for all AbstractMatrix{<:Number} types, including dense matrices. (In the future, however, we may indeed consider supporting this more performant implementation for sparse matrices.)\n\nOn that note, Gibbs–Poole–Stockmeyer has been found to take considerably less time than reverse Cuthill–McKee when matrices are stored in sparse format [GPS76, pp. 246–47]. The dense-matrix implementations of both algorithms in this package, however, result in reverse Cuthill–McKee consistently outperforming Gibbs–Poole–Stockmeyer in terms of runtime (although Gibbs–Poole–Stockmeyer still typically produces lower-bandwidth orderings for larger matrices). This further motivates the desire to implement a sparse version of both algorithms in the future.\n\nExamples\n\nIn the following examples, MatrixBandwidth.random_banded_matrix is used to generate random matrices with minimum bandwidth close to k. In some cases, however, the true minimum bandwidth up to symmetric permutation may be even less than k, making it hard to verify whether Gibbs–Poole–Stockmeyer finds a truly optimal ordering or simply a near-optimal one. Nevertheless, the results are still very good in practice.\n\nGibbs–Poole–Stockmeyer finds a good ordering for a 4040 matrix:\n\njulia> using Random\n\njulia> Random.seed!(561);\n\njulia> (n, k) = (40, 7);\n\njulia> A = random_banded_matrix(n, k);\n\njulia> perm = randperm(n);\n\njulia> A_shuffled = A[perm, perm];\n\njulia> bandwidth(A)\n7\n\njulia> bandwidth(A_shuffled)\n37\n\njulia> minimize_bandwidth(A_shuffled, Minimization.GibbsPooleStockmeyer())\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Gibbs–Poole–Stockmeyer\n * Approach: heuristic\n * Minimum Bandwidth: 7\n * Original Bandwidth: 37\n * Matrix Size: 40×40\n\nGibbs–Poole–Stockmeyer finds a good ordering for a 748748 matrix with multiple (separate) connected components:\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(271828);\n\njulia> (max_cc_size, max_band, p, num_ccs) = (120, 13, 0.3, 11);\n\njulia> components = Vector{SparseMatrixCSC{Float64, Int64}}(undef, num_ccs);\n\njulia> for i in 1:num_ccs # Some components may themselves be disconnected\n           cc_size = rand(0:max_cc_size);\n           cc_band = rand(1:min(max_band, cc_size - 1));\n           components[i] = sparse(random_banded_matrix(cc_size, cc_band; p=p));\n       end\n\njulia> A = blockdiag(components...); # `A` has least 8 connected components\n\njulia> perm = randperm(sum(map(cc -> size(cc, 1), components)));\n\njulia> A_shuffled = A[perm, perm];\n\njulia> res = minimize_bandwidth(A_shuffled, Minimization.GibbsPooleStockmeyer());\n\njulia> A # The original matrix\n748×748 SparseMatrixCSC{Float64, Int64} with 2526 stored entries:\n⎡⢿⣷⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠘⠿⣧⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠘⢿⣷⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠉⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⎦\n\njulia> A_shuffled # A far-from-optimal ordering of `A`\n748×748 SparseMatrixCSC{Float64, Int64} with 2526 stored entries:\n⎡⠰⣦⢎⢪⢐⠆⣗⣔⠆⠀⠀⠠⢃⡦⠵⠸⡐⢌⠴⠤⣤⢅⢒⠰⡢⡄⡰⣄⢂⢊⠎⠀⢝⡀⣼⠤⠅⢒⠰⠢⎤\n⎢⡪⣑⡋⢌⠈⢀⣖⣠⢠⡀⡂⠀⠐⠤⣠⠰⣃⠀⢊⢃⡨⡇⡝⠢⣀⠈⢬⢁⠽⡼⠐⡦⠤⠐⠤⠚⠪⢠⠦⢈⎥\n⎢⠰⠔⠂⢀⡑⢌⠐⠀⠐⡒⠤⡅⠂⡱⡕⠈⡑⠹⠱⣨⢓⣑⠀⠂⠀⢬⢂⣅⡤⢑⠢⢁⠀⢒⡑⡬⡆⠚⠀⡀⎥\n⎢⢙⢽⠘⣹⠐⠀⢱⣶⠂⠂⠈⡵⢰⡤⠖⠰⡂⣠⢨⢰⣛⠀⠰⡌⡐⠄⠠⡦⠜⠃⠐⣦⢂⠄⠷⢅⣉⠰⠿⢴⎥\n⎢⠈⠁⠀⠲⢰⠠⠨⠀⣑⣼⠩⡃⠂⢰⢁⠓⢐⣘⣂⢦⠂⡐⠔⢄⡨⣃⠦⢁⢈⠀⡂⠀⡅⠈⠀⢌⠀⡀⠵⠫⎥\n⎢⠀⡀⠈⠈⠄⠧⢆⡤⠧⠢⠕⢅⡍⢔⡴⠀⡀⠆⡨⠈⣄⠲⠌⠳⢁⠐⠠⠴⠀⠄⠔⠺⠁⡉⢂⣭⠠⡠⠀⣉⎥\n⎢⠩⡴⠐⡄⢌⡠⠐⡶⢈⣀⢃⢍⠋⢄⡖⢩⢌⣖⠈⢀⣴⣙⡀⢓⠁⠠⢬⢈⠅⡤⠅⢰⣁⠌⣌⠆⠸⢀⠒⣁⎥\n⎢⣑⡃⢀⡚⡑⠉⢘⡁⢥⠐⠐⠋⡜⣉⠑⢄⠍⣢⣓⢉⠊⢖⠀⢐⡀⠲⡈⢑⠀⠊⠀⠛⠀⢃⠘⠀⢁⣒⢀⢁⎥\n⎢⡐⢌⠉⠘⣕⡈⠈⣨⣐⢰⠠⠌⢢⢵⠣⣡⢕⣱⣂⢭⢎⠜⠀⠉⡂⣃⢐⠅⠒⠔⡂⠨⡂⢳⠍⢤⠰⡠⣩⡏⎥\n⎢⠐⡇⠮⢐⡑⣢⢂⣒⠨⣜⡂⠊⠂⢀⡝⢘⡌⣜⣱⢞⠂⠾⠊⢔⠙⣢⢭⣹⠑⡑⠀⡉⡄⠁⠀⠴⣇⣃⠃⡑⎥\n⎢⠄⢟⠦⠮⢝⢰⠛⠘⢈⠠⢠⡙⣔⢻⢪⢄⣊⠕⣨⡄⢕⣵⢐⠊⠊⣱⡠⣢⠀⠣⠀⣍⠔⠀⢝⢄⣌⡄⡌⠆⎥\n⎢⢘⡐⠳⡉⠠⠀⡐⠦⠐⢅⢦⡁⢤⢈⢀⢀⡄⠀⢊⢄⡰⠐⠑⢄⠴⡅⠁⢆⡠⣄⠤⢐⠈⡀⠺⠂⠀⢀⡴⡀⎥\n⎢⠈⠮⡀⠘⡀⣄⠐⠌⠦⢪⢁⠐⠁⡀⢠⡈⠬⢨⠳⣠⢎⣠⠔⠧⠛⢄⡀⡹⠡⠌⠃⢀⠣⠁⠉⢀⠑⠁⢦⡉⎥\n⎢⠐⢮⠆⢓⠌⢴⠠⡦⠌⢃⢀⡆⡂⢓⢆⢈⠔⠔⣇⣳⠠⣪⠡⢄⣄⡨⠛⢄⠑⢐⠐⡠⠪⠃⢤⢁⡝⠐⡀⢎⎥\n⎢⡨⢐⣓⡧⢄⢋⠶⠁⠂⠐⠀⠄⠁⡥⡠⠀⢘⠄⢕⠠⠤⡀⠀⢮⡁⠆⢑⢀⠕⣥⣑⠼⡀⡅⠝⠢⠠⠄⠀⠔⎥\n⎢⠊⠁⠰⡤⠌⢂⠰⣤⠈⠈⣰⡁⢁⣁⣤⠀⡈⡈⡄⠠⡄⢤⢀⢃⠉⢀⠐⡠⣑⡜⠑⢄⠀⡼⠠⠲⢈⠠⢃⠁⎥\n⎢⠓⠱⢀⠃⢠⢀⠈⠔⡁⠉⡅⠠⡁⠜⠤⢀⢬⣈⠄⠉⠐⠁⠂⠠⠍⠂⠮⠂⠄⠬⣀⡤⠵⢇⠈⠀⠣⠡⠩⡎⎥\n⎢⠒⡟⣠⠃⡑⡬⠝⢇⡀⢄⡌⣴⠢⠝⠒⠀⠃⣅⢀⡄⠓⢕⠺⠂⠃⢀⠄⢓⠳⡁⢠⡂⠂⠀⠛⢄⡀⢒⠒⠂⎥\n⎢⢡⢁⠊⣂⣨⠉⢃⡘⠀⠠⠀⡢⠒⢂⢡⢰⠐⡢⠭⢹⠂⠽⠀⢀⠕⠀⢓⠉⠀⠆⠂⡐⠍⡂⢠⢈⠁⣤⢀⡈⎥\n⎣⠰⡂⡈⢃⠀⠠⢛⣇⡵⡃⡄⢠⠜⢠⠄⢐⡧⠾⢍⠠⠢⠍⠐⠫⡌⠳⡠⢌⢀⠄⠍⠐⡣⠦⠸⠀⡀⠰⡛⢌⎦\n\njulia> A_shuffled[res.ordering, res.ordering] # A near-optimal reordering of `A_shuffled`\n748×748 SparseMatrixCSC{Float64, Int64} with 2526 stored entries:\n⎡⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠑⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠱⢆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠱⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢻⣶⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣤⣀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢻⣶⡀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣤⡀⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⎦\n\njulia> bandwidth(A)\n12\n\njulia> bandwidth(A_shuffled) # Much larger after shuffling\n731\n\njulia> res # Gets very close to the original bandwidth\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Gibbs–Poole–Stockmeyer\n * Approach: heuristic\n * Minimum Bandwidth: 18\n * Original Bandwidth: 731\n * Matrix Size: 748×748\n\nNotes\n\nNote that the node_selector field must be of the form (A::AbstractMatrix{Bool}) -> Integer (i.e., it must take in an boolean matrix and return an integer). If this is not the case, an ArgumentError is thrown upon construction.\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Minimization.Heuristic.ReverseCuthillMcKee","page":"Public API","title":"MatrixBandwidth.Minimization.Heuristic.ReverseCuthillMcKee","text":"ReverseCuthillMcKee <: HeuristicSolver <: AbstractSolver <: AbstractAlgorithm\n\nThe reverse Cuthill–McKee algorithm is a variant of the Cuthill–McKee algorithm—a heuristic method for minimizing the bandwidth of a structurally symmetric matrix A. Cuthill–McKee considers the graph G(A) whose adjacency matrix is A (ignoring weights and self-loops) and performs a breadth-first search of each connected component of G(A), starting from a low-degree node then visiting its neighbors in order of increasing degree. Particularly effective when A is sparse, this heuristic typically produces an ordering which induces a matrix bandwidth either equal to or very close to the true minimum [CM69, pp. 157–58]. The reverse Cuthill–McKee algorithm simply reverses the ordering produced by application of Cuthill–McKee; it was found in [Geo71, pp. 114–15] that although the bandwidth remains the same, this tends to produce a more optimal matrix profile (a measure of how far, on average, nonzero entries are from the diagonal; see also MatrixBandwidth.profile).\n\nAs noted above, the reverse Cuthill–McKee algorithm requires structurally symmetric input (that is, Ai j must be nonzero if and only if Aj i is nonzero for 1  i j  n).\n\nPerformance\n\nGiven an nn input matrix A, the reverse Cuthill–McKee algorithm runs in O(n²) time.\n\n[CG80] provide a linear-time implementation in the number of nonzero entries of A, which is still quadratic when A is dense but often much faster when dealing with sparse matrices. However, this would require that A be stored as a graph or a sparse matrix, which runs counter to our desire to provide a bandwidth minimization API for all AbstractMatrix{<:Number} types, including dense matrices. (In the future, however, we may indeed consider supporting this more performant implementation for sparse matrices.)\n\nFields\n\nnode_selector::Function: a function that selects a node from some connected component of   the input matrix from which to start the breadth-first search. If no custom heuristic is   specified, this field defaults to pseudo_peripheral_node, which picks a node   \"farthest\" from the others in the component (not necessarily the lowest-degree node).\n\nExamples\n\nIn the following examples, MatrixBandwidth.random_banded_matrix is used to generate random matrices with minimum bandwidth close to k. In some cases, however, the true minimum bandwidth up to symmetric permutation may be even less than k, making it hard to verify whether reverse Cuthill–McKee finds a truly optimal ordering or simply a near-optimal one. Nevertheless, the results are still very good in practice.\n\nReverse Cuthill–McKee finds a good ordering for a 3535 matrix:\n\njulia> using Random\n\njulia> Random.seed!(87);\n\njulia> (n, k) = (35, 3);\n\njulia> A = random_banded_matrix(n, k);\n\njulia> perm = randperm(n);\n\njulia> A_shuffled = A[perm, perm];\n\njulia> bandwidth(A)\n3\n\njulia> bandwidth(A_shuffled) # Much larger after shuffling\n30\n\njulia> minimize_bandwidth(A_shuffled, Minimization.ReverseCuthillMcKee())\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Reverse Cuthill–McKee\n * Approach: heuristic\n * Minimum Bandwidth: 3\n * Original Bandwidth: 30\n * Matrix Size: 35×35\n\nReverse Cuthill–McKee finds a good ordering for a 235235 matrix with multiple (separate) connected components:\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(5747);\n\njulia> (max_cc_size, max_band, p, num_ccs) = (60, 9, 0.2, 8);\n\njulia> components = Vector{SparseMatrixCSC{Float64, Int64}}(undef, num_ccs);\n\njulia> for i in 1:num_ccs # Some components may themselves be disconnected\n           cc_size = rand(0:max_cc_size);\n           cc_band = rand(1:min(max_band, cc_size - 1));\n           components[i] = sparse(random_banded_matrix(cc_size, cc_band; p=p));\n       end\n\njulia> A = blockdiag(components...); # `A` has least 8 connected components\n\njulia> perm = randperm(sum(map(cc -> size(cc, 1), components)));\n\njulia> A_shuffled = A[perm, perm];\n\njulia> res = minimize_bandwidth(A_shuffled, Minimization.ReverseCuthillMcKee());\n\njulia> A # The original matrix\n235×235 SparseMatrixCSC{Float64, Int64} with 445 stored entries:\n⎡⢾⣳⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠘⢿⡷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠈⠏⣥⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠉⢴⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠙⠻⢂⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣮⣿⣢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠚⢿⡳⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⢰⣶⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠾⡧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⣢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⡠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⠖⣀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢏⡱⣄⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢮⣷⣄⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢻⣲⣄⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢻⢖⎦\n\njulia> A_shuffled # A far-from-optimal ordering of `A`\n235×235 SparseMatrixCSC{Float64, Int64} with 445 stored entries:\n⎡⠑⠄⠀⠀⠀⠀⠀⢀⢀⠀⠄⠀⠀⠀⠀⠀⠀⠀⠀⠄⠀⡐⠀⠐⠀⠂⠀⠀⠀⠀⠀⠀⡂⠀⢀⠄⠁⠠⠐⠀⎤\n⎢⠀⠀⠀⢄⡀⠀⢁⠀⠀⠈⠀⠁⠀⠀⠀⢀⠁⠄⠈⠀⠀⠀⠀⠐⠀⠀⠀⠀⠀⠈⠀⠂⠠⠀⠀⠀⠀⠀⡀⠐⎥\n⎢⠀⠀⠀⠈⠁⢀⠀⠑⠀⢀⠁⢀⠀⠈⠀⠘⠌⠀⢀⠀⠄⠀⠂⡄⠄⠁⠀⠀⠈⠀⠀⠀⠀⠀⠀⠀⣂⠀⠀⠀⎥\n⎢⠀⢀⠁⠐⢄⠀⠠⢆⠀⠀⠀⠀⠀⠀⠀⢀⠠⡀⠀⠀⠠⠀⠀⠀⠐⠀⠀⡀⠀⢀⠀⠀⠀⠈⠀⡀⠀⠀⠘⠀⎥\n⎢⠀⠐⡀⠀⠀⢀⠀⠀⢀⢔⠈⢀⠀⠀⣐⠀⠀⠀⢀⠀⠀⠀⠀⠀⠐⠀⠄⢠⠀⠀⠀⠀⠀⠀⠀⡀⠀⠈⠣⡀⎥\n⎢⠀⠁⠄⠀⠁⢀⠀⠀⠂⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⠀⡀⠀⠀⡐⠐⠀⡀⠀⠀⠂⠀⠀⠀⢀⠀⠀⠄⠀⎥\n⎢⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⠀⠀⠑⠀⠀⠀⠀⠀⠀⠀⡠⠀⡀⠀⠀⠄⠀⠀⠠⠀⠀⠠⠀⠀⠀⠀⡠⠄⠀⠄⎥\n⎢⠀⠀⠀⢀⣀⠀⠀⢀⠐⠘⠀⠀⠀⠀⠕⢅⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀⢀⠐⡀⠀⠈⠀⠂⠀⠀⢀⠀⠃⠀⠄⎥\n⎢⠀⠀⠁⠄⠂⠁⠀⠢⠀⠀⠀⠀⠀⠀⠀⠀⠛⢄⢸⠘⠀⠀⠀⠀⠄⠈⠁⠀⠀⠨⠀⠀⢀⠀⠀⠨⠀⠀⠈⠀⎥\n⎢⠀⠄⠂⠀⠀⠐⠀⠀⠀⠐⠀⠀⠀⠀⠀⠀⣒⠒⠁⠀⢠⠀⠀⠀⠐⠀⠀⠀⢁⠀⠐⠈⠀⠀⠂⠀⠂⡀⠀⠀⎥\n⎢⢀⠠⠀⠀⠀⠁⠀⠂⠀⠀⠀⠂⠀⠊⠀⠀⠀⠀⠀⠒⠑⠀⠀⠀⠀⠂⢀⠁⡂⠀⠀⢀⠀⠀⠀⠀⠁⠂⡊⠂⎥\n⎢⢀⠀⢀⠀⠈⠤⠀⠀⠀⠀⠀⠈⠀⠈⠀⠠⠀⠀⠀⠀⠀⠀⠁⢀⠅⠀⢀⠀⠀⠀⢀⠀⡁⠀⠀⠀⠀⠠⠀⠀⎥\n⎢⠠⠀⠀⠀⠄⠁⠐⠀⠐⠀⢀⠠⠀⠄⠀⠀⡀⠁⠐⠀⠠⠀⠁⠁⠁⠀⢀⠄⠀⠉⠀⠃⠀⠀⠀⠀⠠⢠⠂⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠠⠀⣁⠐⠀⠀⠀⢀⠐⠁⠀⠀⠀⠄⠐⠀⠐⠀⠔⡑⠌⠀⠀⠀⠀⠢⢉⠀⠀⠄⠀⠀⠄⎥\n⎢⠀⠀⡀⠀⠂⠀⠀⢀⠀⠀⠀⠈⠀⠂⠀⠈⡀⡀⠁⠐⠈⠈⠀⠀⡄⠀⠀⠀⠄⠅⠀⡀⠀⠀⠀⠠⠀⠰⠀⠂⎥\n⎢⠀⠀⠠⠀⠀⠀⠀⠀⠀⠀⠠⠀⠀⡀⠂⠀⠀⠀⡐⠀⠀⢀⠀⠐⠤⠀⠀⠀⠀⠠⠀⢀⠀⠀⠀⠀⠀⠀⠒⢀⎥\n⎢⠈⠈⠀⠂⠀⠀⡀⠀⠀⠀⠀⠀⠀⠀⠈⠀⠀⠐⠀⠀⠀⠀⠁⠈⠀⠀⡌⢂⠀⠀⠀⠀⠀⠄⠈⠀⠀⡀⠀⠀⎥\n⎢⠀⠔⠀⠀⠀⠀⠀⠠⠀⠠⠀⢀⠀⠀⠀⢀⡀⡀⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀⠂⠀⠡⠂⠀⠀⡠⠀⎥\n⎢⠁⡀⠀⠀⠈⠘⠀⠀⡀⠀⠀⠀⠀⠎⠤⠀⠀⠀⠈⠠⠡⠀⠀⡀⠀⣂⠀⠁⢀⡀⠀⠀⠀⠠⠀⠀⠰⠆⠌⠀⎥\n⎣⠐⠀⢀⠈⠀⠀⠒⠀⠉⠢⠀⠁⠀⠄⠀⠄⠂⠀⠀⠀⠪⠈⠀⠀⠈⠀⠀⠄⠠⠀⠘⢀⠀⠀⠀⠊⠂⠁⠐⢀⎦\n\njulia> A_shuffled[res.ordering, res.ordering] # A near-optimal reordering of `A_shuffled`\n235×235 SparseMatrixCSC{Float64, Int64} with 445 stored entries:\n⎡⠁⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠀⠁⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠈⠀⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠁⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠑⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢛⢔⢤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠳⣿⣿⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠚⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠡⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠫⣦⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢿⣷⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠯⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠯⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠺⢆⡄⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⢻⣲⣄⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢻⣶⡀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣢⡀⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠱⣦⎦\n\njulia> bandwidth(A)\n9\n\njulia> bandwidth(A_shuffled) # Much larger after shuffling\n226\n\njulia> res # Gets very close to the original bandwidth\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Reverse Cuthill–McKee\n * Approach: heuristic\n * Minimum Bandwidth: 9\n * Original Bandwidth: 226\n * Matrix Size: 235×235\n\nNotes\n\nNote that the node_selector field must be of the form (A::AbstractMatrix{Bool}) -> Integer (i.e., it must take in an boolean matrix and return an integer). If this is not the case, an ArgumentError is thrown upon construction.\n\nSee also the documentation for CuthillMcKee—the original (non-reversed) algorithm. (Indeed, the reverse Cuthill–McKee method of _bool_minimal_band_ordering is merely a wrapper around the Cuthill–McKee method.)\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Minimization.Metaheuristic","page":"Public API","title":"MatrixBandwidth.Minimization.Metaheuristic","text":"MatrixBandwidth.Minimization.Metaheuristic\n\nMetaheuristic solvers for matrix bandwidth minimization.\n\nMetaheuristic methods are those which employ higher-level iterative search frameworks such as stochastic techniques or nature-inspired processes to survey the global search space and escape local minima. Unlike heuristic methods—which follow fixed deterministic procedures—metaheuristics adaptively refine candidate solutions over multiple iterations. Although metaheuristic approaches are often slower than heuristic ones (but certainly still faster than exact ones), they shine in complex cases where the latter may get trapped in poor-quality local minima.\n\nThe following metaheuristic algorithms are currently supported:\n\nGreedy randomized adaptive search procedure (GRASP) (GRASP)\nSimulated annealing (SimulatedAnnealing)\nGenetic algorithm (GeneticAlgorithm)\n\nThis submodule is part of the MatrixBandwidth.Minimization submodule of the MatrixBandwidth.jl package.\n\n\n\n\n\n","category":"module"},{"location":"public_api/#MatrixBandwidth.Minimization.Metaheuristic.GRASP","page":"Public API","title":"MatrixBandwidth.Minimization.Metaheuristic.GRASP","text":"GRASP <: MetaheuristicSolver <: AbstractSolver <: AbstractAlgorithm\n\n[TODO: Write here]\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Minimization.Metaheuristic.GeneticAlgorithm","page":"Public API","title":"MatrixBandwidth.Minimization.Metaheuristic.GeneticAlgorithm","text":"GeneticAlgorithm <: MetaheuristicSolver <: AbstractSolver <: AbstractAlgorithm\n\n[TODO: Write here]\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Minimization.Metaheuristic.SimulatedAnnealing","page":"Public API","title":"MatrixBandwidth.Minimization.Metaheuristic.SimulatedAnnealing","text":"SimulatedAnnealing <: MetaheuristicSolver <: AbstractSolver <: AbstractAlgorithm\n\n[TODO: Write here]\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Recognition","page":"Public API","title":"MatrixBandwidth.Recognition","text":"MatrixBandwidth.Recognition\n\nAlgorithms for matrix bandwidth recognition in Julia.\n\nThe bandwidth of an nn matrix A is the minimum non-negative integer k  0 1  n - 1 such that Ai j = 0 whenever i - j  k. Equivalently, A has bandwidth at most k if all entries above the kᵗʰ superdiagonal and below the kᵗʰ subdiagonal are zero, and A has bandwidth at least k if there exists any nonzero entry in the kᵗʰ superdiagonal or subdiagonal.\n\nThe matrix bandwidth recognition problem entails determining whether there exists a permutation matrix P such that the bandwidth of PAPᵀ is at most some fixed non-negative integer k  ℕ—an optimal permutation that fully minimizes the bandwidth of A is not required. Unlike the NP-hard minimization problem, this is decidable in O(nᵏ) time.\n\nThe following algorithms are currently supported:\n\nCaprara–Salazar-González algorithm (CapraraSalazarGonzalez)\nDel Corso–Manzini algorithm (DelCorsoManzini)\nDel Corso–Manzini algorithm with perimeter search (DelCorsoManziniWithPS)\nSaxe–Gurari–Sudborough algorithm (SaxeGurariSudborough)\nBrute-force search (BruteForceSearch)\n\nThis submodule is part of the MatrixBandwidth.jl package.\n\n\n\n\n\n","category":"module"},{"location":"public_api/#MatrixBandwidth.Recognition.BruteForceSearch","page":"Public API","title":"MatrixBandwidth.Recognition.BruteForceSearch","text":"BruteForceSearch <: AbstractDecider <: AbstractAlgorithm\n\nThe simplest method for determining, given some fixed k  ℕ, whether a matrix has bandwidth at most k up to symmetric permutation is to iterate over all orderings and compute the bandwidth induced by each.\n\nSince i₁ i₂  iₙ induces the same bandwidth as iₙ iₙ₁  i₁, we restrict our search to orderings such that i₁  iₙ (with equality checked just in case n = 1).\n\nIf a bandwidth-k ordering is found, the algorithm breaks early instead of continuing to check subsequent permutations.\n\nPerformance\n\nGiven an nn input matrix A, this brute-force algorithm runs in O(n  n²) time:\n\nUp to n2 permutations may be checked (except when n = 1, in which case   1 = 1 permutation is checked). This is, clearly, O(n).\nFor each permutation, the bandwidth function is called on view(A, perm, perm),   which takes O(n²) time.\nTherefore, the overall time complexity is O(n  n²).\n\nExamples\n\nIn many cases, the algorithm iterates over all (if k is smaller than the true minimu bandwidth) or almost all (if k is equally to or only slightly larger than the true minimum) possible permutations—in these cases, it is infeasible to go above 99 or 1010 without incurring multiple-hour runtimes. (Even when k is considerably larger than the true minimum, it is unlikely that a bandwidth-k ordering will be found in a reasonable time frame.) Nevertheless, we see that it is quite effective for, say, 88:\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(314159);\n\njulia> (n, p) = (8, 0.5);\n\njulia> A = sprand(Bool, n, n, p);\n\njulia> A = A + A' # Ensure structural symmetry;\n\njulia> (k_false, k_true) = (3, 5);\n\njulia> has_bandwidth_k_ordering(A, k_false, Recognition.BruteForceSearch())\nResults of Bandwidth Recognition Algorithm\n * Algorithm: Brute-force search\n * Bandwidth Threshold k: 3\n * Has Bandwidth ≤ k Ordering: false\n * Original Bandwidth: 6\n * Matrix Size: 8×8\n\njulia> has_bandwidth_k_ordering(A, k_true, Recognition.BruteForceSearch())\nResults of Bandwidth Recognition Algorithm\n * Algorithm: Brute-force search\n * Bandwidth Threshold k: 5\n * Has Bandwidth ≤ k Ordering: true\n * Original Bandwidth: 6\n * Matrix Size: 8×8\n\nNotes\n\nBrute force is by far the slowest approach to matrix bandwidth minimization and should only be used in very niche cases (like verifying the correctness of other algorithms in unit tests). For 1010 matrices, the algorithm already takes several minutes to run for difficult values of k (namely, values below or only slightly above the true minimum) and allocates several gigabytes of memory. Given the O(n  n²) time complexity, checking \"bandwidth ≤ k\" would take over an hour for many 1111 matrices.\n\nThis holds true even when k is considerably larger than the true minimum bandwidth—as long as it remains below the bandwidth induced by the original ordering, it is unlikely that a bandwidth-k ordering will be found early simply by random chance. Additionally, time complexity will remain on the order of n  n² in the average case.\n\nSee also MatrixBandwidth.Minimization.Exact.BruteForceSearch for the minimization variant of this algorithm (which simply never breaks early, instead iterating over all permutations up to reversal to ensure that the minimum bandwidth is found).\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Recognition.CapraraSalazarGonzalez","page":"Public API","title":"MatrixBandwidth.Recognition.CapraraSalazarGonzalez","text":"CapraraSalazarGonzalez <: AbstractDecider <: AbstractAlgorithm\n\nThe Caprara–Salazar-González recognition algorithm is a method for determining, given some fixed k  ℕ, whether a structurally symmetric matrix A has a bandwidth at most k up to symmetric permutation. The algorithm performs a bidirectional depth-first search of all partial orderings of the rows and columns of A, adding indices one at a time to both the left and right ends. Partial orderings are pruned not only by ensuring that adjacent pairs of currently placed indices are within k of each other but also by employing a branch-and-bound framework with lower bounds on bandwidtth compatibility computed via integer linear programming relaxations. This search is repeated with incrementing values of k until a bandwidth-k ordering is found [CS05], with k initialized to some lower bound on the minimum bandwidth of A up to symmetric permutation.\n\nAs noted above, the Caprara–Salazar-González algorithm requires structurally symmetric input (that is, Ai j must be nonzero if and only if Aj i is nonzero for 1  i j  n).\n\nPerformance\n\n[TODO: Write here]\n\nExamples\n\n[TODO: Write here]\n\nNotes\n\nThis algorithm is not the main one described in the original paper [CS05], which actually never explicitly presents a procedure for matrix bandwidth recognition. However, the paper does define a bandwidth minimization algorithm that repeatedly calls a recognition subroutine—this is precisely the logic we implement here. (We do, however, also implement said minimization algorithm in MatrixBandwidth.Minimization.Exact.CapraraSalazarGonzalez.)\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Recognition.DelCorsoManzini","page":"Public API","title":"MatrixBandwidth.Recognition.DelCorsoManzini","text":"DelCorsoManzini <: AbstractDecider <: AbstractAlgorithm\n\nThe Del Corso–Manzini recognition algorithm is a method for determining, given some fixed k  ℕ, whether a structurally symmetric matrix A has bandwidth at most k up to symmetric permutation. The algorithm performs a depth-first search of all partial orderings of the rows and columns of A, adding indices one at a time. Partial orderings are pruned not only by ensuring that adjacent pairs of currently placed indices are within k of each other but also by tracking the latest positions at which the remaining indices can be placed [DM99].\n\nAs noted above, the Del Corso–Manzini algorithm requires structurally symmetric input (that is, Ai j must be nonzero if and only if Aj i is nonzero for 1  i j  n).\n\nPerformance\n\nGiven an nn input matrix A and threshold bandwidth k, the Del Corso–Manzini algorithm runs in O(n  nk) time:\n\nWe perform a depth-first search of O(n) partial orderings.\nChecking plausibility of each partial ordering takes O(nk) time.\nTherefore, the overall time complexity is O(n  nk).\n\nOf course, this is but an upper bound on the time complexity of Del Corso–Manzini, achieved only in the most pathological of cases. In practice, efficient pruning techniques and compatibility checks result in approximately exponential growth in time complexity with respect to n.\n\nBased on experimental results, the algorithm is feasible for nn matrices up to n  100 or so.\n\nExamples\n\nWe demonstrate both an affirmative and a negative result for the Del Corso–Manzini recognition algorithm on a random 4040 matrix:\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(7878);\n\njulia> (n, p) = (40, 0.1);\n\njulia> A = sprand(n, n, p);\n\njulia> A = A + A' # Ensure structural symmetry;\n\njulia> (k_false, k_true) = (13, 26);\n\njulia> has_bandwidth_k_ordering(A, k_false, Recognition.DelCorsoManzini())\nResults of Bandwidth Recognition Algorithm\n * Algorithm: Del Corso–Manzini\n * Bandwidth Threshold k: 13\n * Has Bandwidth ≤ k Ordering: false\n * Original Bandwidth: 34\n * Matrix Size: 40×40\n\njulia> has_bandwidth_k_ordering(A, k_true, Recognition.DelCorsoManzini())\nResults of Bandwidth Recognition Algorithm\n * Algorithm: Del Corso–Manzini\n * Bandwidth Threshold k: 26\n * Has Bandwidth ≤ k Ordering: true\n * Original Bandwidth: 34\n * Matrix Size: 40×40\n\nNotes\n\nFor readers of the original paper, what we call the Del Corso–Manzini recognition algorithm here is essentially a wrapper around the underlying AddNode subroutine in what [DM99, p. 191] terms the \"MB-ID algorithm\" for bandwidth minimization (not mere recognition). MB-ID (which we also implement in MatrixBandwidth.Minimization.Exact.DelCorsoManzini) calls this recognition procedure with incrementing values of k until a bandwidth-k ordering is found, with k initialized to some lower bound on the minimum bandwidth of A up to symmetric permutation.\n\n[DM99, p. 193] also describes an \"MB-PS algorithm\" for bandwidth minimization, which we implement in MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS. Similarly, the underlying recognition subroutine for MB-PS is implemented in DelCorsoManziniWithPS.\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Recognition.DelCorsoManziniWithPS","page":"Public API","title":"MatrixBandwidth.Recognition.DelCorsoManziniWithPS","text":"DelCorsoManziniWithPS{D} <: AbstractDecider <: AbstractAlgorithm\n\nThe Del Corso–Manzini recognition algorithm with perimeter search is a method for determining, given some fixed k  ℕ, whether a structurally symmetric matrix A has bandwidth at most k up to symmetric permutation. The base Del Corso–Manzini algorithm performs a depth-first search of all partial orderings of the rows and columns of A, adding indices one at a time. Partial orderings are pruned not only by ensuring that adjacent pairs of currently placed indices are within k of each other but also by tracking the latest positions at which the remaining indices can be placed [DM99].\n\nThe incorporation of perimeter search to this approach entails precomputing a \"perimeter\" of d-permutations of row indices of A, where d is a positive integer passed as a parameter to the decider. Each permutation represents a way to select the last d entries of the ordering, and as the construction of the partial ordering progresses, potential endings are pruned to exclude those incompatible with already placed indices. In addition to pruning a potential ending if it contains indices already placed, compatibility is also checked via precomputed time stamps indicating, for each potential ending, a loose lower bound on the earliest position at which any given index can be placed should said ending be selected.\n\nAs noted above, the Del Corso–Manzini algorithm with perimeter search requires structurally symmetric input (that is, Ai j must be nonzero if and only if Aj i is nonzero for 1  i j  n).\n\nFields\n\ndepth::D<:Union{Nothing,Int}: the perimeter search depth. If this field is not set (and   thus automatically initialized to nothing), a default depth is computed by   dcm_ps_optimal_depth as a function of the input matrix every time the decider   is passed to has_bandwidth_k_ordering as a function of the input matrix.   Otherwise, it must be manually set to a positive integer.\n\nConstructors\n\nDelCorsoManziniWithPS(): constructs a new DelCorsoManziniWithPS instance with the   default perimeter search depth initialized to nothing.\nDelCorsoManziniWithPS(depth::Int): constructs a new DelCorsoManziniWithPS instance   with the specified perimeter search depth. depth must be a positive integer.\n\nPerformance\n\nGiven an nn input matrix A, perimeter search depth d, and threshold bandwidth k, the Del Corso–Manzini algorithm with perimeter search runs in O(n  max(nᵈ nk)) time:\n\nWe perform a depth-first search of O(n) partial orderings.\nChecking plausibility of each partial ordering takes O(nk) time, and checking   compatibility with all size-d LPOs takes O(nᵈ) time. Thus, the overall time   complexity for each value of k is O(n  (nᵈ + nk)).\nTherefore, the overall time complexity is O(n  max(nᵈ nk)).\n\nOf course, this is but an upper bound on the time complexity of Del Corso–Manzini with perimeter search, achieved only in the most pathological of cases. In practice, efficient pruning techniques and compatibility checks result in approximately exponential growth in time complexity with respect to n.\n\nBased on experimental results, the algorithm is feasible for nn matrices up to n  100 or so.\n\nExamples\n\nHere, Del Corso–Manzini with perimeter search ascertains that A random 3030 matrix has a minimum bandwidth greater than 9. The depth parameter is not explicitly set; instead, some near-optimal value is automatically computed upon the first has_bandwidth_k_ordering function call.\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(5847);\n\njulia> (n, p) = (30, 0.05);\n\njulia> A = sprand(n, n, p);\n\njulia> A = A + A' # Ensure structural symmetry;\n\njulia> k = 6;\n\njulia> has_bandwidth_k_ordering(A, k, Recognition.DelCorsoManziniWithPS())\nResults of Bandwidth Recognition Algorithm\n * Algorithm: Del Corso–Manzini with perimeter search\n * Bandwidth Threshold k: 6\n * Has Bandwidth ≤ k Ordering: false\n * Original Bandwidth: 27\n * Matrix Size: 30×30\n\nNow, Del Corso–Manzini with perimeter search recognizes that a random 3535 matrix has a minimum bandwidth at most 8. In this case, we explitily set the depth parameter to 4 beforehand instead of relying on Recognition.dcm_ps_optimal_depth.\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(23552);\n\njulia> (n, p, depth) = (35, 0.02, 4);\n\njulia> A = sprand(n, n, p);\n\njulia> A = A + A' # Ensure structural symmetry;\n\njulia> k = 8;\n\njulia> has_bandwidth_k_ordering(A, k, Recognition.DelCorsoManziniWithPS(depth))\nResults of Bandwidth Recognition Algorithm\n * Algorithm: Del Corso–Manzini with perimeter search\n * Bandwidth Threshold k: 8\n * Has Bandwidth ≤ k Ordering: true\n * Original Bandwidth: 32\n * Matrix Size: 35×35\n\nNotes\n\nFor readers of the original paper, what we call the Del Corso–Manzini recognition algorithm with perimeter search here is essentially a wrapper around the underlying AddNode1 and Prune subroutines in what [DM99, p. 193] terms the \"MB-PS algorithm\" for bandwidth minimization (not mere recognition). MB-PS (which we also implement in MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS) calls this recognition procedure with incrementing values of k until a bandwidth-k ordering is found, with k initialized to some lower bound on the minimum bandwidth of A up to symmetric permutation.\n\n[DM99, p. 191] also describes an \"MB-ID algorithm\" for bandwidth minimization, which we implement in MatrixBandwidth.Minimization.Exact.DelCorsoManzini. Similarly, the underlying recognition subroutine for MB-ID is implemented in DelCorsoManzini.\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Recognition.RecognitionResult","page":"Public API","title":"MatrixBandwidth.Recognition.RecognitionResult","text":"RecognitionResult{A,M,O} <: AbstractResult\n\nOutput struct for matrix bandwidth recognition results.\n\nFields\n\nalgorithm::A<:AbstractDecider: the decider used to test the bandwidth.\nmatrix::M<:AbstractMatrix{<:Number}: the original matrix whose bandwidth is tested.\nordering::O<:Union{Nothing,Vector{Int}}: an ordering of the rows and columns of matrix   inducing a bandwidth at most k, if such an ordering exists; otherwise, nothing.\nk::Int: the threshold bandwidth against which to test.\nhas_ordering::Bool: whether the matrix has an ordering inducing a bandwidth at most k.   (This is true if and only if ordering is not nothing.)\n\nConstructors\n\nRecognitionResult(decider, matrix, ordering, k): constructs a new RecognitionResult   instance with the given fields. The has_ordering field is automatically determined   based on whether ordering is nothing or a Vector{Int}.\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Recognition.SaxeGurariSudborough","page":"Public API","title":"MatrixBandwidth.Recognition.SaxeGurariSudborough","text":"SaxeGurariSudborough <: AbstractDecider <: AbstractAlgorithm\n\n[TODO: Write here]\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Recognition.has_bandwidth_k_ordering","page":"Public API","title":"MatrixBandwidth.Recognition.has_bandwidth_k_ordering","text":"has_bandwidth_k_ordering(A, k, decider=CapraraSalazarGonzalez()) -> RecognitionResult\n\nDetermine whether A has bandwidth at most k using the algorithm defined by decider.\n\nThe bandwidth of an nn matrix A is the minimum non-negative integer k  0 1  n - 1 such that Ai j = 0 whenever i - j  k. Equivalently, A has bandwidth at most k if all entries above the kᵗʰ superdiagonal and below the kᵗʰ subdiagonal are zero, and A has bandwidth at least k if there exists any nonzero entry in the kᵗʰ superdiagonal or subdiagonal.\n\nGiven some fixed non-negative integer k, this function determines (with 100% certainty) whether there exists some ordering π of the rows and columns of A such that the bandwidth of PAPᵀ is at most k, where P is the permutation matrix corresponding to π. This is known to be decidable in O(nᵏ) time, although some deciders (e.g., CapraraSalazarGonzalez) run in exponential time instead to produce even quicker runtimes in practice.\n\nIf k  n - 1, then this function immediately answers in the affirmative, since the maximum possible bandwidth of an nn matrix is n - 1. After this initial check, a preliminary lower bound on the bandwidth is computed in O(n³) time using results from Caprara and Salazar-González (2005). If this lower bound is greater than k`\n\nArguments\n\nA::AbstractMatrix{<:Number}: the (square) matrix whose bandwidth is tested.\nk::Int: the threshold bandwidth against which to test.\ndecider::AbstractDecider: the matrix bandwidth recognition algorithm to use; defaults to   CapraraSalazarGonzalez. (See the Recognition module documentation   for a full list of supported deciders.)\n\nReturns\n\n::RecognitionResult: a struct containing the algorithm used, the original matrix A,   the identified ordering of the rows and columns (if one exists), the threshold bandwidth   k, and a boolean indicating whether the ordering exists.\n\nExamples\n\n[TODO: Add here once more deciders are implemented. For now, refer to the Examples sections of the DelCorsoManzini and DelCorsoManziniWithPS docstrings.]\n\nNotes\n\nSome texts define matrix bandwidth to be the minimum non-negative integer k such that Ai j = 0 whenever i - j  k instead, particularly in more mathematically-minded communities. Effectively, this definition treats diagonal matrices as bandwidth 1, tridiagonal matrices as bandwidth 2, and so on. Our definition, on the other hand, is more common in computer science contexts, treating diagonal matrices as bandwidth 0 and tridiagonal matrices as bandwidth 1. (Both definitions, however, agree that the bandwidth of an empty matrix is simply 0.)\n\n\n\n\n\n","category":"function"},{"location":"public_api/#References","page":"Public API","title":"References","text":"","category":"section"},{"location":"public_api/","page":"Public API","title":"Public API","text":"A. Caprara and J.-J. Salazar-González. Laying Out Sparse Graphs with Provably Minimum Bandwidth. INFORMS Journal on Computing 17, 356–73 (2005).\n\n\n\nW. M. Chan and A. George. A linear time implementation of the reverse Cuthill–McKee algorithm. BIT Numerical Mathematics 20, 8–14 (1980).\n\n\n\nE. Cuthill and J. McKee. Reducing the bandwidth of sparse symmetric matrices. In: Proceedings of the 24th National Conference of the ACM (Brandon Systems Press, 1969); pp. 157–72.\n\n\n\nG. M. Del Corso and G. Manzini. Finding Exact Solutions to the Bandwidth Minimization Problem. Computing 62, 189–203 (1999).\n\n\n\nJ. A. George. Computer Implementation of the Finite Element Method. Ph.D. Thesis, Department of Computer Science, Stanford University (1971).\n\n\n\nN. E. Gibbs, W. G. Poole Jr. and P. K. Stockmeyer. An Algorithm for Reducing the Bandwidth and Profile of a Sparse Matrix. SIAM Journal on Numerical Analysis 13, 236–50 (1976).\n\n\n\nJ. G. Lewis. Implementation of the Gibbs–Poole–Stockmeyer and Gibbs–King Algorithms. ACM Transactions on Mathematical Software 8, 180–89 (1982).\n\n\n\nL. O. Mafteiu-Scai. The Bandwidths of a Matrix. A Survey of Algorithms. Annals of West University of Timisoara - Mathematics and Computer Science 52, 183–223 (2014).\n\n\n\n","category":"page"},{"location":"private_api/#MatrixBandwidth.jl-–-Private-API","page":"Private API","title":"MatrixBandwidth.jl – Private API","text":"","category":"section"},{"location":"private_api/","page":"Private API","title":"Private API","text":"Documentation for MatrixBandwidth's private API.","category":"page"},{"location":"private_api/","page":"Private API","title":"Private API","text":"note: Note\nThe following documentation covers only the private API of the package. For public details, see the public API documentation.","category":"page"},{"location":"private_api/#MatrixBandwidth.AbstractAlgorithm","page":"Private API","title":"MatrixBandwidth.AbstractAlgorithm","text":"AbstractAlgorithm\n\nAbstract base type for all matrix bandwidth minimization and recognition algorithms.\n\nInterface\n\nConcrete subtypes of AbstractAlgorithm must implement the following methods:\n\nBase.summary(::T) where {T<:AbstractAlgorithm}: returns a String indicating the name   of the algorithm (e.g., \"Gibbs–Poole–Stockmeyer\").\n_requires_symmetry(::T) where {T<:AbstractAlgorithm}: returns a Bool indicating   whether the algorithm requires the input matrix to be structurally symmetric.\n\nDirect subtypes of AbstractAlgorithm must implement the following method:\n\n_problem(::T) where {T<:AbstractAlgorithm}: returns a Symbol indicating the   matrix bandwidth problem tackled by the algorithm (e.g., :minimization).\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.AbstractResult","page":"Private API","title":"MatrixBandwidth.AbstractResult","text":"AbstractResult\n\nAbstract base type for all matrix bandwidth problem results.\n\nInterface\n\nConcrete subtypes of AbstractResult must implement parametric types\n\nA<:AbstractAlgorithm;\nM<:AbstractMatrix{<:Number}; and\nO<:Union{Nothing,Vector{Int}},\n\nalongside the following fields:\n\nalgorithm::A: the algorithm used to investigate the bandwidth.\nmatrix::M: the matrix whose bandwidth is investigated.\nordering::O: the corresponding ordering of the rows and columns, if a relevant one is   found; otherwise, nothing.\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.NotImplementedError","page":"Private API","title":"MatrixBandwidth.NotImplementedError","text":"NotImplementedError{Nothing}(f, subtype, abstracttype)\nNotImplementedError{Symbol}(f, arg, subtype, abstracttype)\n\nAn exception indicating that a function lacks dispatch to handle a specific argument type.\n\nSemantically, this differs from MethodError in that it connotes a developer-side failure to implement a method rather than erroneous user input. Throughout this package, it is often used to warn when an existing function with multiple dispatch on some abstract type is called on a newly created subtype for which no method has been defined.\n\nFields\n\nf::Function: the function called.\narg::Symbol: the name of the argument with the unsupported type, if the function has   multiple arguments. If the function has only one argument, this field should be set to   nothing.\nsubtype::Type: the type of the argument. May be the actual concrete type or some   intermediate supertype. (For instance, if the relevant input has concrete type A with   hierarchy A <: B <: C and the abstracttype field is C, then both A and B are   perfectly valid choices for subtype.)\nabstracttype::Type: the abstract type under which the argument is meant to fall.\n\nConstructors\n\nNotImplementedError(::Function, ::Type, ::Type): constructs a new NotImplementedError   instance for a single-argument function. Throws an error if the second type is not   abstract or the first type is not a subtype of the second.\nNotImplementedError(::Function, ::Symbol, ::Type, ::Type): constructs a new   NotImplementedError instance for a multi-argument function. Throws an error if the   second type is not abstract or the first type is not a subtype of the second.\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.RectangularMatrixError","page":"Private API","title":"MatrixBandwidth.RectangularMatrixError","text":"RectangularMatrixError(A)\n\nAn exception indicating that the matrix A is not square.\n\nMatrix bandwidth is only defined for square matrices, so this exception is raised when a bandwidth minimization or recognition algorithm is called with a non-square input.\n\nFields\n\nA::AbstractMatrix{<:Number}: the input matrix.\nm::Int: the number of rows of A.\nn::Int: the number of columns of A.\n\nConstructors\n\nRectangularMatrixError(A::AbstractMatrix{<:Number}): constructs a new   RectangularMatrixError instance, automatically inferring m and n by calling   size(A).\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.StructuralAsymmetryError","page":"Private API","title":"MatrixBandwidth.StructuralAsymmetryError","text":"StructuralAsymmetryError(A, algorithm)\n\nAn exception indicating that the matrix A is not structurally symmetric.\n\nAn n×n matrix A is structurally symmetric if Ai j is nonzero if and only if Aj i is nonzero for 1  i j  n. Many (albeit not all) matrix bandwidth minimization and recognition algorithms assume structural symmetry, so this exception is raised when one of these algorithms is called with a structurally asymmetric input.\n\nFields\n\nA::AbstractMatrix{<:Number}: the input matrix.\nalgorithm::AbstractAlgorithm: the algorithm that was called.\nproblem::Symbol: the matrix bandwidth problem tackled by the algorithm (e.g.,   :minimization).\n\nConstructors\n\nStructuralAsymmetryError(A::AbstractMatrix{<:Number}, algorithm::AbstractAlgorithm):   constructs a new StructuralAsymmetryError instance, automatically inferring problem   by calling _problem(algorithm).\n\nNotes\n\nAs noted in the error message for StructuralAsymmetryError instances, users may want to consider symmetrization techniques from [RS06] to minimize the bandwidth of structurally asymmetric matrices. (A prominent one is to simply replace Ai j with 1 whenever Ai j = 0 but Aj i  0.) Of course, the reliability of minimization algorithms is diminished after such a transformation, so users should proceed with caution nonetheless.\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.Minimization.AbstractSolver","page":"Private API","title":"MatrixBandwidth.Minimization.AbstractSolver","text":"AbstractSolver <: AbstractAlgorithm\n\nAbstract base type for all matrix bandwidth minimization solvers.\n\nInterface\n\nAs per the interface of supertype AbstractAlgorithm, concrete subtypes of AbstractSolver must implement the following methods:\n\nBase.summary(::T) where {T<:AbstractSolver}: returns a String indicating the name   of the solver (e.g., \"Gibbs–Poole–Stockmeyer\").\n_requires_symmetry(::T) where {T<:AbstractSolver}: returns a Bool indicating   whether the solver requires the input matrix to be structurally symmetric.\n\nDirect subtypes of AbstractSolver must implement the following method:\n\n_approach(::T) where {T<:AbstractSolver}: returns a Symbol indicating the   category of solver (e.g., :heuristic).\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.Minimization.Exact.ExactSolver","page":"Private API","title":"MatrixBandwidth.Minimization.Exact.ExactSolver","text":"ExactSolver <: AbstractSolver <: AbstractAlgorithm\n\nAbstract type for all exact matrix bandwidth minimization solvers.\n\nExact methods are those which guarantee an optimal ordering producing the true minimum bandwidth of a matrix. Since bandwidth minimization is an NP-complete problem, existing exact algorithms are, at best, exponential in time complexity—much worse than many polynomial-time heuristic approaches (e.g., Gibbs–Poole–Stockmeyer). Such methods, therefore, are not feasible for large matrices, but they remain useful when precise solutions are required for small-to-medium-sized inputs (say, up to 100100).\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.Minimization.Heuristic.HeuristicSolver","page":"Private API","title":"MatrixBandwidth.Minimization.Heuristic.HeuristicSolver","text":"HeuristicSolver <: AbstractSolver <: AbstractAlgorithm\n\nAbstract type for all heuristic matrix bandwidth minimization solvers.\n\nHeuristic methods are those which aim to produce near-optimal solutions in a more performant manner than exact methods. While precise bandwidth minimization is NP-complete, many heuristic algorithms (such as Gibbs–Poole–Stockmeyer) run in polynomial time.\n\nHeuristic algorithms differ from metaheuristic ones in that they do not employ higher-level iterative search frameworks (e.g., stochastic techniques) to survey the global search space and escape local minima; instead, they rely on straightforward deterministic procedures to find good solutions in a single pass.\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.Minimization.Heuristic.pseudo_peripheral_node-Tuple{AbstractMatrix{Bool}}","page":"Private API","title":"MatrixBandwidth.Minimization.Heuristic.pseudo_peripheral_node","text":"pseudo_peripheral_node(A::AbstractMatrix{Bool}) -> Int\n\nSelect a pseudo-peripheral node from the connected graph represented by A.\n\nThis function acts as a node selector for heuristic matrix bandwidth minimization algorithms such as reverse Cuthill–McKee and Gibbs–Poole–Stockmeyer when applies to connected graphs (or their adjacency matrices). It heuristically identifies the node \"farthest\" from the others in the graph as a good starting point for the search process.\n\nIt is assumed that A is the adjacency matrix of some connected, undirected graph; otherwise, undefined behavior may arise.\n\nArguments\n\nA::AbstractMatrix{Bool}: a symmetric matrix with boolean entries, acting as the   adjacency matrix of some connected undirected graph.\n\nReturns\n\nInt: the index of the pseudo-peripheral node selected from the graph.\n\nNotes\n\nThis function takes heavy inspiration from the implementation in [Net25], which accepts a graph object as input and leverages several pre-existing functions in the networkx library. We herein repurpose the logic to work directly on adjacency matrices, avoiding reallocation overhead and an unnecessary dependency on Graphs.jl.\n\n[TODO: Specify the actual academic source for this, not just the NetworkX one.]\n\n\n\n\n\n","category":"method"},{"location":"private_api/#MatrixBandwidth.Minimization.Metaheuristic.MetaheuristicSolver","page":"Private API","title":"MatrixBandwidth.Minimization.Metaheuristic.MetaheuristicSolver","text":"MetaheuristicSolver <: AbstractSolver <: AbstractAlgorithm\n\nAbstract type for all metaheuristic matrix bandwidth minimization solvers.\n\nMetaheuristic methods are those which employ higher-level iterative search frameworks such as stochastic techniques or nature-inspired processes to survey the global search space and escape local minima. Unlike heuristic methods—which follow fixed deterministic procedures—metaheuristics adaptively refine candidate solutions over multiple iterations. Although metaheuristic approaches are often slower than heuristic ones (but certainly still faster than exact ones), they shine in complex cases where the latter may get trapped in poor-quality local minima.\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.Recognition.AbstractDecider","page":"Private API","title":"MatrixBandwidth.Recognition.AbstractDecider","text":"AbstractDecider <: AbstractAlgorithm\n\nAbstract base type for all matrix bandwidth recognition deciders.\n\nInterface\n\nAs per the interface of supertype AbstractAlgorithm, concrete subtypes of AbstractDecider must implement the following methods:\n\nBase.summary(::T) where {T<:AbstractDecider}: returns a String indicating the name   of the decider (e.g., \"Caprara–Salazar-González\").\n_requires_symmetry(::T) where {T<:AbstractDecider}: returns a Bool indicating   whether the decider requires the input matrix to be structurally symmetric.\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.Recognition.dcm_ps_optimal_depth-Tuple{AbstractMatrix{Bool}}","page":"Private API","title":"MatrixBandwidth.Recognition.dcm_ps_optimal_depth","text":"dcm_ps_optimal_depth(A::AbstractMatrix{Bool}) -> Int\n\nCompute a (hopefully) near-optimal Del Corso–Manzini perimeter search depth for A.\n\nTaking experimental results from [DM99, pp. 197–199] into account, this function tries to approximate the optimal depth parameter as a function of both matrix size and density. This depth parameter determines how large of a \"perimeter\" of last-placed indices is precomputed in the Del Corso–Manzini algorithm with perimeter search.\n\nArguments\n\nA::AbstractMatrix{Bool}: the (structurally symmetric and square) input matrix whose   bandwidth is investigated.\n\nReturns\n\n::Int: a (hopefully) near-optimal perimeter search depth for the Del Corso–Manzini   algorithm with perimeter search on A.\n\nNotes\n\nSee Tables 4, 5, and 6 from the original paper for more details on experimental results regarding the optimal perimeter search depth [DM99, pp. 197–199].\n\nSee also DelCorsoManziniWithPS and MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS) for our implementation of the relevant bandwidth recognition and bandwidth minimization algorithms, respectively.\n\n\n\n\n\n","category":"method"},{"location":"private_api/#References","page":"Private API","title":"References","text":"","category":"section"},{"location":"private_api/","page":"Private API","title":"Private API","text":"J. K. Reid and J. A. Scott. Reducing the Total Bandwidth of a Sparse Unsymmetric Matrix. SIAM Journal on Matrix Analysis and Applications 28, 805–21 (2006).\n\n\n\nNetworkX Developers. Source code for networkx.utils.rcm. NetworkX v3.5 documentation (2025). Accessed: 2025-06-11.\n\n\n\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"<table>\n  <tr>\n    <td>Metadata</td>\n    <td>\n      <img src=\"https://img.shields.io/badge/version-v0.1.1-pink.svg\" alt=\"Version\">\n      <a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-A31F34.svg\" alt=\"License: MIT\"></a>\n      <a href=\"https://github.com/JuliaDiff/BlueStyle\"><img src=\"https://img.shields.io/badge/code%20style-blue-4495d1.svg\" alt=\"Code Style: Blue\"></a>\n    </td>\n  </tr>\n  <tr>\n    <td>Documentation</td>\n    <td>\n      <a href=\"https://luis-varona.github.io/MatrixBandwidth.jl/stable/\"><img src=\"https://img.shields.io/badge/docs-stable-darkgreen.svg\" alt=\"Documentation of latest stable version\"></a>\n      <a href=\"https://luis-varona.github.io/MatrixBandwidth.jl/dev/\"><img src=\"https://img.shields.io/badge/docs-dev-rebeccapurple.svg\" alt=\"Documentation of dev version\"></a>\n    </td>\n  </tr>\n  <tr>\n    <td>Continuous integration</td>\n    <td>\n      <a href=\"https://github.com/Luis-Varona/MatrixBandwidth.jl/actions?query=workflow%3ACI+branch%3Amain\"><img src=\"https://github.com/Luis-Varona/MatrixBandwidth.jl/actions/workflows/CI.yml/badge.svg\" alt=\"GitHub Workflow Status\"></a>\n    </td>\n  </tr>\n  <tr>\n    <td>Code coverage</td>\n    <td>\n      <a href=\"https://codecov.io/gh/Luis-Varona/MatrixBandwidth.jl\"><img src=\"https://codecov.io/gh/Luis-Varona/MatrixBandwidth.jl/branch/main/graph/badge.svg\" alt=\"Test coverage from codecov\"></a>\n    </td>\n    </tr>\n    <tr>\n      <td>Static analysis with</td>\n      <td>\n        <a href=\"https://github.com/JuliaTesting/Aqua.jl\"><img src=\"https://raw.githubusercontent.com/JuliaTesting/Aqua.jl/master/badge.svg\" alt=\"Aqua QA\"></a>\n        <a href=\"https://github.com/aviatesk/JET.jl\"><img src=\"https://img.shields.io/badge/%E2%9C%88%20tested%20with-JET.jl%EF%B8%8F-9cf.svg\" alt=\"JET static analysis\"></a>\n      </td>\n    </tr>\n</table>","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MatrixBandwidth.jl offers fast algorithms for matrix bandwidth minimization and matrix bandwidth recognition.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The bandwidth of an n times n matrix A is the minimum non-negative integer k in 0 1 ldots n - 1 such that A_ij = 0 whenever i - j  k. Equivalently, A has bandwidth at most k if all entries above the k^textth superdiagonal and below the k^textth subdiagonal are zero, and A has bandwidth at least k if there exists any nonzero entry in the k^textth superdiagonal or subdiagonal.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The matrix bandwidth minimization problem involves finding a permutation matrix P such that the bandwidth of PAP^mathsfT is minimized; this is known to be NP-complete. Several heuristic algorithms (such as Gibbs–Poole–Stockmeyer) run in polynomial time while still producing near-optimal orderings in practice, but exact methods (like Caprara–Salazar-González) are at least exponential in time complexity and thus are only feasible for relatively small matrices.","category":"page"},{"location":"","page":"Home","title":"Home","text":"On the other hand, the matrix bandwidth recognition problem entails determining whether there exists a permutation matrix P such that the bandwidth of PAP^mathsfT is at most some fixed non-negative integer k in mathbbN—an optimal permutation that fully minimizes the bandwidth of A is not required. Unlike the NP-hard minimization problem, this is decidable in O(n^k) time.","category":"page"},{"location":"#Algorithms","page":"Home","title":"Algorithms","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following algorithms are currently supported:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Minimization\nExact\nCaprara–Salazar-González algorithm [under development]\nDel Corso–Manzini algorithm\nDel Corso–Manzini algorithm with perimeter search\nSaxe–Gurari–Sudborough algorithm [under development]\nBrute-force search\nHeuristic\nGibbs–Poole–Stockmeyer algorithm\nCuthill–McKee algorithm\nReverse Cuthill–McKee algorithm\nMetaheuristic\nGreedy randomized adaptive search procedure (GRASP) [under development]\nSimulated annealing [under development]\nGenetic algorithm [under development]\nRecognition\nCaprara–Salazar-González algorithm [under development]\nDel Corso–Manzini algorithm\nDel Corso–Manzini algorithm with perimeter search\nSaxe–Gurari–Sudborough algorithm [under development]\nBrute-force search","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Although the API is already stable with the bulk of the library already functional and tested, a few algorithms remain under development. Whenever such an algorithm is used, the error ERROR: TODO: Not yet implemented is raised.)","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The only prerequisite is a working Julia installation (v1.10 or later). First, enter Pkg mode by typing ] in the Julia REPL, then run the following command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add MatrixBandwidth","category":"page"},{"location":"#Basic-use","page":"Home","title":"Basic use","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MatrixBandwidth.jl offers unified interfaces for both bandwidth minimization and bandwidth recognition via the minimize_bandwidth and has_bandwidth_k_ordering functions, respectively—the algorithm itself is specified as an argument. For example, to minimize the bandwidth of a random matrix with the reverse Cuthill–McKee algorithm, you can run the following code:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Random, SparseArrays\n\njulia> Random.seed!(8675309);\n\njulia> A = sprand(40, 40, 0.02); A = A + A' # Ensure structural symmetry\n40×40 SparseMatrixCSC{Float64, Int64} with 82 stored entries:\n⎡⠀⠀⠂⠘⠀⠀⠐⠀⠀⠀⠀⠀⠆⠀⠀⠀⠂⠄⠈⠀⎤\n⎢⣈⠀⠤⠃⠀⠀⠀⠈⠀⠀⠀⠀⠂⠂⠀⠀⠀⠂⠀⠄⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠚⠀⠀⠀⠀⠀⠂⠁⠀⠀⠀⠀⎥\n⎢⠐⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠀⠢⠀⠈⠀⠀⠂⡄⎥\n⎢⠀⠀⠀⠀⠚⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠠⠀⠒⎥\n⎢⠀⠀⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⠀⠂⠨⠂⠀⠀⠁⠀⎥\n⎢⠈⠁⠨⠀⠀⠀⠠⡀⠀⠀⠠⠀⠀⠀⠀⠂⠠⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠌⠀⡀⠀⠀⠀⠢⠂⠠⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠈⠄⠠⠀⠀⠀⠀⠀⠀⡐⠀⠀⠀⠂⠀⠀⢀⡰⠀⠀⎥\n⎣⠂⠀⠀⠄⠀⠀⠈⠤⢠⠀⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⎦\n\njulia> res_minimize = minimize_bandwidth(A, Minimization.ReverseCuthillMcKee())\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Reverse Cuthill–McKee\n * Approach: heuristic\n * Minimum Bandwidth: 9\n * Original Bandwidth: 37\n * Matrix Size: 40×40\n\njulia> A[res_minimize.ordering, res_minimize.ordering]\n40×40 SparseMatrixCSC{Float64, Int64} with 82 stored entries:\n⎡⠪⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠒⡀⠈⠆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠈⠡⠄⠁⠁⠠⠂⢄⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠁⡀⠀⠀⠠⠀⠘⢄⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠈⢄⠀⠂⢀⠐⠀⠠⠁⢆⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠒⢄⠀⡀⠀⠀⠀⠌⢢⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠡⢄⡀⠄⠀⠀⠘⡄⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠒⠒⠤⢄⡱⡀⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠪⡢⎦","category":"page"},{"location":"","page":"Home","title":"Home","text":"Similarly, to determine whether said matrix has bandwidth at most, say, 10 (not necessarily caring about the true minimum) via the Del Corso–Manzini algorithm, you can run:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> res_recognize = has_bandwidth_k_ordering(A, 10, Recognition.DelCorsoManzini())\nResults of Bandwidth Recognition Algorithm\n * Algorithm: Del Corso–Manzini\n * Bandwidth Threshold k: 10\n * Has Bandwidth ≤ k Ordering: true\n * Original Bandwidth: 37\n * Matrix Size: 40×40\n\njulia> A[res_recognize.ordering, res_recognize.ordering]\n40×40 SparseMatrixCSC{Float64, Int64} with 82 stored entries:\n⎡⠊⠀⠀⢀⡈⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⢀⠀⠀⢀⠀⠀⡐⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⢆⠈⠀⠐⢀⠐⠀⠅⡀⠤⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠑⢀⠠⠄⠄⠊⠀⠈⠀⠀⠐⢀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠑⠀⡌⠂⠀⢀⠐⠀⠀⠀⠔⡀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠑⢀⠀⠀⠀⠀⠀⠀⠀⠐⠲⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠐⢀⠄⠀⠀⠀⠀⠀⠀⠁⢁⠄⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢰⡀⠀⠀⠀⠀⠀⠑⢀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠅⢀⢄⠀⠀⠀⠀⢀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠀⠐⠀⢀⠀⠀⎦","category":"page"},{"location":"","page":"Home","title":"Home","text":"If no algorithm is explicitly specified, minimize_bandwidth defaults to the Gibbs–Poole–Stockmeyer algorithm:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> res_minimize_default = minimize_bandwidth(A)\nResults of Bandwidth Minimization Algorithm\n * Algorithm: Gibbs–Poole–Stockmeyer\n * Approach: heuristic\n * Minimum Bandwidth: 6\n * Original Bandwidth: 37\n * Matrix Size: 40×40\n\njulia> A[res_minimize_default.ordering, res_minimize_default.ordering]\n40×40 SparseMatrixCSC{Float64, Int64} with 82 stored entries:\n⎡⠪⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠀⠀⡠⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠐⠀⠀⠑⠄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠑⠄⠀⠀⠌⢢⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠈⠢⣁⠀⠀⠨⠆⢀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠈⠢⠆⠄⡡⠚⠄⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⠚⠄⡀⠈⠦⣀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢣⠀⠀⠃⡀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠠⠎⡡⠢⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠂⢠⠒⎦","category":"page"},{"location":"","page":"Home","title":"Home","text":"(We default to Gibbs–Poole–Stockmeyer because it is one of the most accurate heuristic algorithms—note how in this case, it produced a lower-bandwidth ordering than reverse Cuthill–McKee. Of course, if true optimality is required, an exact algorithm such as Caprara–Salazar-González should be used instead.)","category":"page"},{"location":"","page":"Home","title":"Home","text":"has_bandwidth_k_ordering similarly defaults to Caprara–Salazar-González, which we have not yet implemented, so users should specify which of the completed algorithms they wish to use in the meantime or else face an error:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> res_recognize_default = has_bandwidth_k_ordering(A, 10)\nERROR: TODO: Not yet implemented\n[...]","category":"page"},{"location":"","page":"Home","title":"Home","text":"Complementing our various bandwidth minimization and recognition algorithms, MatrixBandwidth.jl exports several additional core functions, including (but not limited to) bandwidth and profile to compute the original bandwidth and profile of a matrix:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> using Random, SparseArrays\n\njulia> Random.seed!(1234);\n\njulia> A = sprand(50, 50, 0.01); A = A + A' # Ensure structural symmetry\n\njulia> bandwidth(A) # Bandwidth prior to any reordering of rows and columns\n38\n\njulia> profile(A) # Profile prior to any reordering of rows and columns\n645","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Closely related to bandwidth, the column profile of a matrix is the sum of the distances from each diagonal entry to the farthest nonzero entry in that column, whereas the row profile is the sum of the distances from each diagonal entry to the farthest nonzero entry in that row. profile(A) computes the column profile of A by default, but it can also be used to compute the row profile.)","category":"page"},{"location":"#Documentation","page":"Home","title":"Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The full documentation is available at GitHub Pages. Documentation for methods and types is also available via the Julia REPL—for instance, to learn more about the minimize_bandwidth function, enter help mode by typing ?, then run the following command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"help?> minimize_bandwidth\nsearch: minimize_bandwidth bandwidth MatrixBandwidth\n\n  minimize_bandwidth(A, solver=GibbsPooleStockmeyer()) -> MinimizationResult\n\n  Minimize the bandwidth of A using the algorithm defined by solver.\n\n  The bandwidth of an n×n matrix A is the minimum non-negative integer k ∈\n  \\{0, 1, …, n - 1\\} such that A[i, j] = 0 whenever |i - j| > k. Equivalently,\n  A has bandwidth at most k if all entries above the kᵗʰ superdiagonal and\n  below the kᵗʰ subdiagonal are zero, and A has bandwidth at least k if there\n  exists any nonzero entry in the kᵗʰ superdiagonal or subdiagonal.\n\n  This function computes a (near-)optimal ordering π of the rows and columns\n  of A so that the bandwidth of PAPᵀ is minimized, where P is the permutation\n  matrix corresponding to π. This is known to be an NP-complete problem;\n  however, several heuristic algorithms such as Gibbs–Poole–Stockmeyer run in\n  polynomial time while still still producing near-optimal orderings in\n  practice. Exact methods like Caprara–Salazar-González are also available,\n  but they are at least exponential in time complexity and thus only feasible\n  for relatively small matrices.\n\n  Arguments\n  ≡≡≡≡≡≡≡≡≡\n  [...]","category":"page"},{"location":"#Citing","page":"Home","title":"Citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"I encourage you to cite this work if you have found any of the algorithms herein useful for your research. Starring the MatrixBandwidth.jl repository on GitHub is also appreciated.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The latest citation information may be found in the CITATION.bib file within the repository.","category":"page"},{"location":"#Project-status","page":"Home","title":"Project status","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The latest stable release of MatrixBandwidth.jl is v0.1.1. Although several algorithms are still under development, the bulk of the library is already functional and tested. I aim to complete development (including documentation and tests) of the remaining algorithms and other utility features by September 2025.","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
