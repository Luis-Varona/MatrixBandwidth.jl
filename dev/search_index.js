var documenterSearchIndex = {"docs":
[{"location":"public_api/#MatrixBandwidth.jl-–-Public-API","page":"Public API","title":"MatrixBandwidth.jl – Public API","text":"","category":"section"},{"location":"public_api/","page":"Public API","title":"Public API","text":"Documentation for MatrixBandwidth's public API.","category":"page"},{"location":"public_api/","page":"Public API","title":"Public API","text":"note: Note\nThe following documentation covers only the public API of the package. For internal details, see the private API documentation.","category":"page"},{"location":"public_api/#MatrixBandwidth.MatrixBandwidth","page":"Public API","title":"MatrixBandwidth.MatrixBandwidth","text":"MatrixBandwidth\n\nExact, heuristic, and metaheuristic algorithms for matrix bandwidth minimization in Julia.\n\nThe bandwidth of a square matrix A is the minimum non-negative integer k  ℕ such that Ai j = 0 whenever i - j  k. Equivalently, A has bandwidth at most k if all entries above the k-th superdiagonal and below the k-th subdiagonal are zero, and A has bandwidth at least k if there exists any nonzero entry in the k-th superdiagonal or subdiagonal.\n\nThe matrix bandwidth minimization problem entails finding a permutation matrix P so that the bandwidth of PAPᵀ is minimized; this is known to be NP-complete. Several heuristic algorithms (such as reverse Cuthill–McKee) run in polynomial time while still producing near-optimal orderings in practice, but exact methods (like MB-PS) are exponential in time complexity and thus are only feasible for relatively small matrices.\n\nThe following algorithms are currently supported:\n\nExact\nMBID: Minimum bandwidth by iterative deepening (MB-ID)\nMBPS: Minimum bandwidth by perimeter search (MB-PS)\nHeuristic\nCuthillMcKee: Cuthill–McKee algorithm\nReverseCuthillMcKee: Reverse Cuthill–McKee algorithm\nMetaheuristic\nSimulatedAnnealing: Simulated annealing\nGeneticAlgorithm: Genetic algorithm\nGRASP: Greedy randomized adaptive search procedure (GRASP)\n\nFull documentation is available for the latest development version of this package.\n\n\n\n\n\n","category":"module"},{"location":"public_api/#MatrixBandwidth.BandwidthResult","page":"Public API","title":"MatrixBandwidth.BandwidthResult","text":"BandwidthResult\n\nOutput struct for matrix bandwidth minimization results.\n\nFields\n\nmatrix::M: the original matrix whose bandwidth is minimized.\nbandwidth::Int: the minimized bandwidth of the matrix.\nordering::Vector{Int}: the (near-)optimal ordering of the rows and columns.\nsolver::S: the algorithm used to minimize the bandwidth.\napproach::Symbol: the approach used by the solver. (Should be one of :exact,   :heuristic, and :metaheuristic.)\n\nConstructors\n\nBandwidthResult(matrix, bandwidth, ordering, solver): constructs a new BandwidthResult   instance with the given fields. The approach field is automatically determined based   on the solver type.\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.bandwidth-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T<:Number","page":"Public API","title":"MatrixBandwidth.bandwidth","text":"bandwidth(A) -> Int\n\nCompute the bandwidth of A without any permutations.\n\nThe bandwidth of a square matrix A is the minimum non-negative integer k  ℕ such that Ai j = 0 whenever i - j  k. Equivalently, A has bandwidth at most k if all entries above the k-th superdiagonal and below the k-th subdiagonal are zero, and A has bandwidth at least k if there exists any nonzero entry in the k-th superdiagonal or subdiagonal.\n\nIn contrast to minimize_bandwidth, this function does not attempt to minimize the bandwidth of A by permuting its rows and columns—it simply computes its bandwidth as is.\n\nArguments\n\nA::AbstractMatrix{T}: the (square) matrix whose bandwidth is to be computed.\n\nReturns\n\n::Int: the bandwidth of A.\n\nExamples\n\nbandwidth correctly identifies the bandwidth of a pentadiagonal matrix as 2 and does not attempt to find a minimizing permutation upon shuffling of its rows and columns:\n\njulia> using Random\n\njulia> Random.seed!(242622);\n\njulia> (n, k) = (8, 2);\n\njulia> perm = randperm(n);\n\njulia> A = (!iszero).(random_banded_matrix(8, 2))\n8×8 BitMatrix:\n 1  0  1  0  0  0  0  0\n 1  0  1  1  0  0  0  0\n 1  1  1  1  1  0  0  0\n 0  1  0  1  1  1  0  0\n 0  0  0  1  1  1  1  0\n 0  0  0  1  1  1  1  1\n 0  0  0  0  1  1  1  1\n 0  0  0  0  0  1  0  1\n\njulia> bandwidth(A)\n2\n\njulia> A_shuffled = A[perm, perm]\n8×8 BitMatrix:\n 1  1  1  0  0  0  1  1\n 1  1  1  0  0  0  0  1\n 1  1  1  0  0  0  1  0\n 0  0  1  1  1  1  1  0\n 0  0  0  1  1  0  0  0\n 0  0  0  1  1  0  1  0\n 1  0  1  0  0  1  1  0\n 1  0  0  0  0  0  0  1\n\njulia> bandwidth(A_shuffled)\n7\n\nNotes\n\nSome texts define matrix bandwidth to be the minimum non-negative integer k such that Ai j = 0 whenever i - j  k instead, particularly in more mathematically-minded communities. Effectively, this definition treats diagonal matrices as bandwidth 1, tridiagonal matrices as bandwidth 2, and so on. Our definition, on the other hand, is more common in computer science contexts, treating diagonal matrices as bandwidth 0 and tridiagonal matrices as bandwidth 1. (Both definitions, however, agree that the bandwidth of an empty matrix is simply 0.)\n\n\n\n\n\n","category":"method"},{"location":"public_api/#MatrixBandwidth.minimize_bandwidth-Union{Tuple{AbstractMatrix{T}}, Tuple{T}, Tuple{AbstractMatrix{T}, MatrixBandwidth.AbstractSolver}} where T<:Number","page":"Public API","title":"MatrixBandwidth.minimize_bandwidth","text":"minimize_bandwidth(A, solver=ReverseCuthillMcKee()) -> BandwidthResult\n\nMinimize the matrix bandwidth of A using the algorithm defined by solver.\n\nThe bandwidth of a square matrix A is the minimum non-negative integer k  ℕ such that Ai j = 0 whenever i - j  k. Equivalently, A has bandwidth at most k if all entries above the k-th superdiagonal and below the k-th subdiagonal are zero, and A has bandwidth at least k if there exists any nonzero entry in the k-th superdiagonal or subdiagonal.\n\nThis function computes a (near-)optimal ordering π of the rows and columns of A so that the bandwidth of PAPᵀ is minimized, where P is the permutation matrix corresponding to π. This is known to be an NP-complete problem; however, several heuristic algorithms such as ReverseCuthillMcKee run in polynomial time while still producing near-optimal orderings in practice. Exact methods like MBPS are also available, but they are exponential in time complexity and thus only feasible for relatively small matrices.\n\nArguments\n\nA::AbstractMatrix{T}: the (square) matrix whose bandwidth is to be minimized.\nsolver::AbstractSolver: the matrix bandwidth minimization algorithm to use; defaults to   ReverseCuthillMcKee. (See the MatrixBandwidth module documentation   for a full list of supported solvers.)\n\nReturns\n\n::BandwidthResult: a struct containing the original matrix A, the minimized bandwidth,   the (near-)optimal ordering of the rows and columns, and the algorithm used.\n\nExamples\n\n[TODO: Add here once more solvers are implemented]\n\nNotes\n\nSome texts define matrix bandwidth to be the minimum non-negative integer k such that Ai j = 0 whenever i - j  k instead, particularly in more mathematically-minded communities. Effectively, this definition treats diagonal matrices as bandwidth 1, tridiagonal matrices as bandwidth 2, and so on. Our definition, on the other hand, is more common in computer science contexts, treating diagonal matrices as bandwidth 0 and tridiagonal matrices as bandwidth 1. (Both definitions, however, agree that the bandwidth of an empty matrix is simply 0.)\n\n\n\n\n\n","category":"method"},{"location":"public_api/#MatrixBandwidth.random_banded_matrix-Tuple{Int64, Int64}","page":"Public API","title":"MatrixBandwidth.random_banded_matrix","text":"random_banded_matrix(n, k; p=0.75, rng=default_rng()) -> Matrix{Float64}\n\nGenerate a random n×n matrix with bandwidth exactly k and sparse bands with density p.\n\nAll entries from this matrix will be from the interval [0, 1]. Entries up to the k-th superdiagonal and down to the k-th subdiagonal are nonzero with probability p, and each band has at least one nonzero entry to ensure that the bandwidth is precisely k.\n\nArguments\n\nn::Int: the order of the matrix to generate. Must be positive.\nk::Int: the desired matrix bandwidth. Must satisfy 0 ≤ k < n.\n\nKeyword Arguments\n\np::Real=0.75: the band density. Must satisfy 0 < p ≤ 1.\nrng::AbstractRNG=Random.default_rng(): the random number generator to use. Defaults to   Random.default_rng().\n\nReturns\n\n::Matrix{Float64}: a random n×n matrix with bandwidth exactly k and sparse bands   with density p.\n\nExamples\n\nGenerate a 66 matrix with bandwidth 1 and the maximum number of nonzero entries:\n\njulia> using Random\n\njulia> A = random_banded_matrix(6, 1; p=1, rng=MersenneTwister(1228))\n6×6 Matrix{Float64}:\n 0.918835  0.816296   0.0       0.0        0.0       0.0\n 0.182127  0.782844   0.616169  0.0        0.0       0.0\n 0.0       0.0445171  0.916205  0.730272   0.0       0.0\n 0.0       0.0        0.966811  0.414062   0.210912  0.0\n 0.0       0.0        0.0       0.0150353  0.135984  0.558082\n 0.0       0.0        0.0       0.0        0.428772  0.329567\n\njulia> bandwidth(A)\n1\n\nGenerate a 77 matrix with bandwidth 3 and band density 0.3:\n\njulia> using Random\n\njulia> A = random_banded_matrix(7, 3; p=0.3, rng=MersenneTwister(0402))\n7×7 Matrix{Float64}:\n 0.856072  0.720893  0.0       0.0       0.0       0.0        0.0\n 0.0       0.0       0.0       0.646516  0.845229  0.0        0.0\n 0.997473  0.773515  0.854375  0.926462  0.21636   0.0        0.0\n 0.0       0.516052  0.220979  0.844818  0.0       0.0395003  0.568892\n 0.0       0.402696  0.499802  0.0       0.304168  0.237423   0.0\n 0.0       0.0       0.0       0.0       0.0       0.0        0.877917\n 0.0       0.0       0.0       0.101071  0.0       0.0        0.829221\n\njulia> bandwidth(A)\n3\n\nGenerate an 88 diagonal (bandwidth 0) matrix with default band density (075):\n\njulia> using Random\n\njulia> A = random_banded_matrix(8, 0; rng=MersenneTwister(0102))\n8×8 Matrix{Float64}:\n 0.781618  0.0      0.0       0.0  0.0        0.0      0.0       0.0\n 0.0       0.56589  0.0       0.0  0.0        0.0      0.0       0.0\n 0.0       0.0      0.966643  0.0  0.0        0.0      0.0       0.0\n 0.0       0.0      0.0       0.0  0.0        0.0      0.0       0.0\n 0.0       0.0      0.0       0.0  0.0412729  0.0      0.0       0.0\n 0.0       0.0      0.0       0.0  0.0        0.70196  0.0       0.0\n 0.0       0.0      0.0       0.0  0.0        0.0      0.494806  0.0\n 0.0       0.0      0.0       0.0  0.0        0.0      0.0       0.227507\n\njulia> bandwidth(A)\n0\n\nNotes\n\nUsers of the MatrixBandwidth package may find this function useful when generating random test data for whatever frameworks, algorithms, etc. they are implementing.\n\n\n\n\n\n","category":"method"},{"location":"public_api/#MatrixBandwidth.Exact","page":"Public API","title":"MatrixBandwidth.Exact","text":"MatrixBandwidth.Exact\n\nExact solvers for matrix bandwidth minimization.\n\nExact methods are those which guarantee an optimal ordering producing the true minimum bandwidth of a matrix. Since bandwidth minimization is an NP-complete problem, existing exact algorithms are, at best, exponential in time complexity—much worse than many polynomial-time heuristic approaches (e.g., reverse Cuthill–McKee). Such methods, therefore, are not feasible for large matrices, but they remain useful when precise solutions are required for small-to-medium-sized inputs (say, up to 100100).\n\nThe following exact algorithms are currently supported:\n\nMBID: Minimum bandwidth by iterative deepening (MB-ID)\nMBPS: Minimum bandwidth by perimeter search (MB-PS)\n\nThis submodule is part of the MatrixBandwidth.jl package.\n\n\n\n\n\n","category":"module"},{"location":"public_api/#MatrixBandwidth.Exact.MBID","page":"Public API","title":"MatrixBandwidth.Exact.MBID","text":"MBID <: ExactSolver <: AbstractSolver\n\nTODO: Write here\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Exact.MBPS","page":"Public API","title":"MatrixBandwidth.Exact.MBPS","text":"MBPS <: ExactSolver <: AbstractSolver\n\nTODO: Write here\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Heuristic","page":"Public API","title":"MatrixBandwidth.Heuristic","text":"MatrixBandwidth.Heuristic\n\nHeuristic solvers for matrix bandwidth minimization.\n\nHeuristic methods are those which aim to produce near-optimal solutions in a more performant manner than exact methods. While precise bandwidth minimization is NP-complete, many heuristic algorithms (such as reverse Cuthill–McKee) run in polynomial time.\n\nHeuristic algorithms differ from metaheuristic ones in that they do not employ higher-level iterative search frameworks (e.g., stochastic techniques) to survey the global search space and escape local minima; instead, they rely on straightforward deterministic procedures to find good solutions in a single pass.\n\nThe following heuristic algorithms are currently supported:\n\nCuthillMcKee: Cuthill–McKee algorithm\nReverseCuthillMcKee: Reverse Cuthill–McKee algorithm\n\nThis submodule is part of the MatrixBandwidth.jl package.\n\n\n\n\n\n","category":"module"},{"location":"public_api/#MatrixBandwidth.Heuristic.CuthillMcKee","page":"Public API","title":"MatrixBandwidth.Heuristic.CuthillMcKee","text":"CuthillMcKee <: HeuristicSolver <: AbstractSolver\n\nThe Cuthill–McKee algorithm is a heuristic method for minimizing the bandwidth of a symmetric matrix A. It considers the graph G(A) whose adjacency matrix is A (ignoring self-loops) and performs a breadth-first search of each connected component of G(A), starting from a low-degree node then visiting its neighbors in order of increasing degree. Particularly effective when A is sparse, this heuristic typically produces an ordering which induces a matrix bandwidth either equal to or very close to the true minimum [CM69, pp. 157–58].\n\nWe also extend the algorithm to work more generally when A is not symmetric by applying it to A + Aᵀ instead, as suggested in [RS06, p. 808]. This approach still tends to produce a fairly good ordering, but it is not guaranteed to be as optimal as directly applying Cuthill–McKee to a symmetric input.\n\nFields\n\nnode_selector::Function: a function that selects a node from some connected component of   the input matrix from which to start the breadth-first search. If no custom heuristic is   specified, this field defaults to pseudo_peripheral_node, which picks a node   \"farthest\" from the others in the component (not necessarily the lowest-degree node).\n\nPerformance\n\nGiven an nn input matrix A, our implementation of Cuthill–McKee runs in O(n^2) time, where n is the number of rows/columns of the input matrix.\n\n[CG80] provide a linear-time implementation in the number of nonzero entries of A, which is still quadratic when A is dense but often much faster when dealing with sparse matrices. However, this would require that A be stored as a graph or a sparse matrix, which runs counter to our desire to provide a bandwidth minimization API for all AbstractMatrix{<:Number} types, including dense matrices. (In the future, however, we may indeed consider supporting this more performant implementation for sparse matrices.)\n\nIt was found in [Geo71, pp. 114–15] that reversing the ordering produced by Cuthill–McKee tends to induce a more optimal matrix profile (a measure of how far, on average, nonzero entries are from the diagonal). This so-called reverse Cuthill–McKee variant is preferred in almost all cases—see ReverseCuthillMcKee and the associated method of _bool_minimal_band_ordering for our implementation.\n\nExamples\n\nCuthill–McKee finds an optimal ordering for an asymmetric 3535 matrix whose rows and columns have been shuffled:\n\njulia> using Random\n\njulia> Random.seed!(13);\n\njulia> (n, k) = (35, 3);\n\njulia> A = random_banded_matrix(n, k);\n\njulia> perm = randperm(n);\n\njulia> A_shuffled = A[perm, perm];\n\njulia> iszero.(A) != iszero.(A') # Proof that the algorithm works for asymmetric input\ntrue\n\njulia> bandwidth(A)\n3\n\njulia> bandwidth(A_shuffled) # Much larger after shuffling\n31\n\njulia> res = minimize_bandwidth(A_shuffled, CuthillMcKee()) # Finds the true minimum\nResults of Matrix Bandwidth Minimization\n * Algorithm: Cuthill–McKee algorithm\n * Approach: Heuristic\n * Minimum bandwidth: 3\n * Original bandwidth: 31\n * Matrix size: 35×35\n\nCuthill–McKee finds a near-optimal ordering for an asymmetric 183183 matrix with multiple (separate) connected components whose rows and columns have been shuffled:\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(37452);\n\njulia> (max_cc_size, max_band, p, num_ccs) = (60, 9, 0.2, 7);\n\njulia> components = Vector{SparseMatrixCSC{Float64, Int64}}(undef, num_ccs);\n\njulia> for i in 1:num_ccs # Some components may themselves be disconnected\n           cc_size = rand(1:max_cc_size);\n           cc_band = rand(0:min(max_band, cc_size - 1));\n           components[i] = sparse(random_banded_matrix(cc_size, cc_band; p=p));\n       end\n\njulia> A = blockdiag(components...); # `A` has least 7 connected components\n\njulia> perm = randperm(sum(map(cc -> size(cc, 1), components)));\n\njulia> A_shuffled = A[perm, perm];\n\njulia> res = minimize_bandwidth(A_shuffled, CuthillMcKee());\n\njulia> A # The original matrix\n183×183 SparseMatrixCSC{Float64, Int64} with 408 stored entries:\n⎡⣜⣹⡤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠪⣿⣭⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠁⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠑⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢱⢆⢠⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠳⢽⡇⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⠽⡺⠦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⣷⣇⣂⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠣⣏⣾⣂⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠲⣻⣾⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠋⡏⢷⣦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠾⠠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⠄⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⢽⣟⣀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠹⠾⠶⡤⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠊⠾⡻⣦⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠃⡟⣵⢄⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠱⢹⣣⎦\n\njulia> A_shuffled # A far-from-optimal ordering of `A`\n183×183 SparseMatrixCSC{Float64, Int64} with 408 stored entries:\n⎡⠁⢄⡂⠀⠀⢀⠀⠀⠀⠂⠂⢀⠀⢀⠀⠀⠀⠀⠀⢀⠐⠠⠂⠀⠀⠀⢀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⎤\n⎢⠀⠈⠁⠤⠀⠄⠀⠀⠀⠀⠠⠀⢀⠀⠀⠁⠡⠀⠀⠀⠈⠀⠀⡀⠀⡀⠒⠀⠀⠘⠀⠀⠀⠀⡀⠀⠀⠀⠀⡄⎥\n⎢⠀⢠⠀⠀⠁⢄⠈⠀⠀⠀⠀⠀⠄⠀⠀⠀⠠⠀⠐⠁⠀⠠⠁⠠⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠤⠀⠀⎥\n⎢⠀⠀⠀⠀⠢⠀⠁⠅⠀⠀⠐⢐⠀⠂⠀⠀⠀⠀⠐⠐⠀⠐⠀⠀⠀⠀⠀⠀⠀⠄⠠⠀⠀⠐⠐⠀⠀⠀⠀⠂⎥\n⎢⠀⠀⠀⠀⠉⢀⢀⠁⠁⢀⠀⡈⠁⠐⠀⠀⠐⠀⠈⠀⠀⡀⠀⠀⠀⠀⠈⠀⠀⠈⠀⠀⠂⠀⠀⠐⠐⠀⠐⠀⎥\n⎢⠀⢀⢀⠀⡀⠁⠀⠀⠈⠀⠑⣀⡀⠀⠀⠀⠐⠀⠀⠈⠀⠀⠈⠇⠀⠀⠐⠀⠀⠀⠀⠀⠠⠀⠀⠄⠁⠂⠁⠐⎥\n⎢⠀⠀⠀⠐⠀⠈⠠⠀⠀⠈⡀⠀⢐⠌⠀⡁⡀⠐⠀⠰⠀⢀⠀⠄⠀⠄⡀⠀⠀⠀⠐⠔⡁⠀⠀⠀⠀⠀⡀⠄⎥\n⎢⠀⠀⠀⠠⠄⠀⠀⠀⠀⠀⠁⠀⠀⠀⠐⢈⠤⡀⠀⠄⠈⠀⠀⠀⠀⠄⠀⠁⠀⠀⠁⠀⠀⠀⠀⠀⠀⠀⢀⠀⎥\n⎢⠈⠀⠁⢀⠀⠀⠀⠀⠐⠀⢀⠀⠀⠂⠀⠠⠑⠀⠀⠀⠈⠂⠀⠂⠐⡀⢀⠤⠂⠀⠐⠀⠀⠀⠐⡠⠀⠂⠀⠀⎥\n⎢⢀⠀⠀⢀⠆⡠⠂⠀⠂⠀⠀⠀⠄⠀⠀⠂⠀⠁⠀⢀⠀⠔⠀⠀⠀⡀⠀⠀⠂⢂⠀⠀⠀⢀⠀⠀⠀⢀⠀⠃⎥\n⎢⠐⡀⠂⡀⠀⠀⢀⠀⢄⠠⡀⠀⠀⠀⠀⠀⠀⠈⡁⡀⠀⢤⠒⡀⠀⠀⠀⠈⠀⠀⠠⠀⠀⠐⠀⠁⠀⠀⡀⠀⎥\n⎢⠀⠀⠀⠠⠠⠀⠀⠀⠁⠀⠂⠈⠀⠀⠀⡀⠁⢤⠄⠈⠈⠠⠀⠄⠈⠀⠀⠠⠀⠀⠀⠀⠈⠀⠠⠌⡀⠈⠠⠀⎥\n⎢⠀⠐⠀⠀⠀⠀⠠⠀⠀⠀⠠⠀⠀⠀⠀⠀⠄⠁⠀⠲⠀⠀⠀⠀⠐⠀⠂⠀⠀⠀⠀⠐⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠠⠀⠀⠐⠀⠀⠀⠈⠀⠀⠀⠀⠀⡀⠀⠀⡀⠀⠀⡄⠀⠀⠐⠄⠀⠀⠐⠀⠊⠀⠐⡢⠈⠀⠀⢀⎥\n⎢⠀⠀⡠⠀⠁⡀⠌⠄⠀⠀⠀⡀⠀⠀⠀⠀⠀⠀⠌⠀⠀⠀⠀⠀⠀⠀⣀⠀⠑⠀⢀⡀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⡐⢂⠀⠅⠀⠂⠀⠀⠀⠀⠀⠀⠄⠀⠀⢀⠀⠠⡀⠂⠄⡀⠀⠀⠂⠀⠀⠘⠑⠀⠀⠀⠀⠀⠠⠢⠀⠀⎥\n⎢⠀⠀⠂⠀⠠⢀⠀⠀⠀⠀⠀⢀⠀⠄⠀⠄⠀⠄⠄⠠⢀⠀⠀⠀⠀⠀⠁⠠⠀⢘⠀⠠⠐⠀⠁⠀⠀⢀⠠⠀⎥\n⎢⠀⢂⠀⠀⠀⠀⠀⠀⠐⠀⠀⡀⡀⠀⠀⢀⠀⠈⠀⠀⠀⠀⡀⠂⠐⠀⠸⠨⡀⠀⠀⠀⢀⡀⠱⠄⠈⠀⠀⠈⎥\n⎢⢀⠐⠀⠠⠀⠂⠈⠀⠀⠀⠀⠀⠂⠡⠀⠄⠀⠀⠀⢠⠀⠠⡀⠀⠀⠀⠀⠀⠀⢐⠀⠀⠀⠐⠂⠀⠁⢐⠂⠀⎥\n⎣⠀⠀⠀⠄⠀⢀⠀⠀⠀⠀⠁⠀⠠⠀⠀⠤⠀⠂⠤⠀⠀⠀⠀⠂⠈⠀⠀⠐⠀⠀⠀⠀⠀⠀⠠⠠⠈⠄⠀⠄⎦\n\njulia> A_shuffled[res.ordering, res.ordering] # A near-optimal reordering of `A_shuffled`\n183×183 SparseMatrixCSC{Float64, Int64} with 408 stored entries:\n⎡⢱⣶⣤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠹⡵⣉⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠈⠢⡤⡤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⢳⡺⣺⠆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠛⡗⡻⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⡎⣽⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣟⣎⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠘⠬⢅⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢳⣓⠆⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢼⣥⣒⢂⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢲⣫⣾⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠳⣜⡼⡤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠃⣽⠳⣲⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠐⠛⢄⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⣘⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠁⠃⢦⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢄⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⠄⠀⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⠀⎦\n\njulia> iszero.(A) != iszero.(A') # Proof that the algorithm works for asymmetric input\ntrue\n\njulia> bandwidth(A)\n7\n\njulia> bandwidth(A_shuffled) # Much larger after shuffling\n170\n\njulia> res # Gets very close to the true minimum\nResults of Matrix Bandwidth Minimization\n * Algorithm: Cuthill–McKee algorithm\n * Approach: Heuristic\n * Minimum bandwidth: 10\n * Original bandwidth: 170\n * Matrix size: 183×183\n\nNotes\n\nNote that the node_selector field must be of the form (A::AbstractMatrix{Bool}) -> Integer (i.e., it must take in an boolean matrix and return an integer). If this is not the case, an ArgumentError is thrown upon construction.\n\nSee also the documentation for supertypes HeuristicSolver and AbstractSolver.\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Heuristic.ReverseCuthillMcKee","page":"Public API","title":"MatrixBandwidth.Heuristic.ReverseCuthillMcKee","text":"ReverseCuthillMcKee <: HeuristicSolver <: AbstractSolver\n\nThe reverse Cuthill–McKee algorithm is a variant of the Cuthill–McKee algorithm—a heuristic method for minimizing the bandwidth of a symmetric matrix A. Cuthill–McKee considers the graph G(A) whose adjacency matrix is A (ignoring self-loops) and performs a breadth-first search of each connected component of G(A), starting from a low-degree node then visiting its neighbors in order of increasing degree. Particularly effective when A is sparse, this heuristic typically produces an ordering which induces a matrix bandwidth either equal to or very close to the true minimum [CM69, pp. 157–58]. The reverse Cuthill–McKee algorithm simply reverses the ordering produced by application of Cuthill–McKee; it was found in [Geo71, pp. 114–15] that although the bandwidth remains the same, this tends to produce a more optimal matrix profile (a measure of how far, on average, nonzero entries are from the diagonal).\n\nWe also extend the algorithm to work more generally when A is not symmetric by applying it to A + Aᵀ instead, as suggested in [RS06, p. 808]. This approach still tends to produce a fairly good ordering, but it is not guaranteed to be as optimal as directly applying reverse Cuthill–McKee to a symmetric input.\n\nPerformance\n\nGiven an nn input matrix A, our implementation of Cuthill–McKee runs in O(n^2) time, where n is the number of rows/columns of the input matrix.\n\n[CG80] provide a linear-time implementation in the number of nonzero entries of A, which is still quadratic when A is dense but often much faster when dealing with sparse matrices. However, this would require that A be stored as a graph or a sparse matrix, which runs counter to our desire to provide a bandwidth minimization API for all AbstractMatrix{<:Number} types, including dense matrices. (In the future, however, we may indeed consider supporting this more performant implementation for sparse matrices.)\n\nFields\n\nnode_selector::Function: a function that selects a node from some connected component of   the input matrix from which to start the breadth-first search. If no custom heuristic is   specified, this field defaults to pseudo_peripheral_node, which picks a node   \"farthest\" from the others in the component (not necessarily the lowest-degree node).\n\nExamples\n\nReverse Cuthill–McKee finds an optimal ordering for an asymmetric 4545 matrix whose rows and columns have been shuffled:\n\njulia> using Random\n\njulia> Random.seed!(87);\n\njulia> (n, k) = (45, 4);\n\njulia> A = random_banded_matrix(n, k);\n\njulia> perm = randperm(n);\n\njulia> A_shuffled = A[perm, perm];\n\njulia> iszero.(A) != iszero.(A') # Proof that the algorithm works for asymmetric input\ntrue\n\njulia> bandwidth(A)\n4\n\njulia> bandwidth(A_shuffled) # Much larger after shuffling\n44\n\njulia> res = minimize_bandwidth(A_shuffled, ReverseCuthillMcKee()) # Finds the true minimum\nResults of Matrix Bandwidth Minimization\n * Algorithm: Reverse Cuthill–McKee algorithm\n * Approach: Heuristic\n * Minimum bandwidth: 4\n * Original bandwidth: 44\n * Matrix size: 45×45\n\nReverse Cuthill–McKee finds a near-optimal ordering for an asymmetric 251251 matrix with multiple (separate) connected components whose rows and columns have been shuffled:\n\njulia> using Random, SparseArrays\n\njulia> Random.seed!(5747);\n\njulia> (max_cc_size, max_band, p, num_ccs) = (60, 9, 0.2, 8);\n\njulia> components = Vector{SparseMatrixCSC{Float64, Int64}}(undef, num_ccs);\n\njulia> for i in 1:num_ccs # Some components may themselves be disconnected\n           cc_size = rand(0:max_cc_size);\n           cc_band = rand(1:min(max_band, cc_size - 1));\n           components[i] = sparse(random_banded_matrix(cc_size, cc_band; p=p));\n       end\n\njulia> A = blockdiag(components...); # `A` has least 8 connected components\n\njulia> perm = randperm(sum(map(cc -> size(cc, 1), components)));\n\njulia> A_shuffled = A[perm, perm];\n\njulia> res = minimize_bandwidth(A_shuffled, ReverseCuthillMcKee());\n\njulia> A # The original matrix\n251×251 SparseMatrixCSC{Float64, Int64} with 617 stored entries:\n⎡⢛⣷⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠈⢿⣇⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠘⠻⣦⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠉⢿⣶⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠘⠹⣢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢿⣷⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠬⣣⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠿⣵⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⢛⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢾⣳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢻⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⡓⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⡥⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠾⢇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⡆⡀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠿⣡⡄⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠻⣤⡀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠳⣆⡀⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣆⎦\n\njulia> A_shuffled # A far-from-optimal ordering of `A`\n251×251 SparseMatrixCSC{Float64, Int64} with 617 stored entries:\n⎡⠑⢅⠀⠀⡠⠁⡀⠀⠨⢀⠀⠀⠄⠀⠀⡂⠀⡁⠒⢄⠀⠂⢀⠀⠀⠐⣁⢀⠀⢂⠠⠁⡠⠢⢀⡂⠀⠀⢀⡀⎤\n⎢⠀⠀⠩⢂⠀⠀⠀⣀⠈⠈⠁⠀⠀⠠⠈⠀⠀⠀⠄⠀⠀⠒⠀⠀⠀⠀⠀⠈⠀⠁⠈⡀⠀⠀⠠⡀⠀⠀⠄⠀⎥\n⎢⡀⠦⠀⠀⠑⠄⠀⠀⠠⠀⠠⠄⠀⠀⢀⠌⠀⠀⠀⠈⢠⠀⠁⡀⠢⠀⠀⠠⡁⠂⡔⡀⠁⠀⢂⠀⠠⢁⠀⠁⎥\n⎢⠠⠈⠄⣠⠐⠀⠐⢄⡁⠀⠀⡀⢰⠀⠁⠀⠈⠀⠀⠀⢀⠂⡀⠀⢀⠀⠁⠂⢂⠈⠀⢀⠀⠀⠀⠈⠀⠈⡀⡠⎥\n⎢⠀⠀⠀⠄⠀⡈⠀⠀⠵⠂⠀⠀⠀⢠⠀⠂⠒⠀⠀⠀⠐⠠⠀⠀⢄⠀⣐⠆⠀⠐⠀⠠⠀⠀⠨⠄⠦⡀⠀⡀⎥\n⎢⠠⠀⢀⠀⠀⠀⠀⠠⠀⠈⡐⢌⠀⠂⠂⠈⠀⠔⠀⠁⣀⠀⢀⠀⠠⠀⠀⠐⠀⠀⠐⠁⠀⠄⠀⠂⠀⠀⠂⢀⎥\n⎢⢨⢁⠀⠠⠀⠀⠀⠄⡀⡀⠀⠁⠂⠂⠁⠐⢀⠀⠀⢁⠐⠘⢂⠀⠀⠀⡀⠀⠔⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠀⎥\n⎢⠢⠂⠂⡀⠀⠔⠀⢂⠀⠀⢨⠀⢀⠀⠐⠐⠀⠀⠀⠂⠐⠄⠈⠀⡀⠂⠁⠤⢀⠂⢀⠀⡀⢀⠀⠀⠀⠀⠀⠀⎥\n⎢⠂⠀⠀⠀⠀⠀⠂⠀⠄⠂⢀⠁⠀⠰⡀⠀⢕⢐⠀⠀⠀⠀⠂⡀⠠⠄⠀⠁⠂⠈⠀⠄⠚⠀⠐⠀⠀⠀⢀⠁⎥\n⎢⠀⠄⠀⠁⠁⠀⡀⠀⠀⠀⠁⠀⠄⠘⡀⠠⡊⠀⠒⠄⠐⠂⠀⡂⠁⠀⠂⠀⠀⠀⠀⢄⠁⠀⠀⠀⠀⠀⠠⡀⎥\n⎢⠈⠘⠀⠂⠀⠀⠀⠐⠀⠃⠀⠀⠀⡀⠀⠄⠁⠀⠄⠀⠎⢑⠀⢂⠀⠈⠁⠀⠀⢀⠀⢀⠀⢀⠰⡁⠀⠢⠀⠀⎥\n⎢⢄⠀⡀⢀⠃⠀⠁⡈⠁⠀⠠⠀⠀⠄⠄⠠⠠⠀⠄⠀⠒⠁⠱⠆⠀⠀⠀⠀⡀⠀⠀⠄⠁⠁⢀⠀⠀⠊⠁⠀⎥\n⎢⠀⠀⠀⠀⠈⠀⠀⠀⠀⠀⠀⠐⠂⠀⠀⡄⠀⠠⠀⠂⠀⠀⠀⠘⠈⠔⠀⠀⠀⠀⠀⠀⠁⠐⢁⢀⠈⡀⠠⠐⎥\n⎢⠀⠐⡀⠀⠠⢠⠠⠀⠒⢀⠀⠀⠀⠈⠀⡄⠀⠄⠀⠀⠈⠀⠁⠀⠀⠀⢶⢐⠄⠀⠀⠤⠀⠀⠐⡐⡘⠀⡁⠄⎥\n⎢⠁⠀⠒⠀⠀⡀⠐⠁⡂⠀⠀⠀⠔⢀⠈⠀⠈⠀⠀⠒⠀⠄⠂⠀⠒⠀⠀⠀⠀⠄⡀⢐⠐⢄⡄⠀⢀⠀⡄⠈⎥\n⎢⠀⠀⠀⠀⠑⠀⠀⠀⠀⠀⠄⠀⠀⠀⠐⠸⠀⠄⠀⢄⠀⠀⡀⡄⠁⠆⢀⠀⠠⠀⠑⠀⠅⢁⠀⠀⠨⠄⠀⠐⎥\n⎢⠠⡂⠀⠀⠀⠂⡀⡀⠀⠁⠀⠀⠀⡠⠀⠀⠐⠀⠀⡉⠀⠀⠀⢀⠐⠀⠉⠀⠀⢄⠀⠁⠐⢔⢀⡀⢂⠀⠀⠀⎥\n⎢⢀⠀⠀⠀⠀⠐⠀⠀⠀⠀⠀⠐⠀⠐⠀⠀⢄⠠⠀⠀⠀⠰⢀⡐⠂⢐⠀⠁⠀⠁⠀⠀⢀⠐⠅⢅⠄⢀⠂⠈⎥\n⎢⠀⠀⠀⠀⠢⢀⠠⠀⠈⠂⠀⠀⡀⠐⡀⠀⠑⠐⠂⠈⠪⠐⠈⠂⠀⠀⠂⠈⠀⠐⠀⠄⠀⠄⠀⠐⠑⢀⠐⠀⎥\n⎣⠀⠀⡀⠁⠐⠀⠀⠈⠀⠉⠉⢀⡀⠀⠁⠀⡈⠀⠀⠂⠀⡀⠀⠀⠀⠂⠀⡀⡄⠀⠁⠂⠁⠀⠀⠀⠀⠀⠑⢆⎦\n\njulia> A_shuffled[res.ordering, res.ordering] # A near-optimal reordering of `A_shuffled`\n251×251 SparseMatrixCSC{Float64, Int64} with 617 stored entries:\n⎡⠑⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤\n⎢⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠈⢿⣷⣠⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠈⠹⢷⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠈⢻⡶⣆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⣾⣧⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣧⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢛⣟⣦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠓⢿⣲⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠻⡦⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠿⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⣶⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⠫⣶⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠿⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢻⣴⡆⠀⠀⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⢱⢶⡄⠀⠀⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣦⣄⠀⠀⠀⎥\n⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠾⣷⡀⠀⎥\n⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠯⣆⎦\n\njulia> iszero.(A) != iszero.(A') # Proof that the algorithm works for asymmetric input\ntrue\n\njulia> bandwidth(A)\n7\n\njulia> bandwidth(A_shuffled) # Much larger after shuffling\n239\n\njulia> res # Gets very close to the true minimum\nResults of Matrix Bandwidth Minimization\n * Algorithm: Reverse Cuthill–McKee algorithm\n * Approach: Heuristic\n * Minimum bandwidth: 10\n * Original bandwidth: 239\n * Matrix size: 251×251\n\nNotes\n\nNote that the node_selector field must be of the form (A::AbstractMatrix{Bool}) -> Integer (i.e., it must take in an boolean matrix and return an integer). If this is not the case, an ArgumentError is thrown upon construction.\n\nSee also the documentation for supertypes HeuristicSolver and AbstractSolver, as well as CuthillMcKee for the original non-reversed algorithm. (Indeed, the reverse Cuthill–McKee method of _bool_minimal_band_ordering is merely a wrapper around the Cuthill–McKee method.)\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Metaheuristic","page":"Public API","title":"MatrixBandwidth.Metaheuristic","text":"MatrixBandwidth.Metaheuristic\n\nMetaheuristic solvers for matrix bandwidth minimization.\n\nThis submodule is part of the MatrixBandwidth.jl package.\n\n\n\n\n\n","category":"module"},{"location":"public_api/#MatrixBandwidth.Metaheuristic.GRASP","page":"Public API","title":"MatrixBandwidth.Metaheuristic.GRASP","text":"GRASP <: MetaheuristicSolver <: AbstractSolver\n\nTODO: Write here\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Metaheuristic.GeneticAlgorithm","page":"Public API","title":"MatrixBandwidth.Metaheuristic.GeneticAlgorithm","text":"GeneticAlgorithm <: MetaheuristicSolver <: AbstractSolver\n\nTODO: Write here\n\n\n\n\n\n","category":"type"},{"location":"public_api/#MatrixBandwidth.Metaheuristic.SimulatedAnnealing","page":"Public API","title":"MatrixBandwidth.Metaheuristic.SimulatedAnnealing","text":"SimulatedAnnealing <: MetaheuristicSolver <: AbstractSolver\n\nTODO: Write here\n\n\n\n\n\n","category":"type"},{"location":"public_api/#References","page":"Public API","title":"References","text":"","category":"section"},{"location":"public_api/","page":"Public API","title":"Public API","text":"W. M. Chan and A. George. A linear time implementation of the reverse Cuthill-McKee algorithm. BIT Numerical Mathematics 20, 8–14 (1980).\n\n\n\nE. Cuthill and J. McKee. Reducing the bandwidth of sparse symmetric matrices. In: Proceedings of the 24th National Conference of the ACM (Brandon Systems Press, 1969); pp. 157–72.\n\n\n\nJ. A. George. Computer Implementation of the Finite Element Method. Ph.D. Thesis, Department of Computer Science, Stanford University (1971).\n\n\n\nJ. K. Reid and J. A. Scott. Reducing the Total Bandwidth of a Sparse Unsymmetric Matrix. SIAM Journal on Matrix Analysis and Applications 28, 805–21 (2006).\n\n\n\n","category":"page"},{"location":"private_api/#MatrixBandwidth.jl-–-Private-API","page":"Private API","title":"MatrixBandwidth.jl – Private API","text":"","category":"section"},{"location":"private_api/","page":"Private API","title":"Private API","text":"Documentation for MatrixBandwidth's private API.","category":"page"},{"location":"private_api/","page":"Private API","title":"Private API","text":"note: Note\nThe following documentation covers only the private API of the package. For public details, see the public API documentation.","category":"page"},{"location":"private_api/#MatrixBandwidth.AbstractSolver","page":"Private API","title":"MatrixBandwidth.AbstractSolver","text":"AbstractSolver\n\nAbstract base type for all matrix bandwidth minimization solvers.\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.NotImplementedError","page":"Private API","title":"MatrixBandwidth.NotImplementedError","text":"NotImplementedError(f, arg, subtype, abstracttype)\n\nAn exception indicating that a function lacks dispatch to handle a specific argument type.\n\nSemantically, this differs from MethodError in that it connotes a developer-side failure to implement a method rather than erroneous user input. Throughout this package, it is often used to warn when an existing function with multiple dispatch on some abstract type is called on a newly created subtype for which no method has been defined.\n\nFields\n\nf::Function: the function called.\narg::Symbol: the name of the argument with the unsupported type.\nsubtype::Type: the type of the argument. May be the actual concrete type or some   intermediate supertype. (For instance, if the relevant input has concrete type A with   hierarchy A <: B <: C and the abstracttype field is C, then both A and B are   perfectly valid choices for subtype.)\nabstracttype::Type: the abstract type under which the argument is meant to fall.\n\nConstructors\n\nNotImplementedError(::Function, ::Symbol, ::Type, ::Type): constructs a new   NotImplementedError instance. Throws an error if the second type is not abstract or   the first type is not a subtype of the second.\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.Exact.ExactSolver","page":"Private API","title":"MatrixBandwidth.Exact.ExactSolver","text":"Exact <: AbstractSolver\n\nAbstract type for all exact matrix bandwidth minimization solvers.\n\nExact methods are those which guarantee an optimal ordering producing the true minimum bandwidth of a matrix. Since bandwidth minimization is an NP-complete problem, existing exact algorithms are, at best, exponential in time complexity—much worse than many polynomial-time heuristic approaches (e.g., reverse Cuthill–McKee). Such methods, therefore, are not feasible for large matrices, but they remain useful when precise solutions are required for small-to-medium-sized inputs (say, up to 100100).\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.Heuristic.HeuristicSolver","page":"Private API","title":"MatrixBandwidth.Heuristic.HeuristicSolver","text":"HeuristicSolver <: AbstractSolver\n\nAbstract type for all heuristic matrix bandwidth minimization solvers.\n\nHeuristic methods are those which aim to produce near-optimal solutions in a more performant manner than exact methods. While precise bandwidth minimization is NP-complete, many heuristic algorithms (such as reverse Cuthill–McKee) run in polynomial time.\n\nHeuristic algorithms differ from metaheuristic ones in that they do not employ higher-level iterative search frameworks (e.g., stochastic techniques) to survey the global search space and escape local minima; instead, they rely on straightforward deterministic procedures to find good solutions in a single pass.\n\n\n\n\n\n","category":"type"},{"location":"private_api/#MatrixBandwidth.Heuristic.pseudo_peripheral_node-Tuple{AbstractMatrix{Bool}}","page":"Private API","title":"MatrixBandwidth.Heuristic.pseudo_peripheral_node","text":"pseudo_peripheral_node(A::AbstractMatrix{Bool}) -> Int\n\nSelect a pseudo-peripheral node from the connected graph represented by A.\n\nThis function acts as a node selector for the Cuthill–McKee and Reverse Cuthill–McKee algorithms, heuristically choosing the node \"farthest\" from the others in the graph. It is assumed that A is the adjacency matrix of some connected, undirected graph; otherwise, undefined behavior may arise.\n\nArguments\n\nA::AbstractMatrix{Bool}: the adjacency matrix of some connected, undirected graph. In   practice, this semantically represents the connected component of some larger graph.\n\nReturns\n\nInt: the index of the pseudo-peripheral node selected from the graph.\n\nNotes\n\nThis function takes heavy inspiration from the implementation in [Net25], which accepts a graph object as input and leverages several pre-existing functions in the networkx library. We herein repurpose the logic to work directly on adjacency matrices, avoiding reallocation overhead and an unnecessary dependency on Graphs.jl.\n\n\n\n\n\n","category":"method"},{"location":"private_api/#MatrixBandwidth.Metaheuristic.MetaheuristicSolver","page":"Private API","title":"MatrixBandwidth.Metaheuristic.MetaheuristicSolver","text":"MetaheuristicSolver <: AbstractSolver\n\nTODO: Write here\n\n\n\n\n\n","category":"type"},{"location":"private_api/#References","page":"Private API","title":"References","text":"","category":"section"},{"location":"private_api/","page":"Private API","title":"Private API","text":"NetworkX Developers. Source code for networkx.utils.rcm. NetworkX v3.5 documentation (2025). Accessed: 2025-06-11.\n\n\n\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"<table>\n  <tr>\n    <td>Metadata</td>\n    <td>\n      <img src=\"https://img.shields.io/badge/version-v0.1.0--dev-pink.svg\" alt=\"Version\">\n      <a href=\"https://opensource.org/licenses/MIT\"><img src=\"https://img.shields.io/badge/License-MIT-A31F34.svg\" alt=\"License: MIT\"></a>\n      <a href=\"https://github.com/JuliaDiff/BlueStyle\"><img src=\"https://img.shields.io/badge/code%20style-blue-4495d1.svg\" alt=\"Code Style: Blue\"></a>\n    </td>\n  </tr>\n  <tr>\n    <td>Documentation</td>\n    <td>\n      <a href=\"https://luis-varona.github.io/MatrixBandwidth.jl/stable/\"><img src=\"https://img.shields.io/badge/docs-stable-darkgreen.svg\" alt=\"Documentation of latest stable version\"></a>\n      <a href=\"https://luis-varona.github.io/MatrixBandwidth.jl/dev/\"><img src=\"https://img.shields.io/badge/docs-dev-rebeccapurple.svg\" alt=\"Documentation of dev version\"></a>\n    </td>\n  </tr>\n  <tr>\n    <td>Continuous integration</td>\n    <td>\n      <a href=\"https://github.com/Luis-Varona/MatrixBandwidth.jl/actions?query=workflow%3ACI+branch%3Amain\"><img src=\"https://github.com/Luis-Varona/MatrixBandwidth.jl/actions/workflows/CI.yml/badge.svg\" alt=\"GitHub Workflow Status\"></a>\n    </td>\n  </tr>\n  <tr>\n    <td>Code coverage</td>\n    <td>\n      <a href=\"https://codecov.io/gh/Luis-Varona/MatrixBandwidth.jl\"><img src=\"https://codecov.io/gh/Luis-Varona/MatrixBandwidth.jl/branch/main/graph/badge.svg\" alt=\"Test coverage from codecov\"></a>\n    </td>\n    </tr>\n    <tr>\n      <td>Static analysis with</td>\n      <td>\n        <a href=\"https://github.com/JuliaTesting/Aqua.jl\"><img src=\"https://raw.githubusercontent.com/JuliaTesting/Aqua.jl/master/badge.svg\" alt=\"Aqua QA\"></a>\n        <a href=\"https://github.com/aviatesk/JET.jl\"><img src=\"https://img.shields.io/badge/%E2%9C%88%20tested%20with-JET.jl%EF%B8%8F-9cf.svg\" alt=\"JET static analysis\"></a>\n      </td>\n    </tr>\n</table>","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MatrixBandwidth.jl offers several exact, heuristic, and metaheuristic algorithms for matrix bandwidth minimization.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The bandwidth of a square matrix A is the minimum non-negative integer k in mathbbN such that A_ij = 0 whenever i - j  k. Equivalently, A has bandwidth at most k if all entries above the k^textth superdiagonal and below the k^textth subdiagonal are zero, and A has bandwidth at least k if there exists any nonzero entry in the k^textth superdiagonal or subdiagonal.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The matrix bandwidth minimization problem entails finding a permutation matrix P so that the bandwidth of PAP^mathsfT is minimized; this is known to be NP-complete. Several heuristic algorithms (such as reverse Cuthill–McKee) run in polynomial time while still producing near-optimal orderings in practice, but exact methods (like MB-PS) are exponential in time complexity and thus are only feasible for relatively small matrices.","category":"page"},{"location":"#Algorithms","page":"Home","title":"Algorithms","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The following algorithms are currently supported:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Exact\nMinimum bandwidth by iterative deepening (MB-ID)\nMinimum bandwidth by perimeter search (MB-PS)\nHeuristic\nCuthill–McKee algorithm\nReverse Cuthill–McKee algorithm\nMetaheuristic\nSimulated annealing\nGenetic algorithm\nGreedy randomized adaptive search procedure (GRASP)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(As we remain in the early stages of development, some of these may not yet be fully implemented and/or tested.)","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The only prerequisite is a working Julia installation (v1.10 or later). First, enter Pkg mode by typing ] in the Julia REPL, then run the following command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add https://github.com/Luis-Varona/MatrixBandwidth.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"When MatrixBandwidth.jl is finally added to the official Julia registry, you will be able to install it more easily with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add MatrixBandwidth","category":"page"},{"location":"#Citing","page":"Home","title":"Citing","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"I encourage you to cite this work if you have found any of the algorithms herein useful for your research. Starring the MatrixBandwidth.jl repository on GitHub is also appreciated.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The latest citation information may be found in the CITATION.bib file within the repository.","category":"page"},{"location":"#Project-status","page":"Home","title":"Project status","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"I aim to release the first stable version of MatrixBandwidth.jl in early July 2025. The current version is a work-in-progress, with much of the API still under development.","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
