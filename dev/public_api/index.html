<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Public API · MatrixBandwidth.jl</title><meta name="title" content="Public API · MatrixBandwidth.jl"/><meta property="og:title" content="Public API · MatrixBandwidth.jl"/><meta property="twitter:title" content="Public API · MatrixBandwidth.jl"/><meta name="description" content="Documentation for MatrixBandwidth.jl."/><meta property="og:description" content="Documentation for MatrixBandwidth.jl."/><meta property="twitter:description" content="Documentation for MatrixBandwidth.jl."/><meta property="og:url" content="https://Luis-Varona.github.io/MatrixBandwidth.jl/public_api/"/><meta property="twitter:url" content="https://Luis-Varona.github.io/MatrixBandwidth.jl/public_api/"/><link rel="canonical" href="https://Luis-Varona.github.io/MatrixBandwidth.jl/public_api/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/styles.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">MatrixBandwidth.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Public API</a><ul class="internal"><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../private_api/">Private API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Public API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Public API</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/Luis-Varona/MatrixBandwidth.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/main/docs/src/public_api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="MatrixBandwidth.jl-–-Public-API"><a class="docs-heading-anchor" href="#MatrixBandwidth.jl-–-Public-API">MatrixBandwidth.jl – Public API</a><a id="MatrixBandwidth.jl-–-Public-API-1"></a><a class="docs-heading-anchor-permalink" href="#MatrixBandwidth.jl-–-Public-API" title="Permalink"></a></h1><p>Documentation for <a href="https://github.com/Luis-Varona/MatrixBandwidth.jl">MatrixBandwidth</a>&#39;s public API.</p><div class="admonition is-info" id="Note-d315dd05c58bb28f"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-d315dd05c58bb28f" title="Permalink"></a></header><div class="admonition-body"><p>The following documentation covers only the public API of the package. For internal details, see the <a href="../private_api/">private API documentation</a>.</p></div></div><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.MatrixBandwidth" href="#MatrixBandwidth.MatrixBandwidth"><code>MatrixBandwidth.MatrixBandwidth</code></a> — <span class="docstring-category">Module</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MatrixBandwidth</code></pre><p>Fast algorithms for matrix bandwidth minimization and matrix bandwidth recognition in Julia.</p><p>The <em>bandwidth</em> of an <span>$n×n$</span> matrix <span>$A$</span> is the minimum non-negative integer <span>$k ∈ [0, n - 1]$</span> such that <span>$A[i, j] = 0$</span> whenever <span>$|i - j| &gt; k$</span>. Equivalently, <span>$A$</span> has bandwidth <em>at most</em> <span>$k$</span> if all entries above the <span>$k$</span>ᵗʰ superdiagonal and below the <span>$k$</span>ᵗʰ subdiagonal are zero, and <span>$A$</span> has bandwidth <em>at least</em> <span>$k$</span> if there exists any nonzero entry in the <span>$k$</span>ᵗʰ superdiagonal or subdiagonal.</p><p>The <em>matrix bandwidth minimization problem</em> involves finding a permutation matrix <span>$P$</span> such that the bandwidth of <span>$PAPᵀ$</span> is minimized; this is known to be NP-complete. Several heuristic algorithms (such as Gibbs–Poole–Stockmeyer) run in polynomial time while still producing near-optimal orderings in practice, but exact methods (like Caprara–Salazar-González) are at least exponential in time complexity and thus are only feasible for relatively small matrices.</p><p>On the other hand, the <em>matrix bandwidth recognition problem</em> entails determining whether there exists a permutation matrix <span>$P$</span> such that the bandwidth of <span>$PAPᵀ$</span> is at most some fixed non-negative integer <span>$k ∈ ℕ$</span>—an optimal permutation that fully minimizes the bandwidth of <span>$A$</span> is not required. Unlike the NP-hard minimization problem, this is decidable in <span>$O(nᵏ)$</span> time.</p><p>The following algorithms are currently supported:</p><ul><li><strong>Minimization</strong><ul><li><em>Exact</em><ul><li>Caprara–Salazar-González algorithm (<a href="#MatrixBandwidth.Minimization.Exact.CapraraSalazarGonzalez"><code>Minimization.CapraraSalazarGonzalez</code></a>)</li><li>Del Corso–Manzini algorithm (<a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManzini"><code>Minimization.DelCorsoManzini</code></a>)</li><li>Del Corso–Manzini algorithm with perimeter search   (<a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS"><code>Minimization.DelCorsoManziniWithPS</code></a>)</li><li>Saxe–Gurari–Sudborough algorithm (<a href="#MatrixBandwidth.Minimization.Exact.SaxeGurariSudborough"><code>Minimization.SaxeGurariSudborough</code></a>)</li><li>Brute-force search (<a href="#MatrixBandwidth.Minimization.Exact.BruteForceSearch"><code>Minimization.BruteForceSearch</code></a>)</li></ul></li><li><em>Heuristic</em><ul><li>Gibbs–Poole–Stockmeyer algorithm (<a href="#MatrixBandwidth.Minimization.Heuristic.GibbsPooleStockmeyer"><code>Minimization.GibbsPooleStockmeyer</code></a>)</li><li>Cuthill–McKee algorithm (<a href="#MatrixBandwidth.Minimization.Heuristic.CuthillMcKee"><code>Minimization.CuthillMcKee</code></a>)</li><li>Reverse Cuthill–McKee algorithm (<a href="#MatrixBandwidth.Minimization.Heuristic.ReverseCuthillMcKee"><code>Minimization.ReverseCuthillMcKee</code></a>)</li></ul></li><li><em>Metaheuristic</em><ul><li>Greedy randomized adaptive search procedure (GRASP) (<a href="#MatrixBandwidth.Minimization.Metaheuristic.GRASP"><code>Minimization.GRASP</code></a>)</li><li>Simulated annealing (<a href="#MatrixBandwidth.Minimization.Metaheuristic.SimulatedAnnealing"><code>Minimization.SimulatedAnnealing</code></a>)</li><li>Genetic algorithm (<a href="#MatrixBandwidth.Minimization.Metaheuristic.GeneticAlgorithm"><code>Minimization.GeneticAlgorithm</code></a>)</li></ul></li></ul></li><li><strong>Recognition</strong><ul><li>Caprara–Salazar-González algorithm (<a href="#MatrixBandwidth.Recognition.CapraraSalazarGonzalez"><code>Recognition.CapraraSalazarGonzalez</code></a>)</li><li>Del Corso–Manzini algorithm (<a href="#MatrixBandwidth.Recognition.DelCorsoManzini"><code>Recognition.DelCorsoManzini</code></a>)</li><li>Del Corso–Manzini algorithm with perimeter search   (<a href="#MatrixBandwidth.Recognition.DelCorsoManziniWithPS"><code>Recognition.DelCorsoManziniWithPS</code></a>)</li><li>Saxe–Gurari–Sudborough algorithm (<a href="#MatrixBandwidth.Recognition.SaxeGurariSudborough"><code>Recognition.SaxeGurariSudborough</code></a>)</li><li>Brute-force search (<a href="#MatrixBandwidth.Recognition.BruteForceSearch"><code>Recognition.BruteForceSearch</code></a>)</li></ul></li></ul><p>The full documentation is available at <a href="https://luis-varona.github.io/MatrixBandwidth.jl/">GitHub Pages</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/MatrixBandwidth.jl#L7-L58">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.bandwidth-Tuple{AbstractMatrix{&lt;:Number}}" href="#MatrixBandwidth.bandwidth-Tuple{AbstractMatrix{&lt;:Number}}"><code>MatrixBandwidth.bandwidth</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">bandwidth(A) -&gt; Int</code></pre><p>Compute the bandwidth of <code>A</code> before any permutation of its rows and columns.</p><p>The <em>bandwidth</em> of an <span>$n×n$</span> matrix <span>$A$</span> is the minimum non-negative integer <span>$k ∈ [0, n - 1]$</span> such that <span>$A[i, j] = 0$</span> whenever <span>$|i - j| &gt; k$</span>. Equivalently, <span>$A$</span> has bandwidth <em>at most</em> <span>$k$</span> if all entries above the <span>$k$</span>ᵗʰ superdiagonal and below the <span>$k$</span>ᵗʰ subdiagonal are zero, and <span>$A$</span> has bandwidth <em>at least</em> <span>$k$</span> if there exists any nonzero entry in the <span>$k$</span>ᵗʰ superdiagonal or subdiagonal.</p><p>In contrast to <a href="#MatrixBandwidth.Minimization.minimize_bandwidth"><code>minimize_bandwidth</code></a>, this function does not attempt to minimize the bandwidth of <code>A</code> by permuting its rows and columns—it simply computes its bandwidth as is.</p><p><strong>Arguments</strong></p><ul><li><code>A::AbstractMatrix{&lt;:Number}</code>: the (square) matrix whose bandwidth is computed.</li></ul><p><strong>Returns</strong></p><ul><li><code>::Int</code>: the bandwidth of <code>A</code>.</li></ul><p><strong>Performance</strong></p><p>Given an <span>$n×n$</span> input matrix <span>$A$</span>, this relatively simple algorithm runs in <span>$O(n²)$</span> time.</p><p><strong>Examples</strong></p><p><code>bandwidth</code> correctly identifies the bandwidth of a pentadiagonal matrix as <span>$2$</span> and does not attempt to find a minimizing permutation upon shuffling of its rows and columns:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; Random.seed!(242622);

julia&gt; (n, k) = (8, 2);

julia&gt; perm = randperm(n);

julia&gt; A = (!iszero).(random_banded_matrix(8, 2))
8×8 BitMatrix:
 1  0  0  0  0  0  0  0
 0  1  0  1  0  0  0  0
 0  0  0  1  1  0  0  0
 0  1  1  1  0  1  0  0
 0  0  1  0  0  0  0  0
 0  0  0  1  0  0  0  0
 0  0  0  0  0  0  0  0
 0  0  0  0  0  0  0  0

julia&gt; bandwidth(A)
2

julia&gt; A_shuffled = A[perm, perm]
8×8 BitMatrix:
 0  0  0  0  0  0  1  0
 0  0  0  0  0  0  0  0
 0  0  0  1  0  0  0  0
 0  0  1  0  0  0  1  0
 0  0  0  0  1  0  0  0
 0  0  0  0  0  1  1  0
 1  0  0  1  0  1  1  0
 0  0  0  0  0  0  0  0

julia&gt; bandwidth(A_shuffled)
6</code></pre><p><strong>Notes</strong></p><p>Some texts define matrix bandwidth to be the minimum non-negative integer <span>$k$</span> such that <span>$A[i, j] = 0$</span> whenever <span>$|i - j| ≥ k$</span> instead, particularly in more mathematically-minded communities. Effectively, this definition treats diagonal matrices as bandwidth <span>$1$</span>, tridiagonal matrices as bandwidth <span>$2$</span>, and so on. Our definition, on the other hand, is more common in computer science contexts, treating diagonal matrices as bandwidth <span>$0$</span> and tridiagonal matrices as bandwidth <span>$1$</span>. (Both definitions, however, agree that the bandwidth of an empty matrix is simply <span>$0$</span>.)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/core.jl#L7-L79">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.bandwidth_lower_bound-Tuple{AbstractMatrix{&lt;:Number}}" href="#MatrixBandwidth.bandwidth_lower_bound-Tuple{AbstractMatrix{&lt;:Number}}"><code>MatrixBandwidth.bandwidth_lower_bound</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">bandwidth_lower_bound(A) -&gt; Int</code></pre><p>Compute a lower bound on the bandwidth of <code>A</code> using [<a href="#CSG05">CS05</a>, pp.359–60]&#39;s results.</p><p><code>A</code> is assumed to be structurally symmetric, since the bound from [<a href="#CSG05">CS05</a>, pp.359–60] was discovered in the context of undirected graphs (whose adjacency matrices are symmetric).</p><p>The <em>bandwidth</em> of an <span>$n×n$</span> matrix <span>$A$</span> is the minimum non-negative integer <span>$k ∈ [0, n - 1]$</span> such that <span>$A[i, j] = 0$</span> whenever <span>$|i - j| &gt; k$</span>. Equivalently, <span>$A$</span> has bandwidth <em>at most</em> <span>$k$</span> if all entries above the <span>$k$</span>ᵗʰ superdiagonal and below the <span>$k$</span>ᵗʰ subdiagonal are zero, and <span>$A$</span> has bandwidth <em>at least</em> <span>$k$</span> if there exists any nonzero entry in the <span>$k$</span>ᵗʰ superdiagonal or subdiagonal.</p><p>In contrast to <a href="#MatrixBandwidth.Minimization.minimize_bandwidth"><code>minimize_bandwidth</code></a>, this function does not attempt to truly minimize the bandwidth of <code>A</code>—it simply returns a lower bound on its bandwidth up to symmetric permutation of its rows and columns. This bound is not generally tight, but it indeed matches the true minimum in many non-trivial cases and is easily computable in <span>$O(n³)$</span> time (dominated by the Floyd–Warshall algorithm call; the core logic itself runs in <span>$O(n²)$</span> time).</p><p><strong>Arguments</strong></p><ul><li><code>A::AbstractMatrix{&lt;:Number}</code>: the (square) matrix on whose bandwidth a lower bound is to   be computed. <code>A</code> must be structurally symmetric (i.e., <code>A[i, j]</code> must be nonzero if   and only if <code>A[j, i]</code> is nonzero for <span>$1 ≤ i, j ≤ n$</span>).</li></ul><p><strong>Returns</strong></p><ul><li><code>::Int</code>: a lower bound on the bandwidth of <code>A</code>. (This bound is tight in many non-trivial   cases but not universally so.)</li></ul><p><strong>Examples</strong></p><p>The function correctly computes a bound less than (or equal to) the true minimum bandwidth of a matrix up to symmetric permutation:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random, SparseArrays, Combinatorics

julia&gt; Random.seed!(21);

julia&gt; (n, p) = (9, 0.4);

julia&gt; A = sprand(n, n, p);

julia&gt; A = A + A&#39; # Ensure structural symmetry;

julia&gt; minimize_bandwidth(A, Minimization.BruteForceSearch())
Results of Bandwidth Minimization Algorithm
 * Algorithm: Brute-force search
 * Approach: exact
 * Minimum Bandwidth: 5
 * Original Bandwidth: 8
 * Matrix Size: 9×9

julia&gt; bandwidth_lower_bound(A) # Always less than or equal to the true minimum bandwidth
4</code></pre><p><strong>Notes</strong></p><p>Some texts define matrix bandwidth to be the minimum non-negative integer <span>$k$</span> such that <span>$A[i, j] = 0$</span> whenever <span>$|i - j| ≥ k$</span> instead, particularly in more mathematically-minded communities. Effectively, this definition treats diagonal matrices as bandwidth <span>$1$</span>, tridiagonal matrices as bandwidth <span>$2$</span>, and so on. Our definition, on the other hand, is more common in computer science contexts, treating diagonal matrices as bandwidth <span>$0$</span> and tridiagonal matrices as bandwidth <span>$1$</span>. (Both definitions, however, agree that the bandwidth of an empty matrix is simply <span>$0$</span>.)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/core.jl#L96-L161">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.random_banded_matrix-Tuple{Int64, Int64}" href="#MatrixBandwidth.random_banded_matrix-Tuple{Int64, Int64}"><code>MatrixBandwidth.random_banded_matrix</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">random_banded_matrix(n, k; p=0.5, rng=default_rng()) -&gt; Matrix{Float64}</code></pre><p>Generate a random <code>n×n</code> structurally symmetric <code>k</code>-banded matrix with band density <code>≈ p</code>.</p><p>By definition of structural symmetry, the <span>$(i, j)$</span>-th entry of the matrix is nonzero if and only if the <span>$(j, i)$</span>-th entry is nonzero as well. All entries from this matrix are from the interval <code>[0, 1]</code>. Entries up to the <code>k</code>ᵗʰ superdiagonal and down to the <code>k</code>ᵗʰ subdiagonal are nonzero with probability <code>p</code>.</p><p>It is also guaranteed that each of these bands (besides the main diagonal) has at least one nonzero entry (even when <code>p</code> is very small), thus ensuring that the matrix has bandwidth precisely <code>k</code> before any reordering. (There may, however, still exist a symmetric permutation inducing a minimum bandwidth less than <code>k</code>, especially for small values of <code>p</code>.)</p><p><strong>Arguments</strong></p><ul><li><code>n::Int</code>: the order of the matrix to generate. Must be positive.</li><li><code>k::Int</code>: the desired matrix bandwidth. Must satisfy <code>0 ≤ k &lt; n</code>.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>p::Real=0.5</code>: the band density. Must satisfy <code>0 &lt; p ≤ 1</code>. Defaults to <code>0.5</code>.</li><li><code>rng::AbstractRNG=Random.default_rng()</code>: the random number generator to use. Defaults to   <code>Random.default_rng()</code>.</li></ul><p><strong>Returns</strong></p><ul><li><code>::Matrix{Float64}</code>: a random <code>n×n</code> matrix with bandwidth exactly <code>k</code> and sparse bands   with density <code>p</code>.</li></ul><p><strong>Examples</strong></p><p>Generate a <span>$6×6$</span> matrix with bandwidth <span>$1$</span> and the maximum number of nonzero entries:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; A = random_banded_matrix(6, 1; p=1, rng=MersenneTwister(1228))
6×6 Matrix{Float64}:
 0.310239  0.346413  0.0       0.0        0.0       0.0
 0.509981  0.917073  0.390771  0.0        0.0       0.0
 0.0       0.760045  0.808396  0.0195686  0.0       0.0
 0.0       0.0       0.222338  0.853164   0.806888  0.0
 0.0       0.0       0.0       0.421603   0.132165  0.805813
 0.0       0.0       0.0       0.0        0.305339  0.0799183

julia&gt; bandwidth(A)
1</code></pre><p>Generate a <span>$7×7$</span> matrix with bandwidth <span>$3$</span> and band density <span>$0.3$</span>:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; A = random_banded_matrix(7, 3; p=0.3, rng=MersenneTwister(0402))
7×7 Matrix{Float64}:
 0.0       0.132699  0.0       0.0       0.0  0.0       0.0
 0.869352  0.0       0.324319  0.926496  0.0  0.0       0.0
 0.0       0.891878  0.0       0.658102  0.0  0.0       0.0
 0.0       0.88859   0.399559  0.0       0.0  0.284285  0.703377
 0.0       0.0       0.0       0.0       0.0  0.0       0.0
 0.0       0.0       0.0       0.489594  0.0  0.0       0.393573
 0.0       0.0       0.0       0.412412  0.0  0.47063   0.0

julia&gt; bandwidth(A)
3</code></pre><p>Generate an <span>$8×8$</span> diagonal (bandwidth <span>$0$</span>) matrix with default band density (<span>$0.5$</span>):</p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; A = random_banded_matrix(8, 0; rng=MersenneTwister(0102))
8×8 Matrix{Float64}:
 0.0  0.0        0.0       0.0       0.0  0.0      0.0  0.0
 0.0  0.0762399  0.0       0.0       0.0  0.0      0.0  0.0
 0.0  0.0        0.373113  0.0       0.0  0.0      0.0  0.0
 0.0  0.0        0.0       0.726309  0.0  0.0      0.0  0.0
 0.0  0.0        0.0       0.0       0.0  0.0      0.0  0.0
 0.0  0.0        0.0       0.0       0.0  0.41974  0.0  0.0
 0.0  0.0        0.0       0.0       0.0  0.0      0.0  0.0
 0.0  0.0        0.0       0.0       0.0  0.0      0.0  0.293132

julia&gt; bandwidth(A)
0</code></pre><p><strong>Notes</strong></p><p>Users of the <a href="#MatrixBandwidth.MatrixBandwidth"><code>MatrixBandwidth</code></a> package may find this function useful when generating random test data for whatever frameworks, algorithms, etc. they are implementing.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/utils.jl#L7-L93">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization" href="#MatrixBandwidth.Minimization"><code>MatrixBandwidth.Minimization</code></a> — <span class="docstring-category">Module</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MatrixBandwidth.Minimization</code></pre><p>Exact, heuristic, and metaheuristic algorithms for matrix bandwidth minimization in Julia.</p><p>The <em>bandwidth</em> of an <span>$n×n$</span> matrix <span>$A$</span> is the minimum non-negative integer <span>$k ∈ [0, n - 1]$</span> such that <span>$A[i, j] = 0$</span> whenever <span>$|i - j| &gt; k$</span>. Equivalently, <span>$A$</span> has bandwidth <em>at most</em> <span>$k$</span> if all entries above the <span>$k$</span>ᵗʰ superdiagonal and below the <span>$k$</span>ᵗʰ subdiagonal are zero, and <span>$A$</span> has bandwidth <em>at least</em> <span>$k$</span> if there exists any nonzero entry in the <span>$k$</span>ᵗʰ superdiagonal or subdiagonal.</p><p>The <em>matrix bandwidth minimization problem</em> involves finding a permutation matrix <span>$P$</span> such that the bandwidth of <span>$PAPᵀ$</span> is minimized; this is known to be NP-complete. Several heuristic algorithms (such as Gibbs–Poole–Stockmeyer) run in polynomial time while still producing near-optimal orderings in practice, but exact methods (like Caprara–Salazar-González) are at least exponential in time complexity and thus are only feasible for relatively small matrices.</p><p>The following algorithms are currently supported:</p><ul><li><em>Exact</em><ul><li>Caprara–Salazar-González algorithm (<a href="#MatrixBandwidth.Minimization.Exact.CapraraSalazarGonzalez"><code>CapraraSalazarGonzalez</code></a>)</li><li>Del Corso–Manzini algorithm (<a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManzini"><code>DelCorsoManzini</code></a>)</li><li>Del Corso–Manzini algorithm with perimeter search (<a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS"><code>DelCorsoManziniWithPS</code></a>)</li><li>Saxe–Gurari–Sudborough algorithm (<a href="#MatrixBandwidth.Minimization.Exact.SaxeGurariSudborough"><code>SaxeGurariSudborough</code></a>)</li><li>Brute-force search (<a href="#MatrixBandwidth.Minimization.Exact.BruteForceSearch"><code>BruteForceSearch</code></a>)</li></ul></li><li><em>Heuristic</em><ul><li>Gibbs–Poole–Stockmeyer algorithm (<a href="#MatrixBandwidth.Minimization.Heuristic.GibbsPooleStockmeyer"><code>GibbsPooleStockmeyer</code></a>)</li><li>Cuthill–McKee algorithm (<a href="#MatrixBandwidth.Minimization.Heuristic.CuthillMcKee"><code>CuthillMcKee</code></a>)</li><li>Reverse Cuthill–McKee algorithm (<a href="#MatrixBandwidth.Minimization.Heuristic.ReverseCuthillMcKee"><code>ReverseCuthillMcKee</code></a>)</li></ul></li><li><em>Metaheuristic</em><ul><li>Greedy randomized adaptive search procedure (GRASP) (<a href="#MatrixBandwidth.Minimization.Metaheuristic.GRASP"><code>GRASP</code></a>)</li><li>Simulated annealing (<a href="#MatrixBandwidth.Minimization.Metaheuristic.SimulatedAnnealing"><code>SimulatedAnnealing</code></a>)</li><li>Genetic algorithm (<a href="#MatrixBandwidth.Minimization.Metaheuristic.GeneticAlgorithm"><code>GeneticAlgorithm</code></a>)</li></ul></li></ul><p>This submodule is part of the <a href="https://github.com/Luis-Varona/MatrixBandwidth.jl">MatrixBandwidth.jl</a> package.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Minimization.jl#L7-L43">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.MinimizationResult" href="#MatrixBandwidth.Minimization.MinimizationResult"><code>MatrixBandwidth.Minimization.MinimizationResult</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MinimizationResult{A,M,O} &lt;: AbstractResult</code></pre><p>Output struct for matrix bandwidth minimization results.</p><p><strong>Fields</strong></p><ul><li><code>algorithm::A&lt;:AbstractSolver</code>: the solver used to minimize the bandwidth.</li><li><code>matrix::M&lt;:AbstractMatrix{&lt;:Number}</code>: the original matrix whose bandwidth is minimized.</li><li><code>ordering::O&lt;:Vector{Int}</code>: the (near-)optimal ordering of the rows and columns.</li><li><code>bandwidth::Int</code>: the minimized bandwidth of the matrix.</li><li><code>approach::Symbol</code>: the approach used by the solver. (Should be one of <code>:exact</code>,   <code>:heuristic</code>, and <code>:metaheuristic</code>.)</li></ul><p><strong>Constructors</strong></p><ul><li><code>MinimizationResult(algorithm, matrix, ordering, bandwidth)</code>: constructs a new   <code>MinimizationResult</code> instance with the given fields. The <code>approach</code> field is   automatically determined based on the algorithm type.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/types.jl#L35-L52">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.minimize_bandwidth" href="#MatrixBandwidth.Minimization.minimize_bandwidth"><code>MatrixBandwidth.Minimization.minimize_bandwidth</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">minimize_bandwidth(A, solver=GibbsPooleStockmeyer()) -&gt; MinimizationResult</code></pre><p>Minimize the bandwidth of <code>A</code> using the algorithm defined by <code>solver</code>.</p><p>The <em>bandwidth</em> of an <span>$n×n$</span> matrix <span>$A$</span> is the minimum non-negative integer <span>$k ∈ [0, n - 1]$</span> such that <span>$A[i, j] = 0$</span> whenever <span>$|i - j| &gt; k$</span>. Equivalently, <span>$A$</span> has bandwidth <em>at most</em> <span>$k$</span> if all entries above the <span>$k$</span>ᵗʰ superdiagonal and below the <span>$k$</span>ᵗʰ subdiagonal are zero, and <span>$A$</span> has bandwidth <em>at least</em> <span>$k$</span> if there exists any nonzero entry in the <span>$k$</span>ᵗʰ superdiagonal or subdiagonal.</p><p>This function computes a (near-)optimal ordering <span>$π$</span> of the rows and columns of <span>$A$</span> so that the bandwidth of <span>$PAPᵀ$</span> is minimized, where <span>$P$</span> is the permutation matrix corresponding to <span>$π$</span>. This is known to be an NP-complete problem; however, several heuristic algorithms such as Gibbs–Poole–Stockmeyer run in polynomial time while still still producing near-optimal orderings in practice. Exact methods like Caprara–Salazar-González are also available, but they are at least exponential in time complexity and thus only feasible for relatively small matrices.</p><p><strong>Arguments</strong></p><ul><li><code>A::AbstractMatrix{&lt;:Number}</code>: the (square) matrix whose bandwidth is minimized.</li><li><code>solver::AbstractSolver</code>: the matrix bandwidth minimization algorithm to use; defaults to   <a href="#MatrixBandwidth.Minimization.Heuristic.GibbsPooleStockmeyer"><code>GibbsPooleStockmeyer</code></a>. (See the <a href="#MatrixBandwidth.Minimization"><code>Minimization</code></a> module documentation for   a full list of supported solvers.)</li></ul><p><strong>Returns</strong></p><ul><li><code>::MinimizationResult</code>: a struct containing the algorithm used, the original matrix <code>A</code>,   the (near-)optimal ordering of the rows and columns, and the minimized bandwidth.</li></ul><p><strong>Examples</strong></p><p>[TODO: Add here once more solvers are implemented. For now, refer to the <strong>Examples</strong> sections of the <a href="#MatrixBandwidth.Minimization.Heuristic.GibbsPooleStockmeyer"><code>GibbsPooleStockmeyer</code></a>, <a href="#MatrixBandwidth.Minimization.Heuristic.CuthillMcKee"><code>CuthillMcKee</code></a>, and <a href="#MatrixBandwidth.Minimization.Heuristic.ReverseCuthillMcKee"><code>ReverseCuthillMcKee</code></a> docstrings.]</p><p><strong>Notes</strong></p><p>Some texts define matrix bandwidth to be the minimum non-negative integer <span>$k$</span> such that <span>$A[i, j] = 0$</span> whenever <span>$|i - j| ≥ k$</span> instead, particularly in more mathematically-minded communities. Effectively, this definition treats diagonal matrices as bandwidth <span>$1$</span>, tridiagonal matrices as bandwidth <span>$2$</span>, and so on. Our definition, on the other hand, is more common in computer science contexts, treating diagonal matrices as bandwidth <span>$0$</span> and tridiagonal matrices as bandwidth <span>$1$</span>. (Both definitions, however, agree that the bandwidth of an empty matrix is simply <span>$0$</span>.)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/core.jl#L7-L49">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Exact" href="#MatrixBandwidth.Minimization.Exact"><code>MatrixBandwidth.Minimization.Exact</code></a> — <span class="docstring-category">Module</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MatrixBandwidth.Minimization.Exact</code></pre><p>Exact solvers for matrix bandwidth minimization.</p><p>Exact methods are those which guarantee an optimal ordering producing the true minimum bandwidth of a matrix. Since bandwidth minimization is an NP-complete problem, existing exact algorithms are, at best, exponential in time complexity—much worse than many polynomial-time heuristic approaches (e.g., Gibbs–Poole–Stockmeyer). Such methods, therefore, are not feasible for large matrices, but they remain useful when precise solutions are required for small-to-medium-sized inputs (say, up to <span>$100×100$</span>).</p><p>The following exact algorithms are currently supported:</p><ul><li>Caprara–Salazar-González algorithm (<a href="#MatrixBandwidth.Minimization.Exact.CapraraSalazarGonzalez"><code>CapraraSalazarGonzalez</code></a>)</li><li>Del Corso–Manzini algorithm (<a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManzini"><code>DelCorsoManzini</code></a>)</li><li>Del Corso–Manzini algorithm with perimeter search (<a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS"><code>DelCorsoManziniWithPS</code></a>)</li><li>Saxe–Gurari–Sudborough algorithm (<a href="#MatrixBandwidth.Minimization.Exact.SaxeGurariSudborough"><code>SaxeGurariSudborough</code></a>)</li><li>Brute-force search (<a href="#MatrixBandwidth.Minimization.Exact.BruteForceSearch"><code>BruteForceSearch</code></a>)</li></ul><p>This submodule is part of the <code>MatrixBandwidth.Minimization</code> submodule of the <a href="https://github.com/Luis-Varona/MatrixBandwidth.jl">MatrixBandwidth.jl</a> package.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Exact/Exact.jl#L7-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Exact.BruteForceSearch" href="#MatrixBandwidth.Minimization.Exact.BruteForceSearch"><code>MatrixBandwidth.Minimization.Exact.BruteForceSearch</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BruteForceSearch &lt;: ExactSolver &lt;: AbstractSolver &lt;: AbstractAlgorithm</code></pre><p>The simplest exact method for minimizing the bandwidth of a matrix is to iterate over all possible symmetric permutations and compare the bandwidths they induce.</p><p>Since <span>$i₁, i₂, … iₙ$</span> induces the same bandwidth as <span>$iₙ, iₙ₋₁, … i₁$</span>, we restrict our search to orderings such that <span>$i₁ ≤ iₙ$</span> (with equality checked just in case <span>$n = 1$</span>).</p><p><strong>Performance</strong></p><p>Given an <span>$n×n$</span> input matrix <span>$A$</span>, this brute-force algorithm runs in <span>$O(n! ⋅ n²)$</span> time:</p><ul><li>Precisely <span>$n!/2$</span> permutations are checked (except when <span>$n = 1$</span>, in which case   <span>$1! = 1$</span> permutation is checked). This is, clearly, <span>$O(n!)$</span>.</li><li>For each permutation, the <a href="#MatrixBandwidth.bandwidth-Tuple{AbstractMatrix{&lt;:Number}}"><code>bandwidth</code></a> function is called on   <span>$view(A, perm, perm)$</span>, which takes <span>$O(n²)$</span> time.</li><li>Therefore, the overall time complexity is <span>$O(n! ⋅ n²)$</span>.</li></ul><p>Indeed, due to the need to exhaustively check all permutations, this is close to a lower bound as well on the the algorithm&#39;s time complexity. (The only reason we cannot claim to have a precise value for the big-<span>$Θ$</span> complexity is that the <a href="#MatrixBandwidth.bandwidth-Tuple{AbstractMatrix{&lt;:Number}}"><code>bandwidth</code></a> function is not <em>exactly</em> <span>$Θ(n²)$</span>, although it is close.)</p><p><strong>Examples</strong></p><p>The algorithm always iterates over all possible permutations, so it is infeasible to go above <span>$9×9$</span> or <span>$10×10$</span> without incurring multiple-hour runtimes. Nevertheless, we see that it is quite effective for, say, <span>$8×8$</span>:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random, SparseArrays

julia&gt; Random.seed!(628318);

julia&gt; (n, p) = (8, 0.2);

julia&gt; A = sprand(Bool, n, n, p);

julia&gt; A = A + A&#39; # Ensure structural symmetry;

julia&gt; minimize_bandwidth(A, Minimization.BruteForceSearch())
Results of Bandwidth Minimization Algorithm
 * Algorithm: Brute-force search
 * Approach: exact
 * Minimum Bandwidth: 3
 * Original Bandwidth: 6
 * Matrix Size: 8×8</code></pre><p><strong>Notes</strong></p><p>Brute force is by far the slowest approach to matrix bandwidth minimization and should only be used in very niche cases (like verifying the correctness of other algorithms in unit tests). For <span>$10×10$</span> matrices, the algorithm already takes several minutes to run (between <span>$2$</span> to <span>$5$</span> on most commercial machines) and allocates over <span>$4$</span> gigabytes of memory. Given the <span>$O(n! ⋅ n²)$</span> time complexity, minimizing the bandwidth of any <span>$11×11$</span> matrix would take over an hour.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Exact/solvers/brute_force_search.jl#L7-L61">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Exact.CapraraSalazarGonzalez" href="#MatrixBandwidth.Minimization.Exact.CapraraSalazarGonzalez"><code>MatrixBandwidth.Minimization.Exact.CapraraSalazarGonzalez</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CapraraSalazarGonzalez &lt;: ExactSolver &lt;: AbstractSolver &lt;: AbstractAlgorithm</code></pre><p>The <em>Caprara–Salazar-González minimization algorithm</em> is an exact method for minimizing the bandwidth of a structurally symmetric matrix <span>$A$</span>. For a fixed <span>$k ∈ ℕ$</span>, the algorithm performs a bidirectional depth-first search of all partial orderings of the rows and columns of <span>$A$</span>, adding indices one at a time to both the left and right ends. Partial orderings are pruned not only by ensuring that adjacent pairs of currently placed indices are within <span>$k$</span> of each other but also by employing a branch-and-bound framework with lower bounds on bandwidtth compatibility computed via integer linear programming relaxations. This search is repeated with incrementing values of <span>$k$</span> until a bandwidth-<span>$k$</span> ordering is found [<a href="#CSG05">CS05</a>], with <span>$k$</span> initialized to some lower bound on the minimum bandwidth of <span>$A$</span> up to symmetric permutation.</p><p>Specifically, this implementation of the Caprara–Salazar-González algorithm uses the <span>$min(α(A), γ(A))$</span> lower bound from the original paper [<a href="#CSG05">CS05</a>, pp. 359–60] as the initial value of <span>$k$</span>. (Further implementation details can be found in the source code for <a href="#MatrixBandwidth.bandwidth_lower_bound-Tuple{AbstractMatrix{&lt;:Number}}"><code>bandwidth_lower_bound</code></a>.)</p><p>As noted above, the Caprara–Salazar-González algorithm requires structurally symmetric input (that is, <span>$A[i, j]$</span> must be nonzero if and only if <span>$A[j, i]$</span> is nonzero for <span>$1 ≤ i, j ≤ n$</span>).</p><p><strong>Performance</strong></p><p>[TODO: Write here]</p><p><strong>Examples</strong></p><p>[TODO: Write here]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Exact/solvers/caprara_salazar_gonzalez.jl#L7-L35">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Exact.DelCorsoManzini" href="#MatrixBandwidth.Minimization.Exact.DelCorsoManzini"><code>MatrixBandwidth.Minimization.Exact.DelCorsoManzini</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DelCorsoManzini &lt;: ExactSolver &lt;: AbstractSolver &lt;: AbstractAlgorithm</code></pre><p>The <em>Del Corso–Manzini minimization algorithm</em> is an exact method for minimizing the bandwidth of a structurally symmetric matrix <span>$A$</span>. For a fixed <span>$k ∈ ℕ$</span>, the algorithm performs a depth-first search of all partial orderings of the rows and columns of <span>$A$</span>, adding indices one at a time. Partial orderings are pruned not only by ensuring that adjacent pairs of currently placed indices are within <span>$k$</span> of each other but also by tracking the latest positions at which the remaining indices can be placed. This search is repeated with incrementing values of <span>$k$</span> until a bandwidth-<span>$k$</span> ordering is found [<a href="#DCM99">DM99</a>], with <span>$k$</span> initialized to some lower bound on the minimum bandwidth of <span>$A$</span> up to symmetric permutation.</p><p>Specifically, this implementation of the Del Corso–Manzini algorithm uses the <span>$min(α(A), γ(A))$</span> lower bound from [<a href="#CSG05">CS05</a>, pp. 359–60] as the initial value of <span>$k$</span>. (Further implementation details can be found in the source code for <a href="#MatrixBandwidth.bandwidth_lower_bound-Tuple{AbstractMatrix{&lt;:Number}}"><code>bandwidth_lower_bound</code></a>.) This improves upon the original algorithm, which used the maximum number of nonzero off-diagonal entries in a single row as a lower bound on the minimum bandwidth of <span>$A$</span> up to symmetric permutation [<a href="#DCM99">DM99</a>, p. 192–93].</p><p>As noted above, the Del Corso–Manzini algorithm requires structurally symmetric input (that is, <span>$A[i, j]$</span> must be nonzero if and only if <span>$A[j, i]$</span> is nonzero for <span>$1 ≤ i, j ≤ n$</span>).</p><p><strong>Performance</strong></p><p>Given an <span>$n×n$</span> input matrix <span>$A$</span>, the Del Corso–Manzini algorithm runs in <span>$O(n! ⋅ n³)$</span> time:</p><ul><li>For each underlying &quot;bandwidth ≤ <span>$k$</span>&quot; check, we perform a depth-first search of   <span>$O(n!)$</span> partial orderings.</li><li>Checking plausibility of each partial ordering takes <span>$O(nk)$</span> time, resulting in   <span>$O(n! ⋅ nk)$</span> steps for each value of <span>$k$</span>.</li><li>The difference between the maximum possible bandwidth (<span>$n - 1$</span>) and our initial lower   bound grows linearly in <span>$n$</span>, so we run the underlying <span>$O(n! ⋅ nk)$</span> recognition   algorithm <span>$O(n)$</span> times.</li><li>Finally, <span>$∑ₖ₌₀ⁿ⁻¹ nk = O(n³)$</span>, so the overall time complexity is <span>$O(n! ⋅ n³)$</span>.</li></ul><p>Of course, this is but an upper bound on the time complexity of Del Corso–Manzini, achieved only in the most pathological of cases. In practice, efficient pruning techniques and compatibility checks—along with [<a href="#CSG05">CS05</a>, pp. 359–60]&#39;s relatively tight initial lower bound on the minimum bandwidth—result in approximately exponential growth in time complexity with respect to <span>$n$</span>.</p><p>Based on experimental results, the algorithm is feasible for <span>$n×n$</span> matrices up to <span>$n ≈ 100$</span> or so.</p><p><strong>Examples</strong></p><p>We verify the optimality of the ordering found by Del Corso–Manzini for a random <span>$9×9$</span> matrix via a brute-force search over all possible permutations up to reversal:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random, SparseArrays

julia&gt; Random.seed!(0117);

julia&gt; (n, p) = (9, 0.5);

julia&gt; A = sprand(n, n, p);

julia&gt; A = A + A&#39; # Ensure structural symmetry;

julia&gt; res_bf = minimize_bandwidth(A, Minimization.BruteForceSearch())
Results of Bandwidth Minimization Algorithm
 * Algorithm: Brute-force search
 * Approach: exact
 * Minimum Bandwidth: 5
 * Original Bandwidth: 8
 * Matrix Size: 9×9

julia&gt; res_dcm = minimize_bandwidth(A, Minimization.DelCorsoManzini())
Results of Bandwidth Minimization Algorithm
 * Algorithm: Del Corso–Manzini
 * Approach: exact
 * Minimum Bandwidth: 5
 * Original Bandwidth: 8
 * Matrix Size: 9×9</code></pre><p>We now generate (and shuffle) a random <span>$40×40$</span> matrix with minimum bandwidth <span>$10$</span> using <a href="#MatrixBandwidth.random_banded_matrix-Tuple{Int64, Int64}"><code>MatrixBandwidth.random_banded_matrix</code></a>. Del Corso–Manzini then finds a bandwidth-<span>$10$</span> ordering, which is (we claim) optimal up to symmetric permutation. (In some cases, <code>random_banded_matrix(n, k)</code> <em>does</em> generate matrices with minimum bandwidth <code>&lt; k</code>. Nevertheless, this example demonstrates that Del Corso–Manzini at the very least finds a good ordering, even though exact optimality—which <em>is</em> guaranteed by the original paper [<a href="#DCM99">DM99</a>]—is not explicitly verified.)</p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; Random.seed!(0201);

julia&gt; (n, k) = (40, 10);

julia&gt; A = random_banded_matrix(n, k);

julia&gt; perm = randperm(n);

julia&gt; A_shuffled = A[perm, perm];

julia&gt; bandwidth(A)
10

julia&gt; bandwidth(A_shuffled) # Much larger after shuffling
36

julia&gt; minimize_bandwidth(A_shuffled, Minimization.DelCorsoManzini())
Results of Bandwidth Minimization Algorithm
 * Algorithm: Del Corso–Manzini
 * Approach: exact
 * Minimum Bandwidth: 10
 * Original Bandwidth: 36
 * Matrix Size: 40×40</code></pre><p><strong>Notes</strong></p><p>For readers of the original paper, what we call the Del Corso–Manzini minimization algorithm here is designated the &quot;MB-ID algorithm&quot; in [<a href="#DCM99">DM99</a>, p. 191]. The so-called &quot;MB-PS algorithm,&quot; on the other hand, we implement in <a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS"><code>DelCorsoManziniWithPS</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Exact/solvers/del_corso_manzini.jl#L7-L121">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS" href="#MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS"><code>MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DelCorsoManziniWithPS{D} &lt;: ExactSolver &lt;: AbstractSolver &lt;: AbstractAlgorithm</code></pre><p>The <em>Del Corso–Manzini minimization algorithm with perimeter search</em> is an exact method for minimizing the bandwidth of a structurally symmetric matrix <span>$A$</span>. The base Del Corso–Manzini algorithm performs a depth-first search of all partial orderings of the rows and columns of <span>$A$</span> for some fixed <span>$k ∈ ℕ$</span>, adding indices one at a time. Partial orderings are pruned not only by ensuring that adjacent pairs of currently placed indices are within <span>$k$</span> of each other but also by tracking the latest positions at which the remaining indices can be placed. This search is repeated with incrementing values of <span>$k$</span> until a bandwidth-<span>$k$</span> ordering is found [<a href="#DCM99">DM99</a>], with <span>$k$</span> initialized to some lower bound on the minimum bandwidth of <span>$A$</span> up to symmetric permutation.</p><p>The incorporation of perimeter search to this approach entails precomputing a &quot;perimeter&quot; of <span>$d$</span>-permutations of row indices of <span>$A$</span>, where <span>$d$</span> is a positive integer passed as a parameter to the solver. Each permutation represents a way to select the last <span>$d$</span> entries of the ordering, and as the construction of the partial ordering progresses, potential endings are pruned to exclude those incompatible with already placed indices. In addition to pruning a potential ending if it contains indices already placed, compatibility is also checked via precomputed time stamps indicating, for each potential ending, a loose lower bound on the earliest position at which any given index can be placed should said ending be selected.</p><p>Like our implementation of the base Del Corso–Manzini algorithm (see <a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManzini"><code>DelCorsoManzini</code></a>), this implementation uses the <span>$min(α(A), γ(A))$</span> lower bound from [<a href="#CSG05">CS05</a>, pp. 359–60] as the initial value of <span>$k$</span>. (Further implementation details can be found in the source code for <a href="#MatrixBandwidth.bandwidth_lower_bound-Tuple{AbstractMatrix{&lt;:Number}}"><code>bandwidth_lower_bound</code></a>.) This improves upon the original algorithm, which used the maximum number of nonzero off-diagonal entries in a single row as a lower bound on the minimum bandwidth of <span>$A$</span> up to symmetric permutation [<a href="#DCM99">DM99</a>, p. 194].</p><p>As noted above, the Del Corso–Manzini algorithm with perimeter search requires structurally symmetric input (that is, <span>$A[i, j]$</span> must be nonzero if and only if <span>$A[j, i]$</span> is nonzero for <span>$1 ≤ i, j ≤ n$</span>).</p><p><strong>Fields</strong></p><ul><li><code>depth::D&lt;:Union{Nothing,Int}</code>: the perimeter search depth. If this field is not set (and)   thus automatically initialized to <code>nothing</code>), a default depth is automatically computed   by <a href="../private_api/#MatrixBandwidth.Recognition.dcm_ps_optimal_depth-Tuple{AbstractMatrix{Bool}}"><code>Recognition.dcm_ps_optimal_depth</code></a> as a function of the input matrix every   time the solver is passed to <a href="#MatrixBandwidth.Minimization.minimize_bandwidth"><code>MatrixBandwidth.Minimization.minimize_bandwidth</code></a>.   Otherwise, it must be manually set to a positive integer.</li></ul><p><strong>Constructors</strong></p><ul><li><code>DelCorsoManziniWithPS()</code>: constructs a new <code>DelCorsoManziniWithPS</code> instance with the   default perimeter search depth initialized to <code>nothing</code>.</li><li><code>DelCorsoManziniWithPS(depth::Int)</code>: constructs a new <code>DelCorsoManziniWithPS</code> instance   with the specified perimeter search depth. <code>depth</code> must be a positive integer.</li></ul><p><strong>Performance</strong></p><p>Given an <span>$n×n$</span> input matrix <span>$A$</span> and perimeter search depth <span>$d$</span>, the Del Corso–Manzini algorithm with perimeter search runs in <span>$O(n! ⋅ nᴰ⁺¹)$</span> time, where <span>$Dᴰ = max(d, 2)$</span>:</p><ul><li>For each underlying &quot;bandwidth ≤ <span>$k$</span>&quot; check, we perform a depth-first search of   <span>$O(n!)$</span> partial orderings.</li><li>Checking plausibility of each partial ordering takes <span>$O(nk)$</span> time, and checking   compatibility with all size-<span>$d$</span> LPOs takes <span>$O(nᵈ)$</span> time. Thus, the overall time   complexity for each value of <span>$k$</span> is <span>$O(n! ⋅ (nᵈ + nk))$</span>.</li><li>The difference between the maximum possible bandwidth (<span>$n - 1$</span>) and our initial lower   bound grows linearly in <span>$n$</span>, so we run the underlying <span>$O(n! ⋅ (nᵈ + nk))$</span>   recognition algorithm <span>$O(n)$</span> times.</li><li>Finally, <span>$∑ₖ₌₀ⁿ⁻¹ (nᵈ + nk) = O(nᵈ⁺¹ + n³)$</span>, so the overall time complexity   is <span>$O(n! ⋅ nᴰ⁺¹)$</span>, where <span>$D = max(d, 2)$</span>.</li></ul><p>Of course, this is but an upper bound on the time complexity of Del Corso–Manzini with perimeter search, achieved only in the most pathological of cases. In practice, efficient pruning techniques and compatibility checks—along with [<a href="#CSG05">CS05</a>, pp. 359–60]&#39;s relatively tight initial lower bound on the minimum bandwidth—result in approximately exponential growth in time complexity with respect to <span>$n$</span>.</p><p>Based on experimental results, the algorithm is feasible for <span>$n×n$</span> matrices up to <span>$n ≈ 100$</span> or so.</p><p><strong>Examples</strong></p><p>We verify the optimality of the ordering found by Del Corso–Manzini with perimeter search for a random <span>$9×9$</span> matrix via a brute-force search over all possible permutations up to reversal. The depth parameter is not explicitly set; instead, some near-optimal value is automatically computed upon the first <a href="#MatrixBandwidth.Minimization.minimize_bandwidth"><code>MatrixBandwidth.Minimization.minimize_bandwidth</code></a> function call.</p><pre><code class="language-julia-repl hljs">julia&gt; using Random, SparseArrays

julia&gt; Random.seed!(548836);

julia&gt; (n, p) = (9, 0.2);

julia&gt; A = sprand(n, n, p);

julia&gt; A = A + A&#39; # Ensure structural symmetry;

julia&gt; res_bf = minimize_bandwidth(A, Minimization.BruteForceSearch())
Results of Bandwidth Minimization Algorithm
 * Algorithm: Brute-force search
 * Approach: exact
 * Minimum Bandwidth: 3
 * Original Bandwidth: 8
 * Matrix Size: 9×9

julia&gt; res_dcm = minimize_bandwidth(A, Minimization.DelCorsoManziniWithPS())
Results of Bandwidth Minimization Algorithm
 * Algorithm: Del Corso–Manzini with perimeter search
 * Approach: exact
 * Minimum Bandwidth: 3
 * Original Bandwidth: 8
 * Matrix Size: 9×9</code></pre><p>We now generate (and shuffle) a random <span>$30×30$</span> matrix with minimum bandwidth <span>$8$</span> using <a href="#MatrixBandwidth.random_banded_matrix-Tuple{Int64, Int64}"><code>MatrixBandwidth.random_banded_matrix</code></a>. Del Corso–Manzini with perimeter search then finds a bandwidth-<span>$8$</span> ordering, which is (we claim) optimal up to symmetric permutation. (In some cases, <code>random_banded_matrix(n, k)</code> <em>does</em> generate matrices with minimum bandwidth <code>&lt; k</code>. Nevertheless, this example demonstrates that Del Corso–Manzini at the very least finds a good ordering, even though exact optimality—which <em>is</em> guaranteed by the original paper [<a href="#DCM99">DM99</a>]—is not explicitly verified.) In this case, we set the depth parameter to <span>$4$</span> beforehand instead of relying on <a href="../private_api/#MatrixBandwidth.Recognition.dcm_ps_optimal_depth-Tuple{AbstractMatrix{Bool}}"><code>Recognition.dcm_ps_optimal_depth</code></a>.</p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; Random.seed!(78779);

julia&gt; (n, k, depth) = (30, 8, 4);

julia&gt; A = random_banded_matrix(n, k);

julia&gt; perm = randperm(n);

julia&gt; A_shuffled = A[perm, perm];

julia&gt; bandwidth(A)
8

julia&gt; bandwidth(A_shuffled) # Much larger after shuffling
25

julia&gt; minimize_bandwidth(A_shuffled, Minimization.DelCorsoManziniWithPS(depth))
Results of Bandwidth Minimization Algorithm
 * Algorithm: Del Corso–Manzini with perimeter search
 * Approach: exact
 * Minimum Bandwidth: 8
 * Original Bandwidth: 25
 * Matrix Size: 30×30</code></pre><p><strong>Notes</strong></p><p>For readers of the original paper, what we call the Del Corso–Manzini minimization algorithm with perimeter search here is designated the &quot;MB-PS algorithm&quot; in [<a href="#DCM99">DM99</a>, p. 193]. The so-called &quot;MB-ID algorithm,&quot; on the other hand, we implement in <a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManzini"><code>DelCorsoManzini</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Exact/solvers/del_corso_manzini.jl#L128-L275">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Exact.SaxeGurariSudborough" href="#MatrixBandwidth.Minimization.Exact.SaxeGurariSudborough"><code>MatrixBandwidth.Minimization.Exact.SaxeGurariSudborough</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SaxeGurariSudborough &lt;: ExactSolver &lt;: AbstractSolver &lt;: AbstractAlgorithm</code></pre><p>TODO: Write here</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Exact/solvers/saxe_gurari_sudborough.jl#L7-L11">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Heuristic" href="#MatrixBandwidth.Minimization.Heuristic"><code>MatrixBandwidth.Minimization.Heuristic</code></a> — <span class="docstring-category">Module</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MatrixBandwidth.Minimization.Heuristic</code></pre><p>Heuristic solvers for matrix bandwidth minimization.</p><p>Heuristic methods are those which aim to produce near-optimal solutions in a more performant manner than exact methods. While precise bandwidth minimization is NP-complete, many heuristic algorithms (such as Gibbs–Poole–Stockmeyer) run in polynomial time.</p><p>Heuristic algorithms differ from metaheuristic ones in that they do not employ higher-level iterative search frameworks (e.g., stochastic techniques) to survey the global search space and escape local minima; instead, they rely on straightforward deterministic procedures to find good solutions in a single pass.</p><p>The following heuristic algorithms are currently supported:</p><ul><li>Gibbs–Poole–Stockmeyer algorithm (<a href="#MatrixBandwidth.Minimization.Heuristic.GibbsPooleStockmeyer"><code>GibbsPooleStockmeyer</code></a>)</li><li>Cuthill–McKee algorithm (<a href="#MatrixBandwidth.Minimization.Heuristic.CuthillMcKee"><code>CuthillMcKee</code></a>)</li><li>Reverse Cuthill–McKee algorithm (<a href="#MatrixBandwidth.Minimization.Heuristic.ReverseCuthillMcKee"><code>ReverseCuthillMcKee</code></a>)</li></ul><p>This submodule is part of the <code>MatrixBandwidth.Minimization</code> submodule of the <a href="https://github.com/Luis-Varona/MatrixBandwidth.jl">MatrixBandwidth.jl</a> package.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Heuristic/Heuristic.jl#L7-L28">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Heuristic.CuthillMcKee" href="#MatrixBandwidth.Minimization.Heuristic.CuthillMcKee"><code>MatrixBandwidth.Minimization.Heuristic.CuthillMcKee</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CuthillMcKee &lt;: HeuristicSolver &lt;: AbstractSolver &lt;: AbstractAlgorithm</code></pre><p>The <em>Cuthill–McKee algorithm</em> is a heuristic method for minimizing the bandwidth of a structurally symmetric matrix <span>$A$</span>. It considers the graph <span>$G(A)$</span> whose adjacency matrix is <span>$A$</span> (ignoring weights and self-loops) and performs a breadth-first search of each connected component of <span>$G(A)$</span>, starting from a low-degree node then visiting its neighbors in order of increasing degree. Particularly effective when <span>$A$</span> is sparse, this heuristic typically produces an ordering which induces a matrix bandwidth either equal to or very close to the true minimum [<a href="#CM69">CM69</a>, pp. 157–58].</p><p>As noted above, the Cuthill–McKee algorithm requires structurally symmetric input (that is, <span>$A[i, j]$</span> must be nonzero if and only if <span>$A[j, i]$</span> is nonzero for <span>$1 ≤ i, j ≤ n$</span>).</p><p><strong>Fields</strong></p><ul><li><code>node_selector::Function</code>: a function that selects a node from some connected component of   the input matrix from which to start the breadth-first search. If no custom heuristic is   specified, this field defaults to <a href="../private_api/#MatrixBandwidth.Minimization.Heuristic.pseudo_peripheral_node-Tuple{AbstractMatrix{Bool}}"><code>pseudo_peripheral_node</code></a>, which picks a node   &quot;farthest&quot; from the others in the component (not necessarily the lowest-degree node).</li></ul><p><strong>Performance</strong></p><p>Given an <span>$n×n$</span> input matrix <span>$A$</span>, the Cuthill–McKee algorithm runs in <span>$O(n²)$</span> time.</p><p>[<a href="#CG80">CG80</a>] provide a linear-time implementation in the number of nonzero entries of <span>$A$</span>, which is still quadratic when <span>$A$</span> is dense but often much faster when dealing with sparse matrices. However, this would require that <span>$A$</span> be stored as a graph or a sparse matrix, which runs counter to our desire to provide a bandwidth minimization API for all <code>AbstractMatrix{&lt;:Number}</code> types, including dense matrices. (In the future, however, we may indeed consider supporting this more performant implementation for sparse matrices.)</p><p>It was found in [<a href="#Geo71">Geo71</a>, pp. 114–15] that reversing the ordering produced by Cuthill–McKee tends to induce a more optimal <em>matrix profile</em> (a measure of how far, on average, nonzero entries are from the diagonal). This so-called <em>reverse Cuthill–McKee</em> variant is preferred in almost all cases—see <a href="#MatrixBandwidth.Minimization.Heuristic.ReverseCuthillMcKee"><code>ReverseCuthillMcKee</code></a> and the associated method of <code>_bool_minimal_band_ordering</code> for our implementation.</p><p><strong>Examples</strong></p><p>In the following examples, <a href="#MatrixBandwidth.random_banded_matrix-Tuple{Int64, Int64}"><code>MatrixBandwidth.random_banded_matrix</code></a> is used to generate random matrices with minimum bandwidth <em>close to</em> <span>$k$</span>. In some cases, however, the true minimum bandwidth up to symmetric permutation may be even less than <span>$k$</span>, making it hard to verify whether Cuthill–McKee finds a truly optimal ordering or simply a near-optimal one. Nevertheless, the results are still very good in practice.</p><p>Cuthill–McKee finds a good ordering for a <span>$30×30$</span> matrix:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; Random.seed!(13);

julia&gt; (n, k) = (30, 5);

julia&gt; A = random_banded_matrix(n, k);

julia&gt; perm = randperm(n);

julia&gt; A_shuffled = A[perm, perm];

julia&gt; bandwidth(A)
5

julia&gt; bandwidth(A_shuffled) # Much larger after shuffling
25

julia&gt; minimize_bandwidth(A_shuffled, Minimization.CuthillMcKee())
Results of Bandwidth Minimization Algorithm
 * Algorithm: Cuthill–McKee
 * Approach: heuristic
 * Minimum Bandwidth: 5
 * Original Bandwidth: 25
 * Matrix Size: 30×30</code></pre><p>Cuthill–McKee finds a good ordering for a structurally symmetric <span>$183×183$</span> matrix with multiple (separate) connected components:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random, SparseArrays

julia&gt; Random.seed!(37452);

julia&gt; (max_cc_size, max_band, p, num_ccs) = (60, 9, 0.2, 7);

julia&gt; components = Vector{SparseMatrixCSC{Float64, Int64}}(undef, num_ccs);

julia&gt; for i in 1:num_ccs # Some components may themselves be disconnected
           cc_size = rand(1:max_cc_size);
           cc_band = rand(0:min(max_band, cc_size - 1));
           components[i] = sparse(random_banded_matrix(cc_size, cc_band; p=p));
       end

julia&gt; A = blockdiag(components...); # `A` has least 7 connected components

julia&gt; perm = randperm(sum(map(cc -&gt; size(cc, 1), components)));

julia&gt; A_shuffled = A[perm, perm];

julia&gt; res = minimize_bandwidth(A_shuffled, Minimization.CuthillMcKee());

julia&gt; A # The original matrix
276×276 SparseMatrixCSC{Float64, Int64} with 464 stored entries:
⎡⢾⡷⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤
⎢⠀⠘⢻⣲⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠘⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠈⠿⡧⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠉⢯⡷⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠚⣤⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⢻⣶⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠯⡧⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣤⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠛⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠱⣢⡀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⡢⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢴⣷⡀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠿⣧⡀⠀⎥
⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⡷⎦

julia&gt; A_shuffled # A far-from-optimal ordering of `A`
276×276 SparseMatrixCSC{Float64, Int64} with 464 stored entries:
⎡⠁⢄⠀⢀⠀⠀⠀⢀⠠⠀⠀⠐⠀⠀⠀⠐⢀⡐⠀⠀⠀⢀⠀⠀⠀⠀⠐⠀⢠⠀⠀⠀⡄⠀⠀⠐⠀⠀⠂⠄⎤
⎢⠀⢀⠱⠂⠀⠀⠀⠈⠀⠀⠀⠀⠀⠀⢨⠀⠀⠀⠀⠀⡀⠁⠠⠀⠘⠀⠀⠡⢀⠈⠀⠀⠀⠀⠀⠀⠄⠀⠁⠁⎥
⎢⠀⠀⠀⠀⠑⢀⠀⠂⠀⠀⠀⠀⢐⠀⠀⠠⠈⠠⠀⠀⠀⠐⠀⠐⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⢢⢀⢀⠀⎥
⎢⠀⢀⡀⠀⠠⠀⠁⠄⠀⠠⠀⠄⠀⠀⠀⠄⠀⠀⠀⠀⢀⠀⠀⢀⠀⠑⠀⠀⠐⠠⠀⠀⠠⠨⠂⠀⠀⠀⠀⠀⎥
⎢⠀⠂⠀⠀⠀⠀⠀⡀⠱⢆⡀⠂⠀⠀⠀⠀⠀⠀⢀⢊⠀⠐⠐⠈⠀⠈⠀⢀⠄⠀⡀⠀⢁⢀⠠⠀⠃⠀⠊⠀⎥
⎢⢀⠀⠀⠀⠀⠀⠀⠄⠠⠈⠑⠀⢀⠐⠀⠌⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀⠀⢀⠉⢀⠀⠠⠈⠀⠀⣁⠁⎥
⎢⠀⠀⠀⠀⠐⠐⠀⠀⠀⠀⢀⠐⠁⠄⠈⠀⢌⠀⠆⠠⢀⠀⠄⠐⠰⠀⠀⠀⠁⠰⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⢀⠀⠂⠒⠀⡀⠀⠄⠀⠀⡀⠄⠂⠀⠐⢄⠁⢀⠀⠀⠀⡀⠀⠀⠀⠀⡠⠀⠀⠀⠀⠀⠀⠀⠀⢈⠀⠀⠀⠁⎥
⎢⢀⠰⠀⠀⠂⡀⠀⠀⠀⠀⠀⠈⠂⠑⠁⢀⠐⠄⠄⠂⠂⠜⠄⠀⠀⠀⡄⠀⠀⢀⠀⠠⠀⢀⠄⠀⢀⠀⠂⡂⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⡠⢐⠀⠀⠈⡁⠀⠀⠠⠁⠀⢀⠀⠀⠀⠀⡀⠀⠀⢀⠀⠈⠃⠀⠸⠈⠠⠀⠀⠀⢄⠂⎥
⎢⠀⢀⠄⠈⢀⠀⠀⠐⢀⠀⠀⠀⠀⠐⠀⠠⣈⠄⠀⠀⠐⢀⠀⡀⠀⠀⠀⠀⠀⠀⠐⠀⠀⠊⠀⠠⠀⠐⠀⠀⎥
⎢⠀⠀⠀⠂⢀⠀⠀⢀⡐⠀⠀⠀⢀⠁⠀⠀⠀⠁⠀⠀⠀⠠⠄⣥⠉⠈⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⡀⠀⠀⠀⎥
⎢⠀⠀⠒⠀⠀⠀⢄⠀⡀⠀⠀⠀⠐⠂⠀⠀⠀⠀⠀⠈⠀⠀⡃⠀⠁⢀⠀⠀⢀⡀⢈⠈⠀⠀⠀⠂⠀⠠⠂⠂⎥
⎢⠐⠀⠄⡀⠀⠀⠀⠀⠀⢀⠀⠈⠀⠀⠀⠊⠀⠉⠀⢀⠀⠀⠀⠀⠀⠀⠑⠀⠀⠀⠀⠀⢀⠀⠈⠀⠛⠃⢄⠀⎥
⎢⠀⠒⡀⠐⠀⠀⠐⡀⠀⠁⠀⠀⢁⡀⠀⠀⠀⢀⡀⠀⠀⠀⠀⠀⠀⠰⠀⠀⠀⢄⠀⠰⠀⠠⠠⢀⠀⠀⢂⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⡄⠐⠂⠀⠀⠀⠀⡀⠉⠀⠐⠀⠀⠈⡂⠐⠀⠀⢀⡀⠀⣠⠀⠄⠠⠀⠀⡀⠀⠀⎥
⎢⠀⠉⠀⠀⠀⠀⡀⡂⠁⢐⠀⠐⠀⠀⠀⠀⠀⢀⡒⠂⡠⠀⠀⠀⠀⠀⠀⠐⠀⡀⠀⠄⠑⠄⠀⠀⠀⠀⠀⠀⎥
⎢⢀⠀⠀⠀⠀⡀⠈⠀⠀⠂⡀⠂⠀⠀⡀⢀⠀⠁⠀⠂⠀⡀⠀⠀⠠⠀⠂⠀⠀⢂⠀⠂⠀⠀⠁⢀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠁⠈⢒⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠐⠀⠀⢀⠀⠀⠈⠀⡀⠿⠀⠀⠀⠀⠠⠀⠀⠀⠀⠱⠆⠀⠀⎥
⎣⠈⠄⠅⠀⠀⠐⠀⠀⠊⠀⠅⠘⠀⠀⠄⠀⠨⠠⠠⠑⠀⠀⠀⠀⠨⠀⠀⠑⠈⠐⠀⠀⠀⠀⠀⠀⠀⠀⠔⢅⎦

julia&gt; A_shuffled[res.ordering, res.ordering] # A near-optimal reordering of `A_shuffled`
276×276 SparseMatrixCSC{Float64, Int64} with 464 stored entries:
⎡⠱⣦⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤
⎢⠀⠉⠻⣦⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠘⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⡦⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⡦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠺⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠚⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢄⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢄⠀⠀⎥
⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⢄⎦

julia&gt; bandwidth(A)
7

julia&gt; bandwidth(A_shuffled) # Much larger after shuffling
266

julia&gt; res # Even better than the original bandwidth (which was, clearly, not yet optimal)
Results of Bandwidth Minimization Algorithm
 * Algorithm: Cuthill–McKee
 * Approach: heuristic
 * Minimum Bandwidth: 5
 * Original Bandwidth: 266
 * Matrix Size: 276×276</code></pre><p><strong>Notes</strong></p><p>Note that the <code>node_selector</code> field must be of the form <code>(A::AbstractMatrix{Bool}) -&gt; Integer</code> (i.e., it must take in an boolean matrix and return an integer). If this is not the case, an <code>ArgumentError</code> is thrown upon construction.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Heuristic/solvers/cuthill_mckee.jl#L7-L192">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Heuristic.GibbsPooleStockmeyer" href="#MatrixBandwidth.Minimization.Heuristic.GibbsPooleStockmeyer"><code>MatrixBandwidth.Minimization.Heuristic.GibbsPooleStockmeyer</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GibbsPooleStockmeyer &lt;: HeuristicSolver &lt;: AbstractSolver &lt;: AbstractAlgorithm</code></pre><p>The <em>Gibbs–Poole–Stockmeyer algorithm</em> is a heuristic method for minimizing the bandwidth of a structurally symmetric matrix <span>$A$</span>. It considers the graph <span>$G(A)$</span> whose adjacency matrix is <span>$A$</span> (ignoring weights and self-loops) and builds an ordering by identifying a pair of &quot;endpoints&quot; in the graph far from each other, constructing sets of levels from these endpoints, and merging these level structures in such a way that minimizes the size of the largest level in the final combined structure. Based on the classical reverse Cuthill–McKee algorithm [<a href="#Geo71">Geo71</a>], this heuristic typically produces an ordering which induces a matrix bandwidth either equal to or very close to the true minimum, with improvements in bandwidths over reverse Cuthill–McKee more noticeable once input size exceeds <span>$400×400$</span> or so [<a href="#GPS76">GPS76</a>, pp. 246–47].</p><p>Whereas the original paper outlined a strategy for conditionally reversing the orderings of individual &quot;connected components&quot; [<a href="#GPS76">GPS76</a>, p. 241] (treating the input matrix <span>$A$</span> as an undirected graph), this implementation instead reverses the entire final ordering in every case, similarly to <a href="#MatrixBandwidth.Minimization.Heuristic.ReverseCuthillMcKee"><code>ReverseCuthillMcKee</code></a>. Conditional reversals are not only more complex to implement but also slightly more time-consuming, with the only benefit being a marginally smaller <em>matrix profile</em> (a measure of how far, on average, nonzero entries are from the diagonal). Since such reversal strategies do not affect matrix bandwidth (the primary focus of this package), we thus opt for the simpler unconditional reversal.</p><p>As noted above, the Gibbs–Poole–Stockmeyer algorithm requires structurally symmetric input (that is, <span>$A[i, j]$</span> must be nonzero if and only if <span>$A[j, i]$</span> is nonzero for <span>$1 ≤ i, j ≤ n$</span>).</p><p><strong>Fields</strong></p><ul><li><code>node_selector::Function</code>: a function that selects a node from some connected component of   the input matrix from which to start the breadth-first search. If no custom heuristic is   specified, this field defaults to <a href="../private_api/#MatrixBandwidth.Minimization.Heuristic.pseudo_peripheral_node-Tuple{AbstractMatrix{Bool}}"><code>pseudo_peripheral_node</code></a>, which picks a node   &quot;farthest&quot; from the others in the component (not necessarily the lowest-degree node).</li></ul><p><strong>Performance</strong></p><p>Given an <span>$n×n$</span> input matrix <span>$A$</span>, the Gibbs–Poole–Stockmeyer algorithm runs in <span>$O(n²)$</span> time.</p><p>[<a href="#Lew82">Lew82</a>] provides a notably faster and more memory-efficient implementation, relying on sparse storage of the input matrix. However, this would run counter to our desire to provide a bandwidth minimization API for all <code>AbstractMatrix{&lt;:Number}</code> types, including dense matrices. (In the future, however, we may indeed consider supporting this more performant implementation for sparse matrices.)</p><p>On that note, Gibbs–Poole–Stockmeyer has been found to take considerably less time than reverse Cuthill–McKee when matrices are stored in sparse format [<a href="#GPS76">GPS76</a>, pp. 246–47]. The dense-matrix implementations of both algorithms in this package, however, result in reverse Cuthill–McKee consistently outperforming Gibbs–Poole–Stockmeyer in terms of runtime (although Gibbs–Poole–Stockmeyer still typically produces lower-bandwidth orderings for larger matrices). This further motivates the desire to implement a sparse version of both algorithms in the future.</p><p><strong>Examples</strong></p><p>In the following examples, <a href="#MatrixBandwidth.random_banded_matrix-Tuple{Int64, Int64}"><code>MatrixBandwidth.random_banded_matrix</code></a> is used to generate random matrices with minimum bandwidth <em>close to</em> <span>$k$</span>. In some cases, however, the true minimum bandwidth up to symmetric permutation may be even less than <span>$k$</span>, making it hard to verify whether Gibbs–Poole–Stockmeyer finds a truly optimal ordering or simply a near-optimal one. Nevertheless, the results are still very good in practice.</p><p>Gibbs–Poole–Stockmeyer finds a good ordering for a <span>$40×40$</span> matrix:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; Random.seed!(561);

julia&gt; (n, k) = (40, 7);

julia&gt; A = random_banded_matrix(n, k);

julia&gt; perm = randperm(n);

julia&gt; A_shuffled = A[perm, perm];

julia&gt; bandwidth(A)
7

julia&gt; bandwidth(A_shuffled)
37

julia&gt; minimize_bandwidth(A_shuffled, Minimization.GibbsPooleStockmeyer())
Results of Bandwidth Minimization Algorithm
 * Algorithm: Gibbs–Poole–Stockmeyer
 * Approach: heuristic
 * Minimum Bandwidth: 7
 * Original Bandwidth: 37
 * Matrix Size: 40×40</code></pre><p>Gibbs–Poole–Stockmeyer finds a good ordering for a <span>$748×748$</span> matrix with multiple (separate) connected components:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random, SparseArrays

julia&gt; Random.seed!(271828);

julia&gt; (max_cc_size, max_band, p, num_ccs) = (120, 13, 0.3, 11);

julia&gt; components = Vector{SparseMatrixCSC{Float64, Int64}}(undef, num_ccs);

julia&gt; for i in 1:num_ccs # Some components may themselves be disconnected
           cc_size = rand(0:max_cc_size);
           cc_band = rand(1:min(max_band, cc_size - 1));
           components[i] = sparse(random_banded_matrix(cc_size, cc_band; p=p));
       end

julia&gt; A = blockdiag(components...); # `A` has least 8 connected components

julia&gt; perm = randperm(sum(map(cc -&gt; size(cc, 1), components)));

julia&gt; A_shuffled = A[perm, perm];

julia&gt; res = minimize_bandwidth(A_shuffled, Minimization.GibbsPooleStockmeyer());

julia&gt; A # The original matrix
748×748 SparseMatrixCSC{Float64, Int64} with 2526 stored entries:
⎡⢿⣷⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤
⎢⠀⠘⠿⣧⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠘⢿⣷⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠉⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⎥
⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⎦

julia&gt; A_shuffled # A far-from-optimal ordering of `A`
748×748 SparseMatrixCSC{Float64, Int64} with 2526 stored entries:
⎡⠰⣦⢎⢪⢐⠆⣗⣔⠆⠀⠀⠠⢃⡦⠵⠸⡐⢌⠴⠤⣤⢅⢒⠰⡢⡄⡰⣄⢂⢊⠎⠀⢝⡀⣼⠤⠅⢒⠰⠢⎤
⎢⡪⣑⡋⢌⠈⢀⣖⣠⢠⡀⡂⠀⠐⠤⣠⠰⣃⠀⢊⢃⡨⡇⡝⠢⣀⠈⢬⢁⠽⡼⠐⡦⠤⠐⠤⠚⠪⢠⠦⢈⎥
⎢⠰⠔⠂⢀⡑⢌⠐⠀⠐⡒⠤⡅⠂⡱⡕⠈⡑⠹⠱⣨⢓⣑⠀⠂⠀⢬⢂⣅⡤⢑⠢⢁⠀⢒⡑⡬⡆⠚⠀⡀⎥
⎢⢙⢽⠘⣹⠐⠀⢱⣶⠂⠂⠈⡵⢰⡤⠖⠰⡂⣠⢨⢰⣛⠀⠰⡌⡐⠄⠠⡦⠜⠃⠐⣦⢂⠄⠷⢅⣉⠰⠿⢴⎥
⎢⠈⠁⠀⠲⢰⠠⠨⠀⣑⣼⠩⡃⠂⢰⢁⠓⢐⣘⣂⢦⠂⡐⠔⢄⡨⣃⠦⢁⢈⠀⡂⠀⡅⠈⠀⢌⠀⡀⠵⠫⎥
⎢⠀⡀⠈⠈⠄⠧⢆⡤⠧⠢⠕⢅⡍⢔⡴⠀⡀⠆⡨⠈⣄⠲⠌⠳⢁⠐⠠⠴⠀⠄⠔⠺⠁⡉⢂⣭⠠⡠⠀⣉⎥
⎢⠩⡴⠐⡄⢌⡠⠐⡶⢈⣀⢃⢍⠋⢄⡖⢩⢌⣖⠈⢀⣴⣙⡀⢓⠁⠠⢬⢈⠅⡤⠅⢰⣁⠌⣌⠆⠸⢀⠒⣁⎥
⎢⣑⡃⢀⡚⡑⠉⢘⡁⢥⠐⠐⠋⡜⣉⠑⢄⠍⣢⣓⢉⠊⢖⠀⢐⡀⠲⡈⢑⠀⠊⠀⠛⠀⢃⠘⠀⢁⣒⢀⢁⎥
⎢⡐⢌⠉⠘⣕⡈⠈⣨⣐⢰⠠⠌⢢⢵⠣⣡⢕⣱⣂⢭⢎⠜⠀⠉⡂⣃⢐⠅⠒⠔⡂⠨⡂⢳⠍⢤⠰⡠⣩⡏⎥
⎢⠐⡇⠮⢐⡑⣢⢂⣒⠨⣜⡂⠊⠂⢀⡝⢘⡌⣜⣱⢞⠂⠾⠊⢔⠙⣢⢭⣹⠑⡑⠀⡉⡄⠁⠀⠴⣇⣃⠃⡑⎥
⎢⠄⢟⠦⠮⢝⢰⠛⠘⢈⠠⢠⡙⣔⢻⢪⢄⣊⠕⣨⡄⢕⣵⢐⠊⠊⣱⡠⣢⠀⠣⠀⣍⠔⠀⢝⢄⣌⡄⡌⠆⎥
⎢⢘⡐⠳⡉⠠⠀⡐⠦⠐⢅⢦⡁⢤⢈⢀⢀⡄⠀⢊⢄⡰⠐⠑⢄⠴⡅⠁⢆⡠⣄⠤⢐⠈⡀⠺⠂⠀⢀⡴⡀⎥
⎢⠈⠮⡀⠘⡀⣄⠐⠌⠦⢪⢁⠐⠁⡀⢠⡈⠬⢨⠳⣠⢎⣠⠔⠧⠛⢄⡀⡹⠡⠌⠃⢀⠣⠁⠉⢀⠑⠁⢦⡉⎥
⎢⠐⢮⠆⢓⠌⢴⠠⡦⠌⢃⢀⡆⡂⢓⢆⢈⠔⠔⣇⣳⠠⣪⠡⢄⣄⡨⠛⢄⠑⢐⠐⡠⠪⠃⢤⢁⡝⠐⡀⢎⎥
⎢⡨⢐⣓⡧⢄⢋⠶⠁⠂⠐⠀⠄⠁⡥⡠⠀⢘⠄⢕⠠⠤⡀⠀⢮⡁⠆⢑⢀⠕⣥⣑⠼⡀⡅⠝⠢⠠⠄⠀⠔⎥
⎢⠊⠁⠰⡤⠌⢂⠰⣤⠈⠈⣰⡁⢁⣁⣤⠀⡈⡈⡄⠠⡄⢤⢀⢃⠉⢀⠐⡠⣑⡜⠑⢄⠀⡼⠠⠲⢈⠠⢃⠁⎥
⎢⠓⠱⢀⠃⢠⢀⠈⠔⡁⠉⡅⠠⡁⠜⠤⢀⢬⣈⠄⠉⠐⠁⠂⠠⠍⠂⠮⠂⠄⠬⣀⡤⠵⢇⠈⠀⠣⠡⠩⡎⎥
⎢⠒⡟⣠⠃⡑⡬⠝⢇⡀⢄⡌⣴⠢⠝⠒⠀⠃⣅⢀⡄⠓⢕⠺⠂⠃⢀⠄⢓⠳⡁⢠⡂⠂⠀⠛⢄⡀⢒⠒⠂⎥
⎢⢡⢁⠊⣂⣨⠉⢃⡘⠀⠠⠀⡢⠒⢂⢡⢰⠐⡢⠭⢹⠂⠽⠀⢀⠕⠀⢓⠉⠀⠆⠂⡐⠍⡂⢠⢈⠁⣤⢀⡈⎥
⎣⠰⡂⡈⢃⠀⠠⢛⣇⡵⡃⡄⢠⠜⢠⠄⢐⡧⠾⢍⠠⠢⠍⠐⠫⡌⠳⡠⢌⢀⠄⠍⠐⡣⠦⠸⠀⡀⠰⡛⢌⎦

julia&gt; A_shuffled[res.ordering, res.ordering] # A near-optimal reordering of `A_shuffled`
748×748 SparseMatrixCSC{Float64, Int64} with 2526 stored entries:
⎡⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤
⎢⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠑⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠱⢆⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠱⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢻⣶⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⡀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣤⣀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢻⣶⡀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⣤⡀⠀⎥
⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣦⎦

julia&gt; bandwidth(A)
12

julia&gt; bandwidth(A_shuffled) # Much larger after shuffling
731

julia&gt; res # Gets very close to the original bandwidth
Results of Bandwidth Minimization Algorithm
 * Algorithm: Gibbs–Poole–Stockmeyer
 * Approach: heuristic
 * Minimum Bandwidth: 18
 * Original Bandwidth: 731
 * Matrix Size: 748×748</code></pre><p><strong>Notes</strong></p><p>Note that the <code>node_selector</code> field must be of the form <code>(A::AbstractMatrix{Bool}) -&gt; Integer</code> (i.e., it must take in an boolean matrix and return an integer). If this is not the case, an <code>ArgumentError</code> is thrown upon construction.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Heuristic/solvers/gibbs_poole_stockmeyer.jl#L7-L207">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Heuristic.ReverseCuthillMcKee" href="#MatrixBandwidth.Minimization.Heuristic.ReverseCuthillMcKee"><code>MatrixBandwidth.Minimization.Heuristic.ReverseCuthillMcKee</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ReverseCuthillMcKee &lt;: HeuristicSolver &lt;: AbstractSolver &lt;: AbstractAlgorithm</code></pre><p>The <em>reverse Cuthill–McKee algorithm</em> is a variant of the <em>Cuthill–McKee algorithm</em>—a heuristic method for minimizing the bandwidth of a structurally symmetric matrix <span>$A$</span>. Cuthill–McKee considers the graph <span>$G(A)$</span> whose adjacency matrix is <span>$A$</span> (ignoring weights and self-loops) and performs a breadth-first search of each connected component of <span>$G(A)$</span>, starting from a low-degree node then visiting its neighbors in order of increasing degree. Particularly effective when <span>$A$</span> is sparse, this heuristic typically produces an ordering which induces a matrix bandwidth either equal to or very close to the true minimum [<a href="#CM69">CM69</a>, pp. 157–58]. The reverse Cuthill–McKee algorithm simply reverses the ordering produced by application of Cuthill–McKee; it was found in [<a href="#Geo71">Geo71</a>, pp. 114–15] that although the bandwidth remains the same, this tends to produce a more optimal <em>matrix profile</em> (a measure of how far, on average, nonzero entries are from the diagonal).</p><p>As noted above, the reverse Cuthill–McKee algorithm requires structurally symmetric input (that is, <span>$A[i, j]$</span> must be nonzero if and only if <span>$A[j, i]$</span> is nonzero for <span>$1 ≤ i, j ≤ n$</span>).</p><p><strong>Performance</strong></p><p>Given an <span>$n×n$</span> input matrix <span>$A$</span>, the reverse Cuthill–McKee algorithm runs in <span>$O(n²)$</span> time.</p><p>[<a href="#CG80">CG80</a>] provide a linear-time implementation in the number of nonzero entries of <span>$A$</span>, which is still quadratic when <span>$A$</span> is dense but often much faster when dealing with sparse matrices. However, this would require that <span>$A$</span> be stored as a graph or a sparse matrix, which runs counter to our desire to provide a bandwidth minimization API for all <code>AbstractMatrix{&lt;:Number}</code> types, including dense matrices. (In the future, however, we may indeed consider supporting this more performant implementation for sparse matrices.)</p><p><strong>Fields</strong></p><ul><li><code>node_selector::Function</code>: a function that selects a node from some connected component of   the input matrix from which to start the breadth-first search. If no custom heuristic is   specified, this field defaults to <a href="../private_api/#MatrixBandwidth.Minimization.Heuristic.pseudo_peripheral_node-Tuple{AbstractMatrix{Bool}}"><code>pseudo_peripheral_node</code></a>, which picks a node   &quot;farthest&quot; from the others in the component (not necessarily the lowest-degree node).</li></ul><p><strong>Examples</strong></p><p>In the following examples, <a href="#MatrixBandwidth.random_banded_matrix-Tuple{Int64, Int64}"><code>MatrixBandwidth.random_banded_matrix</code></a> is used to generate random matrices with minimum bandwidth <em>close to</em> <span>$k$</span>. In some cases, however, the true minimum bandwidth up to symmetric permutation may be even less than <span>$k$</span>, making it hard to verify whether reverse Cuthill–McKee finds a truly optimal ordering or simply a near-optimal one. Nevertheless, the results are still very good in practice.</p><p>Reverse Cuthill–McKee finds a good ordering for a <span>$35×35$</span> matrix:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random

julia&gt; Random.seed!(87);

julia&gt; (n, k) = (35, 3);

julia&gt; A = random_banded_matrix(n, k);

julia&gt; perm = randperm(n);

julia&gt; A_shuffled = A[perm, perm];

julia&gt; bandwidth(A)
3

julia&gt; bandwidth(A_shuffled) # Much larger after shuffling
30

julia&gt; minimize_bandwidth(A_shuffled, Minimization.ReverseCuthillMcKee())
Results of Bandwidth Minimization Algorithm
 * Algorithm: Reverse Cuthill–McKee
 * Approach: heuristic
 * Minimum Bandwidth: 3
 * Original Bandwidth: 30
 * Matrix Size: 35×35</code></pre><p>Reverse Cuthill–McKee finds a good ordering for a <span>$235×235$</span> matrix with multiple (separate) connected components:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random, SparseArrays

julia&gt; Random.seed!(5747);

julia&gt; (max_cc_size, max_band, p, num_ccs) = (60, 9, 0.2, 8);

julia&gt; components = Vector{SparseMatrixCSC{Float64, Int64}}(undef, num_ccs);

julia&gt; for i in 1:num_ccs # Some components may themselves be disconnected
           cc_size = rand(0:max_cc_size);
           cc_band = rand(1:min(max_band, cc_size - 1));
           components[i] = sparse(random_banded_matrix(cc_size, cc_band; p=p));
       end

julia&gt; A = blockdiag(components...); # `A` has least 8 connected components

julia&gt; perm = randperm(sum(map(cc -&gt; size(cc, 1), components)));

julia&gt; A_shuffled = A[perm, perm];

julia&gt; res = minimize_bandwidth(A_shuffled, Minimization.ReverseCuthillMcKee());

julia&gt; A # The original matrix
235×235 SparseMatrixCSC{Float64, Int64} with 445 stored entries:
⎡⢾⣳⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤
⎢⠀⠘⢿⡷⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠈⠏⣥⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠉⢴⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠙⠻⢂⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣮⣿⣢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠚⢿⡳⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⢰⣶⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠾⡧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⣢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⡠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⠖⣀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⢏⡱⣄⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢮⣷⣄⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢻⣲⣄⠀⎥
⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢻⢖⎦

julia&gt; A_shuffled # A far-from-optimal ordering of `A`
235×235 SparseMatrixCSC{Float64, Int64} with 445 stored entries:
⎡⠑⠄⠀⠀⠀⠀⠀⢀⢀⠀⠄⠀⠀⠀⠀⠀⠀⠀⠀⠄⠀⡐⠀⠐⠀⠂⠀⠀⠀⠀⠀⠀⡂⠀⢀⠄⠁⠠⠐⠀⎤
⎢⠀⠀⠀⢄⡀⠀⢁⠀⠀⠈⠀⠁⠀⠀⠀⢀⠁⠄⠈⠀⠀⠀⠀⠐⠀⠀⠀⠀⠀⠈⠀⠂⠠⠀⠀⠀⠀⠀⡀⠐⎥
⎢⠀⠀⠀⠈⠁⢀⠀⠑⠀⢀⠁⢀⠀⠈⠀⠘⠌⠀⢀⠀⠄⠀⠂⡄⠄⠁⠀⠀⠈⠀⠀⠀⠀⠀⠀⠀⣂⠀⠀⠀⎥
⎢⠀⢀⠁⠐⢄⠀⠠⢆⠀⠀⠀⠀⠀⠀⠀⢀⠠⡀⠀⠀⠠⠀⠀⠀⠐⠀⠀⡀⠀⢀⠀⠀⠀⠈⠀⡀⠀⠀⠘⠀⎥
⎢⠀⠐⡀⠀⠀⢀⠀⠀⢀⢔⠈⢀⠀⠀⣐⠀⠀⠀⢀⠀⠀⠀⠀⠀⠐⠀⠄⢠⠀⠀⠀⠀⠀⠀⠀⡀⠀⠈⠣⡀⎥
⎢⠀⠁⠄⠀⠁⢀⠀⠀⠂⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⠀⡀⠀⠀⡐⠐⠀⡀⠀⠀⠂⠀⠀⠀⢀⠀⠀⠄⠀⎥
⎢⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⠀⠀⠑⠀⠀⠀⠀⠀⠀⠀⡠⠀⡀⠀⠀⠄⠀⠀⠠⠀⠀⠠⠀⠀⠀⠀⡠⠄⠀⠄⎥
⎢⠀⠀⠀⢀⣀⠀⠀⢀⠐⠘⠀⠀⠀⠀⠕⢅⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀⢀⠐⡀⠀⠈⠀⠂⠀⠀⢀⠀⠃⠀⠄⎥
⎢⠀⠀⠁⠄⠂⠁⠀⠢⠀⠀⠀⠀⠀⠀⠀⠀⠛⢄⢸⠘⠀⠀⠀⠀⠄⠈⠁⠀⠀⠨⠀⠀⢀⠀⠀⠨⠀⠀⠈⠀⎥
⎢⠀⠄⠂⠀⠀⠐⠀⠀⠀⠐⠀⠀⠀⠀⠀⠀⣒⠒⠁⠀⢠⠀⠀⠀⠐⠀⠀⠀⢁⠀⠐⠈⠀⠀⠂⠀⠂⡀⠀⠀⎥
⎢⢀⠠⠀⠀⠀⠁⠀⠂⠀⠀⠀⠂⠀⠊⠀⠀⠀⠀⠀⠒⠑⠀⠀⠀⠀⠂⢀⠁⡂⠀⠀⢀⠀⠀⠀⠀⠁⠂⡊⠂⎥
⎢⢀⠀⢀⠀⠈⠤⠀⠀⠀⠀⠀⠈⠀⠈⠀⠠⠀⠀⠀⠀⠀⠀⠁⢀⠅⠀⢀⠀⠀⠀⢀⠀⡁⠀⠀⠀⠀⠠⠀⠀⎥
⎢⠠⠀⠀⠀⠄⠁⠐⠀⠐⠀⢀⠠⠀⠄⠀⠀⡀⠁⠐⠀⠠⠀⠁⠁⠁⠀⢀⠄⠀⠉⠀⠃⠀⠀⠀⠀⠠⢠⠂⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠠⠀⣁⠐⠀⠀⠀⢀⠐⠁⠀⠀⠀⠄⠐⠀⠐⠀⠔⡑⠌⠀⠀⠀⠀⠢⢉⠀⠀⠄⠀⠀⠄⎥
⎢⠀⠀⡀⠀⠂⠀⠀⢀⠀⠀⠀⠈⠀⠂⠀⠈⡀⡀⠁⠐⠈⠈⠀⠀⡄⠀⠀⠀⠄⠅⠀⡀⠀⠀⠀⠠⠀⠰⠀⠂⎥
⎢⠀⠀⠠⠀⠀⠀⠀⠀⠀⠀⠠⠀⠀⡀⠂⠀⠀⠀⡐⠀⠀⢀⠀⠐⠤⠀⠀⠀⠀⠠⠀⢀⠀⠀⠀⠀⠀⠀⠒⢀⎥
⎢⠈⠈⠀⠂⠀⠀⡀⠀⠀⠀⠀⠀⠀⠀⠈⠀⠀⠐⠀⠀⠀⠀⠁⠈⠀⠀⡌⢂⠀⠀⠀⠀⠀⠄⠈⠀⠀⡀⠀⠀⎥
⎢⠀⠔⠀⠀⠀⠀⠀⠠⠀⠠⠀⢀⠀⠀⠀⢀⡀⡀⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀⠂⠀⠡⠂⠀⠀⡠⠀⎥
⎢⠁⡀⠀⠀⠈⠘⠀⠀⡀⠀⠀⠀⠀⠎⠤⠀⠀⠀⠈⠠⠡⠀⠀⡀⠀⣂⠀⠁⢀⡀⠀⠀⠀⠠⠀⠀⠰⠆⠌⠀⎥
⎣⠐⠀⢀⠈⠀⠀⠒⠀⠉⠢⠀⠁⠀⠄⠀⠄⠂⠀⠀⠀⠪⠈⠀⠀⠈⠀⠀⠄⠠⠀⠘⢀⠀⠀⠀⠊⠂⠁⠐⢀⎦

julia&gt; A_shuffled[res.ordering, res.ordering] # A near-optimal reordering of `A_shuffled`
235×235 SparseMatrixCSC{Float64, Int64} with 445 stored entries:
⎡⠁⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎤
⎢⠀⠀⠁⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠈⠀⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠁⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠑⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢛⢔⢤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠳⣿⣿⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠚⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠡⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠫⣦⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⢿⣷⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠯⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠯⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠺⢆⡄⠀⠀⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⢻⣲⣄⠀⠀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢻⣶⡀⠀⠀⠀⎥
⎢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣢⡀⠀⎥
⎣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠱⣦⎦

julia&gt; bandwidth(A)
9

julia&gt; bandwidth(A_shuffled) # Much larger after shuffling
226

julia&gt; res # Gets very close to the original bandwidth
Results of Bandwidth Minimization Algorithm
 * Algorithm: Reverse Cuthill–McKee
 * Approach: heuristic
 * Minimum Bandwidth: 9
 * Original Bandwidth: 226
 * Matrix Size: 235×235</code></pre><p><strong>Notes</strong></p><p>Note that the <code>node_selector</code> field must be of the form <code>(A::AbstractMatrix{Bool}) -&gt; Integer</code> (i.e., it must take in an boolean matrix and return an integer). If this is not the case, an <code>ArgumentError</code> is thrown upon construction.</p><p>See also the documentation for <a href="#MatrixBandwidth.Minimization.Heuristic.CuthillMcKee"><code>CuthillMcKee</code></a>—the original (non-reversed) algorithm. (Indeed, the reverse Cuthill–McKee method of <code>_bool_minimal_band_ordering</code> is merely a wrapper around the Cuthill–McKee method.)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Heuristic/solvers/cuthill_mckee.jl#L206-L395">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Metaheuristic" href="#MatrixBandwidth.Minimization.Metaheuristic"><code>MatrixBandwidth.Minimization.Metaheuristic</code></a> — <span class="docstring-category">Module</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MatrixBandwidth.Minimization.Metaheuristic</code></pre><p>Metaheuristic solvers for matrix bandwidth minimization.</p><p>Metaheuristic methods are those which employ higher-level iterative search frameworks such as stochastic techniques or nature-inspired processes to survey the global search space and escape local minima. Unlike heuristic methods—which follow fixed deterministic procedures—metaheuristics adaptively refine candidate solutions over multiple iterations. Although metaheuristic approaches are often slower than heuristic ones (but certainly still faster than exact ones), they shine in complex cases where the latter may get trapped in poor-quality local minima.</p><p>The following metaheuristic algorithms are currently supported:</p><ul><li>Greedy randomized adaptive search procedure (GRASP) (<a href="#MatrixBandwidth.Minimization.Metaheuristic.GRASP"><code>GRASP</code></a>)</li><li>Simulated annealing (<a href="#MatrixBandwidth.Minimization.Metaheuristic.SimulatedAnnealing"><code>SimulatedAnnealing</code></a>)</li><li>Genetic algorithm (<a href="#MatrixBandwidth.Minimization.Metaheuristic.GeneticAlgorithm"><code>GeneticAlgorithm</code></a>)</li></ul><p>This submodule is part of the <code>MatrixBandwidth.Minimization</code> submodule of the <a href="https://github.com/Luis-Varona/MatrixBandwidth.jl">MatrixBandwidth.jl</a> package.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Metaheuristic/Metaheuristic.jl#L7-L27">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Metaheuristic.GRASP" href="#MatrixBandwidth.Minimization.Metaheuristic.GRASP"><code>MatrixBandwidth.Minimization.Metaheuristic.GRASP</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GRASP &lt;: MetaheuristicSolver &lt;: AbstractSolver &lt;: AbstractAlgorithm</code></pre><p>[TODO: Write here]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Metaheuristic/solvers/grasp.jl#L7-L11">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Metaheuristic.GeneticAlgorithm" href="#MatrixBandwidth.Minimization.Metaheuristic.GeneticAlgorithm"><code>MatrixBandwidth.Minimization.Metaheuristic.GeneticAlgorithm</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">GeneticAlgorithm &lt;: MetaheuristicSolver &lt;: AbstractSolver &lt;: AbstractAlgorithm</code></pre><p>[TODO: Write here]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Metaheuristic/solvers/genetic_algorithm.jl#L7-L11">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Minimization.Metaheuristic.SimulatedAnnealing" href="#MatrixBandwidth.Minimization.Metaheuristic.SimulatedAnnealing"><code>MatrixBandwidth.Minimization.Metaheuristic.SimulatedAnnealing</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SimulatedAnnealing &lt;: MetaheuristicSolver &lt;: AbstractSolver &lt;: AbstractAlgorithm</code></pre><p>[TODO: Write here]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Minimization/Metaheuristic/solvers/simulated_annealing.jl#L7-L11">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Recognition" href="#MatrixBandwidth.Recognition"><code>MatrixBandwidth.Recognition</code></a> — <span class="docstring-category">Module</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">MatrixBandwidth.Recognition</code></pre><p>Algorithms for matrix bandwidth recognition in Julia.</p><p>The <em>bandwidth</em> of an <span>$n×n$</span> matrix <span>$A$</span> is the minimum non-negative integer <span>$k ∈ [0, n - 1]$</span> such that <span>$A[i, j] = 0$</span> whenever <span>$|i - j| &gt; k$</span>. Equivalently, <span>$A$</span> has bandwidth <em>at most</em> <span>$k$</span> if all entries above the <span>$k$</span>ᵗʰ superdiagonal and below the <span>$k$</span>ᵗʰ subdiagonal are zero, and <span>$A$</span> has bandwidth <em>at least</em> <span>$k$</span> if there exists any nonzero entry in the <span>$k$</span>ᵗʰ superdiagonal or subdiagonal.</p><p>The <em>matrix bandwidth recognition problem</em> entails determining whether there exists a permutation matrix <span>$P$</span> such that the bandwidth of <span>$PAPᵀ$</span> is at most some fixed non-negative integer <span>$k ∈ ℕ$</span>—an optimal permutation that fully minimizes the bandwidth of <span>$A$</span> is not required. Unlike the NP-hard minimization problem, this is decidable in <span>$O(nᵏ)$</span> time.</p><p>The following algorithms are currently supported:</p><ul><li>Caprara–Salazar-González algorithm (<a href="#MatrixBandwidth.Recognition.CapraraSalazarGonzalez"><code>CapraraSalazarGonzalez</code></a>)</li><li>Del Corso–Manzini algorithm (<a href="#MatrixBandwidth.Recognition.DelCorsoManzini"><code>DelCorsoManzini</code></a>)</li><li>Del Corso–Manzini algorithm with perimeter search (<a href="#MatrixBandwidth.Recognition.DelCorsoManziniWithPS"><code>DelCorsoManziniWithPS</code></a>)</li><li>Saxe–Gurari–Sudborough algorithm (<a href="#MatrixBandwidth.Recognition.SaxeGurariSudborough"><code>SaxeGurariSudborough</code></a>)</li><li>Brute-force search (<a href="#MatrixBandwidth.Recognition.BruteForceSearch"><code>BruteForceSearch</code></a>)</li></ul><p>This submodule is part of the <a href="https://github.com/Luis-Varona/MatrixBandwidth.jl">MatrixBandwidth.jl</a> package.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Recognition/Recognition.jl#L7-L33">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Recognition.BruteForceSearch" href="#MatrixBandwidth.Recognition.BruteForceSearch"><code>MatrixBandwidth.Recognition.BruteForceSearch</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">BruteForceSearch &lt;: AbstractDecider &lt;: AbstractAlgorithm</code></pre><p>The simplest method for determining, given some fixed <span>$k ∈ ℕ$</span>, whether a matrix has bandwidth at most <span>$k$</span> up to symmetric permutation is to iterate over all orderings and compute the bandwidth induced by each.</p><p>Since <span>$i₁, i₂, … iₙ$</span> induces the same bandwidth as <span>$iₙ, iₙ₋₁, … i₁$</span>, we restrict our search to orderings such that <span>$i₁ ≤ iₙ$</span> (with equality checked just in case <span>$n = 1$</span>).</p><p>If a bandwidth-<span>$k$</span> ordering is found, the algorithm breaks early instead of continuing to check subsequent permutations.</p><p><strong>Performance</strong></p><p>Given an <span>$n×n$</span> input matrix <span>$A$</span>, this brute-force algorithm runs in <span>$O(n! ⋅ n²)$</span> time:</p><ul><li>Up to <span>$n!/2$</span> permutations may be checked (except when <span>$n = 1$</span>, in which case   <span>$1! = 1$</span> permutation is checked). This is, clearly, <span>$O(n!)$</span>.</li><li>For each permutation, the <a href="#MatrixBandwidth.bandwidth-Tuple{AbstractMatrix{&lt;:Number}}"><code>bandwidth</code></a> function is called on   <span>$view(A, perm, perm)$</span>, which takes <span>$O(n²)$</span> time.</li><li>Therefore, the overall time complexity is <span>$O(n! ⋅ n²)$</span>.</li></ul><p><strong>Examples</strong></p><p>In many cases, the algorithm iterates over all (if <span>$k$</span> is smaller than the true minimu bandwidth) or almost all (if <span>$k$</span> is equally to or only slightly larger than the true minimum) possible permutations—in these cases, it is infeasible to go above <span>$9×9$</span> or <span>$10×10$</span> without incurring multiple-hour runtimes. (Even when <span>$k$</span> is considerably larger than the true minimum, it is unlikely that a bandwidth-<span>$k$</span> ordering will be found in a reasonable time frame.) Nevertheless, we see that it is quite effective for, say, <span>$8×8$</span>:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random, SparseArrays

julia&gt; Random.seed!(314159);

julia&gt; (n, p) = (8, 0.5);

julia&gt; A = sprand(Bool, n, n, p);

julia&gt; A = A + A&#39; # Ensure structural symmetry;

julia&gt; (k_false, k_true) = (3, 5);

julia&gt; has_bandwidth_k_ordering(A, k_false, Recognition.BruteForceSearch())
Results of Bandwidth Recognition Algorithm
 * Algorithm: Brute-force search
 * Bandwidth Threshold k: 3
 * Has Bandwidth ≤ k Ordering: false
 * Original Bandwidth: 6
 * Matrix Size: 8×8

julia&gt; has_bandwidth_k_ordering(A, k_true, Recognition.BruteForceSearch())
Results of Bandwidth Recognition Algorithm
 * Algorithm: Brute-force search
 * Bandwidth Threshold k: 5
 * Has Bandwidth ≤ k Ordering: true
 * Original Bandwidth: 6
 * Matrix Size: 8×8</code></pre><p><strong>Notes</strong></p><p>Brute force is by far the slowest approach to matrix bandwidth minimization and should only be used in very niche cases (like verifying the correctness of other algorithms in unit tests). For <span>$10×10$</span> matrices, the algorithm already takes several minutes to run for difficult values of <span>$k$</span> (namely, values below or only slightly above the true minimum) and allocates several gigabytes of memory. Given the <span>$O(n! ⋅ n²)$</span> time complexity, checking &quot;bandwidth ≤ k&quot; would take over an hour for many <span>$11×11$</span> matrices.</p><p>This holds true even when <span>$k$</span> is considerably larger than the true minimum bandwidth—as long as it remains below the bandwidth induced by the original ordering, it is unlikely that a bandwidth-<span>$k$</span> ordering will be found early simply by random chance. Additionally, time complexity will remain on the order of <span>$n! ⋅ n²$</span> in the average case.</p><p>See also <a href="#MatrixBandwidth.Minimization.Exact.BruteForceSearch"><code>MatrixBandwidth.Minimization.Exact.BruteForceSearch</code></a> for the minimization variant of this algorithm (which simply never breaks early, instead iterating over all permutations up to reversal to ensure that the minimum bandwidth is found).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Recognition/deciders/brute_force_search.jl#L7-L81">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Recognition.CapraraSalazarGonzalez" href="#MatrixBandwidth.Recognition.CapraraSalazarGonzalez"><code>MatrixBandwidth.Recognition.CapraraSalazarGonzalez</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">CapraraSalazarGonzalez &lt;: AbstractDecider &lt;: AbstractAlgorithm</code></pre><p>The <em>Caprara–Salazar-González recognition algorithm</em> is a method for determining, given some fixed <span>$k ∈ ℕ$</span>, whether a structurally symmetric matrix <span>$A$</span> has a bandwidth at most <span>$k$</span> up to symmetric permutation. The algorithm performs a bidirectional depth-first search of all partial orderings of the rows and columns of <span>$A$</span>, adding indices one at a time to both the left and right ends. Partial orderings are pruned not only by ensuring that adjacent pairs of currently placed indices are within <span>$k$</span> of each other but also by employing a branch-and-bound framework with lower bounds on bandwidtth compatibility computed via integer linear programming relaxations. This search is repeated with incrementing values of <span>$k$</span> until a bandwidth-<span>$k$</span> ordering is found [<a href="#CSG05">CS05</a>], with <span>$k$</span> initialized to some lower bound on the minimum bandwidth of <span>$A$</span> up to symmetric permutation.</p><p>As noted above, the Caprara–Salazar-González algorithm requires structurally symmetric input (that is, <span>$A[i, j]$</span> must be nonzero if and only if <span>$A[j, i]$</span> is nonzero for <span>$1 ≤ i, j ≤ n$</span>).</p><p><strong>Performance</strong></p><p>[TODO: Write here]</p><p><strong>Examples</strong></p><p>[TODO: Write here]</p><p><strong>Notes</strong></p><p>This algorithm is not the main one described in the original paper [<a href="#CSG05">CS05</a>], which actually never explicitly presents a procedure for matrix bandwidth recognition. However, the paper does define a bandwidth minimization algorithm that repeatedly calls a recognition subroutine—this is precisely the logic we implement here. (We do, however, also implement said minimization algorithm in <a href="#MatrixBandwidth.Minimization.Exact.CapraraSalazarGonzalez"><code>MatrixBandwidth.Minimization.Exact.CapraraSalazarGonzalez</code></a>.)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Recognition/deciders/caprara_salazar_gonzalez.jl#L7-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Recognition.DelCorsoManzini" href="#MatrixBandwidth.Recognition.DelCorsoManzini"><code>MatrixBandwidth.Recognition.DelCorsoManzini</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DelCorsoManzini &lt;: AbstractDecider &lt;: AbstractAlgorithm</code></pre><p>The <em>Del Corso–Manzini recognition algorithm</em> is a method for determining, given some fixed <span>$k ∈ ℕ$</span>, whether a structurally symmetric matrix <span>$A$</span> has bandwidth at most <span>$k$</span> up to symmetric permutation. The algorithm performs a depth-first search of all partial orderings of the rows and columns of <span>$A$</span>, adding indices one at a time. Partial orderings are pruned not only by ensuring that adjacent pairs of currently placed indices are within <span>$k$</span> of each other but also by tracking the latest positions at which the remaining indices can be placed [<a href="#DCM99">DM99</a>].</p><p>As noted above, the Del Corso–Manzini algorithm requires structurally symmetric input (that is, <span>$A[i, j]$</span> must be nonzero if and only if <span>$A[j, i]$</span> is nonzero for <span>$1 ≤ i, j ≤ n$</span>).</p><p><strong>Performance</strong></p><p>Given an <span>$n×n$</span> input matrix <span>$A$</span> and threshold bandwidth <span>$k$</span>, the Del Corso–Manzini algorithm runs in <span>$O(n! ⋅ nk)$</span> time:</p><ul><li>We perform a depth-first search of <span>$O(n!)$</span> partial orderings.</li><li>Checking plausibility of each partial ordering takes <span>$O(nk)$</span> time.</li><li>Therefore, the overall time complexity is <span>$O(n! ⋅ nk)$</span>.</li></ul><p>Of course, this is but an upper bound on the time complexity of Del Corso–Manzini, achieved only in the most pathological of cases. In practice, efficient pruning techniques and compatibility checks result in approximately exponential growth in time complexity with respect to <span>$n$</span>.</p><p>Based on experimental results, the algorithm is feasible for <span>$n×n$</span> matrices up to <span>$n ≈ 100$</span> or so.</p><p><strong>Examples</strong></p><p>We demonstrate both an affirmative and a negative result for the Del Corso–Manzini recognition algorithm on a random <span>$40×40$</span> matrix:</p><pre><code class="language-julia-repl hljs">julia&gt; using Random, SparseArrays

julia&gt; Random.seed!(7878);

julia&gt; (n, p) = (40, 0.1);

julia&gt; A = sprand(n, n, p);

julia&gt; A = A + A&#39; # Ensure structural symmetry;

julia&gt; (k_false, k_true) = (13, 26);

julia&gt; has_bandwidth_k_ordering(A, k_false, Recognition.DelCorsoManzini())
Results of Bandwidth Recognition Algorithm
 * Algorithm: Del Corso–Manzini
 * Bandwidth Threshold k: 13
 * Has Bandwidth ≤ k Ordering: false
 * Original Bandwidth: 34
 * Matrix Size: 40×40

julia&gt; has_bandwidth_k_ordering(A, k_true, Recognition.DelCorsoManzini())
Results of Bandwidth Recognition Algorithm
 * Algorithm: Del Corso–Manzini
 * Bandwidth Threshold k: 26
 * Has Bandwidth ≤ k Ordering: true
 * Original Bandwidth: 34
 * Matrix Size: 40×40</code></pre><p><strong>Notes</strong></p><p>For readers of the original paper, what we call the Del Corso–Manzini recognition algorithm here is essentially a wrapper around the underlying <code>AddNode</code> subroutine in what [<a href="#DCM99">DM99</a>, p. 191] terms the &quot;MB-ID algorithm&quot; for bandwidth minimization (not mere recognition). MB-ID (which we also implement in <a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManzini"><code>MatrixBandwidth.Minimization.Exact.DelCorsoManzini</code></a>) calls this recognition procedure with incrementing values of <span>$k$</span> until a bandwidth-<span>$k$</span> ordering is found, with <span>$k$</span> initialized to some lower bound on the minimum bandwidth of <span>$A$</span> up to symmetric permutation.</p><p>[<a href="#DCM99">DM99</a>, p. 193] also describes an &quot;MB-PS algorithm&quot; for bandwidth minimization, which we implement in <a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS"><code>MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS</code></a>. Similarly, the underlying recognition subroutine for MB-PS is implemented in <a href="#MatrixBandwidth.Recognition.DelCorsoManziniWithPS"><code>DelCorsoManziniWithPS</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Recognition/deciders/del_corso_manzini.jl#L7-L83">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Recognition.DelCorsoManziniWithPS" href="#MatrixBandwidth.Recognition.DelCorsoManziniWithPS"><code>MatrixBandwidth.Recognition.DelCorsoManziniWithPS</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DelCorsoManziniWithPS{D} &lt;: AbstractDecider &lt;: AbstractAlgorithm</code></pre><p>The <em>Del Corso–Manzini recognition algorithm with perimeter search</em> is a method for determining, given some fixed <span>$k ∈ ℕ$</span>, whether a structurally symmetric matrix <span>$A$</span> has bandwidth at most <span>$k$</span> up to symmetric permutation. The base Del Corso–Manzini algorithm performs a depth-first search of all partial orderings of the rows and columns of <span>$A$</span>, adding indices one at a time. Partial orderings are pruned not only by ensuring that adjacent pairs of currently placed indices are within <span>$k$</span> of each other but also by tracking the latest positions at which the remaining indices can be placed [<a href="#DCM99">DM99</a>].</p><p>The incorporation of perimeter search to this approach entails precomputing a &quot;perimeter&quot; of <span>$d$</span>-permutations of row indices of <span>$A$</span>, where <span>$d$</span> is a positive integer passed as a parameter to the decider. Each permutation represents a way to select the last <span>$d$</span> entries of the ordering, and as the construction of the partial ordering progresses, potential endings are pruned to exclude those incompatible with already placed indices. In addition to pruning a potential ending if it contains indices already placed, compatibility is also checked via precomputed time stamps indicating, for each potential ending, a loose lower bound on the earliest position at which any given index can be placed should said ending be selected.</p><p>As noted above, the Del Corso–Manzini algorithm with perimeter search requires structurally symmetric input (that is, <span>$A[i, j]$</span> must be nonzero if and only if <span>$A[j, i]$</span> is nonzero for <span>$1 ≤ i, j ≤ n$</span>).</p><p><strong>Fields</strong></p><ul><li><code>depth::D&lt;:Union{Nothing,Int}</code>: the perimeter search depth. If this field is not set (and   thus automatically initialized to <code>nothing</code>), a default depth is computed by   <a href="../private_api/#MatrixBandwidth.Recognition.dcm_ps_optimal_depth-Tuple{AbstractMatrix{Bool}}"><code>dcm_ps_optimal_depth</code></a> as a function of the input matrix every time the decider   is passed to <a href="#MatrixBandwidth.Recognition.has_bandwidth_k_ordering"><code>has_bandwidth_k_ordering</code></a> as a function of the input matrix.   Otherwise, it must be manually set to a positive integer.</li></ul><p><strong>Constructors</strong></p><ul><li><code>DelCorsoManziniWithPS()</code>: constructs a new <code>DelCorsoManziniWithPS</code> instance with the   default perimeter search depth initialized to <code>nothing</code>.</li><li><code>DelCorsoManziniWithPS(depth::Int)</code>: constructs a new <code>DelCorsoManziniWithPS</code> instance   with the specified perimeter search depth. <code>depth</code> must be a positive integer.</li></ul><p><strong>Performance</strong></p><p>Given an <span>$n×n$</span> input matrix <span>$A$</span>, perimeter search depth <span>$d$</span>, and threshold bandwidth <span>$k$</span>, the Del Corso–Manzini algorithm with perimeter search runs in <span>$O(n! ⋅ max(nᵈ, nk))$</span> time:</p><ul><li>We perform a depth-first search of <span>$O(n!)$</span> partial orderings.</li><li>Checking plausibility of each partial ordering takes <span>$O(nk)$</span> time, and checking   compatibility with all size-<span>$d$</span> LPOs takes <span>$O(nᵈ)$</span> time. Thus, the overall time   complexity for each value of <span>$k$</span> is <span>$O(n! ⋅ (nᵈ + nk))$</span>.</li><li>Therefore, the overall time complexity is <span>$O(n! ⋅ max(nᵈ, nk))$</span>.</li></ul><p>Of course, this is but an upper bound on the time complexity of Del Corso–Manzini with perimeter search, achieved only in the most pathological of cases. In practice, efficient pruning techniques and compatibility checks result in approximately exponential growth in time complexity with respect to <span>$n$</span>.</p><p>Based on experimental results, the algorithm is feasible for <span>$n×n$</span> matrices up to <span>$n ≈ 100$</span> or so.</p><p><strong>Examples</strong></p><p>Here, Del Corso–Manzini with perimeter search ascertains that A random <span>$30×30$</span> matrix has a minimum bandwidth greater than <span>$9$</span>. The depth parameter is not explicitly set; instead, some near-optimal value is automatically computed upon the first <a href="#MatrixBandwidth.Recognition.has_bandwidth_k_ordering"><code>has_bandwidth_k_ordering</code></a> function call.</p><pre><code class="language-julia-repl hljs">julia&gt; using Random, SparseArrays

julia&gt; Random.seed!(5847);

julia&gt; (n, p) = (30, 0.05);

julia&gt; A = sprand(n, n, p);

julia&gt; A = A + A&#39; # Ensure structural symmetry;

julia&gt; k = 6;

julia&gt; has_bandwidth_k_ordering(A, k, Recognition.DelCorsoManziniWithPS())
Results of Bandwidth Recognition Algorithm
 * Algorithm: Del Corso–Manzini with perimeter search
 * Bandwidth Threshold k: 6
 * Has Bandwidth ≤ k Ordering: false
 * Original Bandwidth: 27
 * Matrix Size: 30×30</code></pre><p>Now, Del Corso–Manzini with perimeter search recognizes that a random <span>$35×35$</span> matrix has a minimum bandwidth at most <span>$8$</span>. In this case, we explitily set the depth parameter to <span>$4$</span> beforehand instead of relying on <a href="../private_api/#MatrixBandwidth.Recognition.dcm_ps_optimal_depth-Tuple{AbstractMatrix{Bool}}"><code>Recognition.dcm_ps_optimal_depth</code></a>.</p><pre><code class="language-julia-repl hljs">julia&gt; using Random, SparseArrays

julia&gt; Random.seed!(23552);

julia&gt; (n, p, depth) = (35, 0.02, 4);

julia&gt; A = sprand(n, n, p);

julia&gt; A = A + A&#39; # Ensure structural symmetry;

julia&gt; k = 8;

julia&gt; has_bandwidth_k_ordering(A, k, Recognition.DelCorsoManziniWithPS(depth))
Results of Bandwidth Recognition Algorithm
 * Algorithm: Del Corso–Manzini with perimeter search
 * Bandwidth Threshold k: 8
 * Has Bandwidth ≤ k Ordering: true
 * Original Bandwidth: 32
 * Matrix Size: 35×35</code></pre><p><strong>Notes</strong></p><p>For readers of the original paper, what we call the Del Corso–Manzini recognition algorithm with perimeter search here is essentially a wrapper around the underlying <code>AddNode1</code> and <code>Prune</code> subroutines in what [<a href="#DCM99">DM99</a>, p. 193] terms the &quot;MB-PS algorithm&quot; for bandwidth minimization (not mere recognition). MB-PS (which we also implement in <a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS"><code>MatrixBandwidth.Minimization.Exact.DelCorsoManziniWithPS</code></a>) calls this recognition procedure with incrementing values of <span>$k$</span> until a bandwidth-<span>$k$</span> ordering is found, with <span>$k$</span> initialized to some lower bound on the minimum bandwidth of <span>$A$</span> up to symmetric permutation.</p><p>[<a href="#DCM99">DM99</a>, p. 191] also describes an &quot;MB-ID algorithm&quot; for bandwidth minimization, which we implement in <a href="#MatrixBandwidth.Minimization.Exact.DelCorsoManzini"><code>MatrixBandwidth.Minimization.Exact.DelCorsoManzini</code></a>. Similarly, the underlying recognition subroutine for MB-ID is implemented in <a href="#MatrixBandwidth.Recognition.DelCorsoManzini"><code>DelCorsoManzini</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Recognition/deciders/del_corso_manzini.jl#L90-L211">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Recognition.RecognitionResult" href="#MatrixBandwidth.Recognition.RecognitionResult"><code>MatrixBandwidth.Recognition.RecognitionResult</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RecognitionResult{A,M,O} &lt;: AbstractResult</code></pre><p>Output struct for matrix bandwidth recognition results.</p><p><strong>Fields</strong></p><ul><li><code>algorithm::A&lt;:AbstractDecider</code>: the decider used to test the bandwidth.</li><li><code>matrix::M&lt;:AbstractMatrix{&lt;:Number}</code>: the original matrix whose bandwidth is tested.</li><li><code>ordering::O&lt;:Union{Nothing,Vector{Int}}</code>: an ordering of the rows and columns of <code>matrix</code>   inducing a bandwidth at most <code>k</code>, if such an ordering exists; otherwise, <code>nothing</code>.</li><li><code>k::Int</code>: the threshold bandwidth against which to test.</li><li><code>has_ordering::Bool</code>: whether the matrix has an ordering inducing a bandwidth at most <code>k</code>.   (This is <code>true</code> if and only if <code>ordering</code> is not <code>nothing</code>.)</li></ul><p><strong>Constructors</strong></p><ul><li><code>RecognitionResult(decider, matrix, ordering, k)</code>: constructs a new <code>RecognitionResult</code>   instance with the given fields. The <code>has_ordering</code> field is automatically determined   based on whether <code>ordering</code> is <code>nothing</code> or a <code>Vector{Int}</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Recognition/types.jl#L24-L42">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Recognition.SaxeGurariSudborough" href="#MatrixBandwidth.Recognition.SaxeGurariSudborough"><code>MatrixBandwidth.Recognition.SaxeGurariSudborough</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SaxeGurariSudborough &lt;: AbstractDecider &lt;: AbstractAlgorithm</code></pre><p>[TODO: Write here]</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Recognition/deciders/saxe_gurari_sudborough.jl#L7-L11">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MatrixBandwidth.Recognition.has_bandwidth_k_ordering" href="#MatrixBandwidth.Recognition.has_bandwidth_k_ordering"><code>MatrixBandwidth.Recognition.has_bandwidth_k_ordering</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">has_bandwidth_k_ordering(A, k, decider=CapraraSalazarGonzalez()) -&gt; RecognitionResult</code></pre><p>Determine whether <code>A</code> has bandwidth at most <code>k</code> using the algorithm defined by <code>decider</code>.</p><p>The <em>bandwidth</em> of an <span>$n×n$</span> matrix <span>$A$</span> is the minimum non-negative integer <span>$k ∈ [0, n - 1]$</span> such that <span>$A[i, j] = 0$</span> whenever <span>$|i - j| &gt; k$</span>. Equivalently, <span>$A$</span> has bandwidth <em>at most</em> <span>$k$</span> if all entries above the <span>$k$</span>ᵗʰ superdiagonal and below the <span>$k$</span>ᵗʰ subdiagonal are zero, and <span>$A$</span> has bandwidth <em>at least</em> <span>$k$</span> if there exists any nonzero entry in the <span>$k$</span>ᵗʰ superdiagonal or subdiagonal.</p><p>Given some fixed non-negative integer <code>k</code>, this function determines (with 100% certainty) whether there exists some ordering <span>$π$</span> of the rows and columns of <span>$A$</span> such that the bandwidth of <span>$PAPᵀ$</span> is at most <code>k</code>, where <span>$P$</span> is the permutation matrix corresponding to <span>$π$</span>. This is known to be decidable in <span>$O(nᵏ)$</span> time, although some deciders (e.g., <a href="#MatrixBandwidth.Recognition.CapraraSalazarGonzalez"><code>CapraraSalazarGonzalez</code></a>) run in exponential time instead to produce even quicker runtimes in practice.</p><p>If <span>$k ≥ n - 1$</span>, then this function immediately answers in the affirmative, since the maximum possible bandwidth of an <span>$n×n$</span> matrix is <span>$n - 1$</span>. After this initial check, a preliminary lower bound on the bandwidth is computed in <span>$O(n³)$</span> time using results from Caprara and Salazar-González (2005). If this lower bound is greater than <span>$k$</span>`</p><p><strong>Arguments</strong></p><ul><li><code>A::AbstractMatrix{&lt;:Number}</code>: the (square) matrix whose bandwidth is tested.</li><li><code>k::Int</code>: the threshold bandwidth against which to test.</li><li><code>decider::AbstractDecider</code>: the matrix bandwidth recognition algorithm to use; defaults to   <a href="#MatrixBandwidth.Recognition.CapraraSalazarGonzalez"><code>CapraraSalazarGonzalez</code></a>. (See the <a href="#MatrixBandwidth.Recognition"><code>Recognition</code></a> module documentation   for a full list of supported deciders.)</li></ul><p><strong>Returns</strong></p><ul><li><code>::RecognitionResult</code>: a struct containing the algorithm used, the original matrix <code>A</code>,   the identified ordering of the rows and columns (if one exists), the threshold bandwidth   <code>k</code>, and a boolean indicating whether the ordering exists.</li></ul><p><strong>Examples</strong></p><p>[TODO: Add here once more deciders are implemented. For now, refer to the <strong>Examples</strong> sections of the <a href="#MatrixBandwidth.Recognition.DelCorsoManzini"><code>DelCorsoManzini</code></a> and <a href="#MatrixBandwidth.Recognition.DelCorsoManziniWithPS"><code>DelCorsoManziniWithPS</code></a> docstrings.]</p><p><strong>Notes</strong></p><p>Some texts define matrix bandwidth to be the minimum non-negative integer <span>$k$</span> such that <span>$A[i, j] = 0$</span> whenever <span>$|i - j| ≥ k$</span> instead, particularly in more mathematically-minded communities. Effectively, this definition treats diagonal matrices as bandwidth <span>$1$</span>, tridiagonal matrices as bandwidth <span>$2$</span>, and so on. Our definition, on the other hand, is more common in computer science contexts, treating diagonal matrices as bandwidth <span>$0$</span> and tridiagonal matrices as bandwidth <span>$1$</span>. (Both definitions, however, agree that the bandwidth of an empty matrix is simply <span>$0$</span>.)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/Luis-Varona/MatrixBandwidth.jl/blob/0470182920cdafb93a61d9d5eda41bf3acbd65ef/src/Recognition/core.jl#L7-L54">source</a></section></article><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><div class="citation canonical"><dl><dt>[CS05]</dt><dd><div id="CSG05">A. Caprara and J.-J. Salazar-González. <a href="https://doi.org/10.1287/ijoc.1040.0083"><em>Laying Out Sparse Graphs with Provably Minimum Bandwidth</em></a>. <em>INFORMS Journal on Computing</em> <strong>17</strong>, 356–73 (2005).</div></dd><dt>[CG80]</dt><dd><div id="CG80">W. M. Chan and A. George. <a href="https://doi.org/10.1007/BF01933580"><em>A linear time implementation of the reverse Cuthill–McKee algorithm</em></a>. <em>BIT Numerical Mathematics</em> <strong>20</strong>, 8–14 (1980).</div></dd><dt>[CM69]</dt><dd><div id="CM69">E. Cuthill and J. McKee. <a href="https://doi.org/10.1145/800195.805928"><em>Reducing the bandwidth of sparse symmetric matrices</em></a>. In: <em>Proceedings of the 24th National Conference of the ACM</em> (Brandon Systems Press, 1969); pp. 157–72.</div></dd><dt>[DM99]</dt><dd><div id="DCM99">G. M. Del Corso and G. Manzini. <a href="https://doi.org/10.1007/s006070050002"><em>Finding Exact Solutions to the Bandwidth Minimization Problem</em></a>. <em>Computing</em> <strong>62</strong>, 189–203 (1999).</div></dd><dt>[Geo71]</dt><dd><div id="Geo71">J. A. George. <a href="https://apps.dtic.mil/sti/tr/pdf/AD0726171.pdf"><em>Computer Implementation of the Finite Element Method</em></a>. Ph.D. Thesis, Department of Computer Science, Stanford University (1971).</div></dd><dt>[GPS76]</dt><dd><div id="GPS76">N. E. Gibbs, W. G. Poole Jr. and P. K. Stockmeyer. <a href="https://doi.org/10.1137/0713023"><em>An Algorithm for Reducing the Bandwidth and Profile of a Sparse Matrix</em></a>. <em>SIAM Journal on Numerical Analysis</em> <strong>13</strong>, 236–50 (1976).</div></dd><dt>[Lew82]</dt><dd><div id="Lew82">J. G. Lewis. <a href="https://doi.org/10.1145/355993.355998"><em>Implementation of the Gibbs–Poole–Stockmeyer and Gibbs–King Algorithms</em></a>. <em>ACM Transactions on Mathematical Software</em> <strong>8</strong>, 180–89 (1982).</div></dd></dl></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../private_api/">Private API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Wednesday 23 July 2025 02:22">Wednesday 23 July 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
